"""
WarehouseDataValidator
---------------------------------------
üè≠ Data Warehouse Ï†ÑÏö© Validator
- DataPathResolver Í∏∞Î∞ò Í≤ΩÎ°ú ÏûêÎèô ÏÑ∏ÌåÖ
- BaseDataValidator ÏÉÅÏÜç
"""

from pathlib import Path
from plugins.validators.base_data_validator import BaseDataValidator
from plugins.utils.path_manager import DataPathResolver
from plugins.config import constants as C
import pandas as pd
import json

class WarehouseDataValidator(BaseDataValidator):
    """
    ‚úÖ Data Warehouse Í≥ÑÏ∏µ Ïú†Ìö®ÏÑ± Í≤ÄÏ¶ù Validator
    ÏòàÏãú Íµ¨Ï°∞:
      /opt/airflow/data/data_warehouse/snapshot/equity/asset_master/country_code=KOR/trd_dt=2025-11-11/asset_master.parquet
    """

    def __init__(
        self,
        domain: str,
        trd_dt: str,
        vendor: str = "eodhd",
        exchange_code: str = "ALL",
        domain_group: str = "equity",
        country_code: str | None = None,
        allow_empty: bool = False,
        **kwargs,
    ):
        """
        - domain: Í≤ÄÏ¶ù ÎåÄÏÉÅ ÎèÑÎ©îÏù∏ (Ïòà: exchange, asset, price Îì±)
        - trd_dt: Í≤ÄÏ¶ù Í∏∞Ï§ÄÏùº
        - vendor: Îç∞Ïù¥ÌÑ∞ Î≤§Îçî (Ïòà: eodhd)
        - country_code: Íµ≠Í∞ÄÏΩîÎìú (ÏÑ†ÌÉù)
        """

        # ‚úÖ warehouse Í≤ΩÎ°ú ÏûêÎèô ÏÉùÏÑ±
        dataset_path = DataPathResolver.warehouse_snapshot(
            domain_group=domain_group,
            domain=domain,
            country_code=country_code,
            trd_dt=trd_dt,
        )

        super().__init__(
            domain=domain,
            domain_group=domain_group,
            layer="warehouse",
            trd_dt=trd_dt,
            vendor=vendor,
            exchange_code=exchange_code,
            allow_empty=allow_empty,
            **kwargs,
        )

        self.dataset_path = dataset_path
        self.country_code = country_code
        self.log.info(f"üì¶ Warehouse dataset path: {self.dataset_path}")

    # -------------------------------------------------------------------------
    # ‚úÖ Í≤∞Í≥º Ï†ÄÏû• (Parquet + Meta)
    # -------------------------------------------------------------------------
    def _save_result(self, result: dict, df: pd.DataFrame) -> Path:
        """
        ‚úÖ Warehouse Ï†ÑÏö© parquet Ï†ÄÏû•
        DataPathResolver Í∏∞Î∞ò:
          /data_warehouse/snapshot/{group}/{domain_name}/country_code=XXX/trd_dt=YYYY-MM-DD/{domain_name}.parquet
        """

        snapshot_dir = DataPathResolver.warehouse_snapshot(
            domain_group=self.domain_group,
            domain=self.domain,
            country_code=self.country_code,
            trd_dt=self.trd_dt,
        )
        snapshot_dir.mkdir(parents=True, exist_ok=True)

        # ÎèÑÎ©îÏù∏Î™Ö ÌëúÏ§ÄÌôî (ex: asset ‚Üí asset_master)
        domain_name = C.WAREHOUSE_DOMAINS.get(self.domain, self.domain)
        parquet_path = snapshot_dir / f"{domain_name}.parquet"

        # ‚úÖ parquet Ï†ÄÏû•
        df.to_parquet(parquet_path, index=False)
        self.log.info(f"‚úÖ Parquet Ï†ÄÏû• ÏôÑÎ£å: {parquet_path} ({len(df):,}Ìñâ)")

        return parquet_path



    def _load_dataset(self) -> pd.DataFrame:
        """
        ‚úÖ domainÎ≥Ñ ÌååÏùº Ìè¨Îß∑ Íµ¨Î∂Ñ Î°úÏßÅ
        - ÏùºÎ∞ò ÎèÑÎ©îÏù∏(asset, price Îì±): Parquet ÌååÏùº
        - fundamentals: security_id Îã®ÏúÑ JSON Ìè¥Îçî
        """
        if not self.dataset_path.exists():
            self.log.warning(f"‚ö†Ô∏è Dataset path not found: {self.dataset_path}")
            return pd.DataFrame()

        # -------------------------------------------------------------
        # 1Ô∏è‚É£ Fundamentals Ï†ÑÏö© Ï≤òÎ¶¨ (JSON Íµ¨Ï°∞)
        # -------------------------------------------------------------
        if self.domain in ["fundamentals", "fundamental_master"]:
            security_dirs = [p for p in self.dataset_path.glob("security_id=*") if p.is_dir()]
            if not security_dirs:
                self.log.warning(f"‚ö†Ô∏è No security_id folders found in {self.dataset_path}")
                return pd.DataFrame()

            dfs = []
            for d in security_dirs:
                general_path = d / "General.json"
                if not general_path.exists():
                    continue
                try:
                    data = json.load(open(general_path, "r", encoding="utf-8"))
                    df = pd.json_normalize(data, sep="_")
                    df["security_id"] = d.name.split("=")[-1]
                    dfs.append(df)
                except Exception as e:
                    self.log.warning(f"‚ö†Ô∏è Failed to load {general_path.name}: {e}")

            if not dfs:
                self.log.warning(f"‚ö†Ô∏è No valid JSON files in {self.dataset_path}")
                return pd.DataFrame()

            combined = pd.concat(dfs, ignore_index=True)
            self.log.info(f"‚úÖ Loaded {len(combined):,} rows from {len(dfs)} security_id folders")
            return combined

        # -------------------------------------------------------------
        # 2Ô∏è‚É£ ÏùºÎ∞ò Warehouse ÎèÑÎ©îÏù∏ Ï≤òÎ¶¨ (Parquet)
        # -------------------------------------------------------------
        elif self.dataset_path.is_dir():
            parquet_files = [f for f in self.dataset_path.glob("*.parquet") if not f.name.startswith("_")]
            if not parquet_files:
                self.log.warning(f"‚ö†Ô∏è No Parquet files in {self.dataset_path}")
                return pd.DataFrame()

            dfs = []
            for f in parquet_files:
                try:
                    df = pd.read_parquet(f)
                    if not df.empty:
                        dfs.append(df)
                except Exception as e:
                    self.log.warning(f"‚ö†Ô∏è Failed to load {f.name}: {e}")

            if not dfs:
                self.log.warning(f"‚ö†Ô∏è No valid data in {self.dataset_path}")
                return pd.DataFrame()

            combined = pd.concat(dfs, ignore_index=True)
            self.log.info(f"‚úÖ Loaded {len(combined):,} rows from {len(parquet_files)} Parquet file(s)")
            return combined

        elif self.dataset_path.suffix.lower() == ".parquet":
            df = pd.read_parquet(self.dataset_path)
            self.log.info(f"‚úÖ Loaded single parquet file: {self.dataset_path}")
            return df

        else:
            self.log.warning(f"‚ö†Ô∏è Unsupported file type: {self.dataset_path}")
            return pd.DataFrame()
