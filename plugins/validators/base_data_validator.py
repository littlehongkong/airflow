"""
Base Data Validator (Layer Unified with Vendor Support)
--------------------------------------------------------
âœ… ê¸°ëŠ¥ ìš”ì•½
1ï¸âƒ£ Lake / Warehouse / Mart ê³µí†µ Pandera + Soda Core(DuckDB) ê²€ì¦ ì—”ì§„
2ï¸âƒ£ ëª¨ë“  ê²½ë¡œëŠ” constants.py ê¸°ë°˜ (vendor í¬í•¨)
3ï¸âƒ£ ê²€ì¦ ì‹¤íŒ¨ ì‹œ Airflow Task ì‹¤íŒ¨ + validated ê²½ë¡œì— ë©”íƒ€íŒŒì¼ ì €ì¥
"""

import os, yaml, tempfile
from pathlib import Path
from datetime import datetime, timezone
from typing import Dict, Any, Optional
import pandas as pd
import pandera.pandas as pa
from pandera import DataFrameSchema, Column
from soda.scan import Scan
from filelock import FileLock
import logging
import gc
import json
import psutil
import duckdb
import re
# âœ… import ê²½ë¡œë¥¼ í˜„ì¬ êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì •
from plugins.config import constants as C



class BaseDataValidator:
    def __init__(
        self,
        domain: str,
        layer: str,
        trd_dt: Optional[str] = None,
        dataset_path: Optional[str] = None,
        vendor: Optional[str] = "eodhd",
        exchange_code: Optional[str] = "ALL",
        allow_empty: bool = False,
        domain_group: Optional[str] = None,
        **kwargs,
    ):
        self.domain = domain
        self.domain_group = domain_group
        self.layer = layer.lower()
        self.trd_dt = trd_dt
        self.vendor = vendor
        self.exchange_code = exchange_code
        self.allow_empty = allow_empty
        self.dataset_path = Path(dataset_path)
        self.log = logging.getLogger(f"{__name__}.{self.__class__.__name__}")

        # âœ… constants ê¸°ë°˜ ê²½ë¡œ ì„¤ì •
        if self.layer == "lake":
            self.schema_root = C.VALIDATOR_SCHEMA_LAKE / domain_group / vendor.lower()
            self.check_root = C.VALIDATOR_CHECKS_LAKE / domain_group / vendor.lower()
            self.data_root = C.DATA_LAKE_ROOT

        elif self.layer == "warehouse":
            # âœ… equity ë„ë©”ì¸ í´ë”ë¥¼ í¬í•¨í•˜ë„ë¡ ìˆ˜ì •
            self.schema_root = C.VALIDATOR_SCHEMA_WAREHOUSE / domain_group
            self.check_root = C.VALIDATOR_CHECKS_WAREHOUSE / domain_group
            self.data_root = C.DATA_WAREHOUSE_ROOT
        #
        # else:
        #     # âœ… mart ë„ ë™ì¼í•˜ê²Œ ë„ë©”ì¸ ë‹¨ìœ„ í´ë” ì¶”ê°€
        #     self.schema_root = VALIDATOR_SCHEMA_MART / domain_group
        #     self.check_root = VALIDATOR_CHECKS_MART / domain_group
        #     self.data_root = DATA_MART_ROOT
    # -------------------------------------------------------------------------
    # 1ï¸âƒ£ Main Validation
    # -------------------------------------------------------------------------
    def validate(self, context: Optional[dict] = None) -> Dict[str, Any]:
        df = self._load_dataset()

        if df.empty:
            if self.allow_empty:
                print(f"âœ… No data found for {self.domain} (allow_empty=True) â†’ SKIP validation.")

                result = {
                    "dataset": self.domain,
                    "layer": self.layer,
                    "vendor": self.vendor,
                    "exchange_code": getattr(self, "exchange_code", None),
                    "trd_dt": self.trd_dt,
                    "status": "skipped",
                    "record_count": 0,
                    "checks": {},
                    "validated_source": str(self.dataset_path),
                    "validated_at": datetime.now(timezone.utc).isoformat(),
                    "message": "No data found (allow_empty=True)",
                }

                validated_dir = self._save_result(result, df)
                print(f"ğŸ§¾ Skipped validation result saved: {validated_dir}")
                return result

            else:
                raise ValueError(f"âŒ No data found for {self.domain} (allow_empty=False)")

        checks = self._define_checks(df)
        status = self._aggregate_status(checks)

        result = {
            "dataset": self.domain,
            "layer": self.layer,
            "vendor": self.vendor,
            "trd_dt": self.trd_dt,
            "status": status,
            "record_count": len(df),
            "checks": checks,
            "validated_source": str(self.dataset_path),
            "validated_at": datetime.now(timezone.utc).isoformat(),
        }

        validated_dir = self._save_result(result, df)
        if status != "success":
            raise ValueError(f"âŒ Validation failed â€” see {validated_dir}/_last_validated.json")

        meta_file_path = str(self.dataset_path)
        self._update_latest_snapshot_meta(self.domain, self.trd_dt, meta_file_path)

        print(f"âœ… Validation SUCCESS â€” saved to {validated_dir}")
        return result

    # -------------------------------------------------------------------------
    # 2ï¸âƒ£ Load Dataset
    # -------------------------------------------------------------------------
    def _load_dataset(self) -> pd.DataFrame:
        """
        âœ… ë°ì´í„°ì…‹ ë¡œë” (ëŒ€ìš©ëŸ‰ ì•ˆì „ ë²„ì „)
        - fundamentals: íŒŒì¼ë³„ ìŠ¤íŠ¸ë¦¬ë° ê²€ì¦ ì§€ì›
        - ê¸°íƒ€: parquet/jsonl ë‹¨ì¼ íŒŒì¼ ë¡œë“œ
        """


        if self.dataset_path.is_dir():
            files = list(self.dataset_path.glob("*.json"))
            if not files:
                raise FileNotFoundError(f"âŒ {self.dataset_path} ë‚´ì— JSON íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")

            dfs = []
            for f in files:
                try:
                    # âœ… JSON ì½ê¸°
                    with open(f, "r", encoding="utf-8") as fp:
                        data = json.load(fp)

                    # âœ… flatten ì²˜ë¦¬ - sep='_' ì‚¬ìš©!
                    df_flat = pd.json_normalize(data, sep='_')

                    dfs.append(df_flat)
                except Exception as e:
                    self.log.warning(f"âš ï¸ {f.name} ë¡œë“œ ì‹¤íŒ¨: {e}")
                    continue

            combined = pd.concat(dfs, ignore_index=True)

            # âœ… í˜¹ì‹œ ëª¨ë¥¼ . ì œê±° (2ì°¨ ì•ˆì „ì¥ì¹˜)
            combined.columns = [c.replace(".", "_") for c in combined.columns]

            # âœ… ê²€ì¦
            assert all("." not in col for col in combined.columns), \
                f"âŒ ì»¬ëŸ¼ëª…ì— ì—¬ì „íˆ . ì¡´ì¬: {[c for c in combined.columns if '.' in c][:5]}"

            self.log.info(f"ğŸ” ë³€í™˜ëœ ì»¬ëŸ¼ ìƒ˜í”Œ: {list(combined.columns[:10])}")
            self.log.info(f"âœ… Flatten ì™„ë£Œ: {len(combined):,}í–‰, {len(combined.columns)}ì—´")

            return combined


        # âœ… ë‹¨ì¼ íŒŒì¼ ì²˜ë¦¬
        if self.dataset_path is None or not self.dataset_path.exists():
            return pd.DataFrame()

        ext = self.dataset_path.suffix.lower()
        if ext == ".parquet":
            return pd.read_parquet(self.dataset_path)
        elif ext in [".json", ".jsonl"]:
            return duckdb.query(f"SELECT * FROM read_json_auto('{self.dataset_path}')").to_df()
        else:
            raise ValueError(f"âŒ Unsupported file format: {ext}")

    # -------------------------------------------------------------------------
    # 3ï¸âƒ£ Pandera + Soda Core Validation
    # -------------------------------------------------------------------------
    def _define_checks(self, df: pd.DataFrame) -> Dict[str, Any]:
        checks = {}

        # âœ… numeric ì»¬ëŸ¼ íƒ€ì… ë³´ì • (Pandera Float ëŒ€ì‘)
        for col in ["active_tickers", "previous_day_updated_tickers", "updated_tickers"]:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors="coerce")

        # âœ… Pandera ê²€ì¦
        self.log.info("Pandera ê²€ì¦ ì‹œì‘")
        schema_path = self.schema_root / f"{self.domain}.json"
        if schema_path.exists():
            with open(schema_path, "r", encoding="utf-8") as f:
                schema_def = json.load(f)
            checks["pandera_schema"] = self._validate_with_pandera(df, schema_def)
            self.log.info(f"Pandera ê²€ì¦ ì™„ë£Œ : {checks['pandera_schema']}")
        else:
            print(f"âš ï¸ Pandera schema not found: {schema_path}")

        # âœ… Soda Core ê²€ì¦ (DuckDB)
        if self.domain == "fundamentals":
            # âœ… Type í•„ë“œ íƒìƒ‰ (General.Type or General.Type.value)
            type_col = None
            for cand in ["General.Type", "Type", "General.Type.value"]:
                if cand in df.columns:
                    type_col = cand
                    break

            # ê¸°ë³¸ê°’ Common Stock
            fund_type = "stock"
            if type_col and "ETF" in df[type_col].astype(str).unique():
                fund_type = "etf"

            soda_filename = f"{self.domain}_{fund_type}.yml"
            soda_path = self.check_root / soda_filename

            if soda_path.exists():
                print(f"âœ… Using Soda checks: {soda_path.name}")

                # print(df.columns)
                # print(df.head())

                checks.update(self._run_soda_duckdb_validation(df, soda_path))
            else:
                print(f"âš ï¸ Soda check file not found: {soda_path}")

        else:
            # âœ… ì¼ë°˜ ë„ë©”ì¸ ì²˜ë¦¬
            soda_path = self.check_root / f"{self.domain}.yml"
            if soda_path.exists():
                checks.update(self._run_soda_duckdb_validation(df, soda_path))
            else:
                print(f"âš ï¸ Soda check file not found: {soda_path}")

        return checks

    # -------------------------------------------------------------------------
    # 4ï¸âƒ£ Pandera Validation
    # -------------------------------------------------------------------------
    def _validate_with_pandera(self, df: pd.DataFrame, schema_def: dict) -> Dict[str, Any]:
        # 1) ìŠ¤í‚¤ë§ˆ â†’ pandera dtype ë§¤í•‘
        type_map = {
            "String": pa.String,
            "Int": pa.Int,
            "Float": pa.Float,
            "DateTime": pa.DateTime,
            "Datetime": pa.DateTime,
            "Bool": pa.Bool,
        }

        try:
            # 2) í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
            defined_cols = [c["name"] for c in schema_def.get("columns", [])]
            missing_cols = [c for c in defined_cols if c not in df.columns]
            if missing_cols:
                raise ValueError(f"âŒ Missing required columns in dataset: {missing_cols}")

            # 3) ì‚¬ì „ ì •ê·œí™”: ê³µë°±â†’NaN, ë¬¸ìì—´ íŠ¸ë¦¼
            for c in df.columns:
                if df[c].dtype == object:
                    df[c] = df[c].apply(lambda x: x.strip() if isinstance(x, str) else x)
                    df[c] = df[c].replace({"": None})

            # 4) ìŠ¤í‚¤ë§ˆ ê¸°ë°˜ íƒ€ì… ê°•ì œ(coerce)
            #    - ìˆ«ì/ë‚ ì§œê°€ ë¬¸ìì—´ì´ì–´ë„ ì˜¬ë°”ë¥¸ dtypeìœ¼ë¡œ ë³€í™˜
            for col_def in schema_def.get("columns", []):
                name = col_def["name"]
                typ = (col_def.get("type") or "String").lower()
                if name not in df.columns:
                    continue
                try:
                    if typ in ("float",):
                        df[name] = pd.to_numeric(df[name], errors="coerce")
                    elif typ in ("int",):
                        df[name] = pd.to_numeric(df[name], errors="coerce").astype("Int64")
                    elif typ in ("datetime", "datetime"):
                        # ë‚ ì§œëŠ” í‘œì¤€ YYYY-MM-DD, YYYY-MM-DDTHH:MM:SS ëª¨ë‘ í—ˆìš©
                        df[name] = pd.to_datetime(df[name], errors="coerce", utc=False)
                    elif typ in ("bool",):
                        # "true"/"false"/1/0 ë“± ë¬¸ìì—´ë„ ì²˜ë¦¬
                        df[name] = df[name].astype(str).str.lower().map(
                            {"true": True, "false": False, "1": True, "0": False}
                        ).astype("boolean")
                    else:
                        df[name] = df[name].astype("string")
                except Exception as e:
                    self.log.warning(f"âš ï¸ Coercion failed for {name}({typ}): {e}")

            # 5) pandera Column ì •ì˜ (nullable ë°˜ì˜)
            columns = {
                c["name"]: Column(
                    type_map.get(c["type"], pa.String),
                    nullable=c.get("nullable", True),
                    coerce=True,  # ì»¬ëŸ¼ ë‹¨ìœ„ coerce
                )
                for c in schema_def.get("columns", [])
            }

            # 6) ìŠ¤í‚¤ë§ˆ ê²€ì¦ (ìŠ¤í‚¤ë§ˆ ë ˆë²¨ coerce ì¶”ê°€)
            schema = DataFrameSchema(columns, coerce=True)
            schema.validate(df, lazy=True)

            # 7) ì¶”ê°€ constraints (íŒ¨í„´/NOT NULL ë“±)
            cons = schema_def.get("constraints", {})
            if "patterns" in cons:
                for col, pattern in cons["patterns"].items():
                    if col in df.columns:
                        invalid_mask = df[col].dropna().astype(str).str.match(pattern) == False
                        if invalid_mask.any():
                            bad = df.loc[invalid_mask, col].head(5).tolist()
                            raise ValueError(f"âŒ Pattern mismatch in '{col}': samples={bad}")

            if "non_nullable" in cons:
                for col in cons["non_nullable"]:
                    if col in df.columns and df[col].isna().any():
                        cnt = int(df[col].isna().sum())
                        raise ValueError(f"âŒ Null values found in non-nullable column '{col}' (rows={cnt})")

            return {"passed": True, "message": "Pandera schema validation passed"}

        except Exception as e:
            # ë” í’ë¶€í•œ ë””ë²„ê¹… ì •ë³´ë¥¼ ë¡œê·¸ì— ë‚¨ê¹€
            self.log.error(f"[Pandera] {e}")
            # ì‹¤íŒ¨ ì»¬ëŸ¼ ìœ„ì£¼ë¡œ ê°„ë‹¨ í”„ë¡œíŒŒì¼
            try:
                debug_cols = [c["name"] for c in schema_def.get("columns", [])]
                preview = df[debug_cols].head(3).to_dict(orient="records")
                self.log.error(f"[Pandera] Sample rows: {preview}")
            except Exception:
                pass
            return {"passed": False, "message": str(e)}

    # -------------------------------------------------------------------------
    # 5ï¸âƒ£ Soda Core Validation (DuckDB)
    # -------------------------------------------------------------------------
    def _run_soda_duckdb_validation(self, df: pd.DataFrame, soda_path: Path) -> Dict[str, Any]:

        checks = {}
        db_path = None
        tmp_config_path = None
        scan = Scan()
        try:

            # ğŸ” ë””ë²„ê¹…: DuckDB ë“±ë¡ ì „ ì»¬ëŸ¼ëª… í™•ì¸
            # self.log.info("=" * 80)
            # self.log.info(f"ğŸ” DuckDB ë“±ë¡ ì§ì „ ì»¬ëŸ¼ëª… (ì²˜ìŒ 10ê°œ):")
            # self.log.info(df.columns.tolist()[:10])
            # self.log.info(f"ğŸ” General ê´€ë ¨ ì»¬ëŸ¼:")
            # self.log.info([c for c in df.columns if 'General' in c][:10])
            # self.log.info("=" * 80)

            with tempfile.NamedTemporaryFile(suffix=".duckdb", delete=False) as db_file:
                db_path = db_file.name
            if os.path.exists(db_path):
                os.unlink(db_path)

            con = duckdb.connect(database=db_path)
            con.execute(f"CREATE TABLE {self.domain} AS SELECT * FROM df")
            con.close()

            del con

            tmp_config = {
                "data_source my_duckdb": {"type": "duckdb", "path": db_path}
            }

            with tempfile.NamedTemporaryFile(mode="w", suffix=".yml", delete=False) as tmp_file:
                yaml.dump(tmp_config, tmp_file)
                tmp_config_path = tmp_file.name

            scan.set_data_source_name("my_duckdb")
            scan.add_configuration_yaml_file(tmp_config_path)
            scan.add_sodacl_yaml_files(str(soda_path))

            scan_start = datetime.now(timezone.utc)
            exit_code = scan.execute()
            scan_end = datetime.now(timezone.utc)

            checks["soda_core"] = {
                "passed": exit_code == 0,
                "message": "All Soda rules passed" if exit_code == 0 else "Some Soda rules failed",
                "execution_time": (scan_end - scan_start).total_seconds(),
            }

        except Exception as e:
            msg = f"Soda Core validation error: {e}"
            checks["soda_core"] = {"passed": False, "message": msg}
            raise RuntimeError(msg)


        finally:
            for path in [tmp_config_path, db_path, f"{db_path}.wal"]:
                if path and os.path.exists(path):
                    os.unlink(path)
            del scan
            gc.collect()

        return checks

    def _flatten_json_column(self, df: pd.DataFrame) -> pd.DataFrame:
        dict_cols = [c for c in df.columns if df[c].apply(lambda x: isinstance(x, dict)).any()]
        for col in dict_cols:
            expanded = pd.json_normalize(df[col])
            expanded.columns = [f"{col}.{sub}" for sub in expanded.columns]
            df = pd.concat([df.drop(columns=[col]), expanded], axis=1)
        # ğŸ” recursive flatten
        if any(df[c].apply(lambda x: isinstance(x, dict)).any() for c in df.columns):
            return self._flatten_json_column(df)
        return df

    # -------------------------------------------------------------------------
    # 6ï¸âƒ£ Save Validation Result
    # -------------------------------------------------------------------------
    def _save_result(self, result: dict, df: pd.DataFrame) -> Path:
        """
        âœ… ëª¨ë“  ë„ë©”ì¸ ê³µí†µ ì €ì¥ ë¡œì§
        - ê¸°ë³¸ì ìœ¼ë¡œ parquet ì €ì¥
        - validated ë””ë ‰í† ë¦¬ êµ¬ì¡°: data_root/validated/domain_group/domain/vendor=.../exchange_code=.../trd_dt=...
        """

        validated_dir = (
                self.data_root
                / "validated"
                / self.domain_group
                / self.domain
                / f"vendor={self.vendor}"
                / f"exchange_code={self.exchange_code}"
                / f"trd_dt={self.trd_dt}"
        )
        validated_dir.mkdir(parents=True, exist_ok=True)

        # âœ… parquet ì €ì¥
        parquet_path = validated_dir / f"{self.domain}.parquet"
        df.to_parquet(parquet_path, index=False)
        self.log.info(f"âœ… Parquet ì €ì¥ ì™„ë£Œ: {parquet_path} ({len(df):,}í–‰)")

        # âœ… _last_validated.json ìƒì„±
        meta_path = validated_dir / "_last_validated.json"
        with open(meta_path, "w", encoding="utf-8") as f:
            json.dump(result, f, indent=2, ensure_ascii=False)

        return validated_dir

    # -------------------------------------------------------------------------
    # 7ï¸âƒ£ Utility
    # -------------------------------------------------------------------------
    def _aggregate_status(self, checks: Dict[str, Any]) -> str:
        if not checks:
            return "skipped"
        if any(not c.get("passed", False) for c in checks.values()):
            return "failed"
        return "success"


    # -------------------------------------------------------------------------
    # âœ… ìµœì‹  snapshot ë©”íƒ€ ì—…ë°ì´íŠ¸
    # -------------------------------------------------------------------------
    def _update_latest_snapshot_meta(self, domain: str, trd_dt: str, meta_file: str):
        """
        âœ… ìµœì‹  ë©”íƒ€íŒŒì¼ ì—…ë°ì´íŠ¸ (Lake / Warehouse ëª¨ë‘ ì§€ì›)
        - backfill ê³ ë ¤: ê¸°ì¡´ ë‚ ì§œë³´ë‹¤ ìµœì‹ ì¼ ê²½ìš°ë§Œ ê°±ì‹ 
        - ì—¬ëŸ¬ í”„ë¡œì„¸ìŠ¤ ì ‘ê·¼ ì‹œ file lockìœ¼ë¡œ ë™ì‹œì„± ì œì–´
        """

        # âœ… layer êµ¬ë¶„: lake â†’ LATEST_VALIDATED_META / warehouse â†’ LATEST_SNAPSHOT_META
        if getattr(self, "layer", "lake") == "lake":
            meta_path = C.LATEST_VALIDATED_META_PATH
            lock_path = C.LATEST_VALIDATED_META_LOCK
        else:
            meta_path = C.LATEST_SNAPSHOT_META_PATH
            lock_path = C.LATEST_SNAPSHOT_META_LOCK

        meta_path.parent.mkdir(parents=True, exist_ok=True)

        with FileLock(str(lock_path)):
            if meta_path.exists():
                try:
                    with open(meta_path, "r", encoding="utf-8") as f:
                        latest_meta = json.load(f)
                except json.JSONDecodeError:
                    latest_meta = {}
            else:
                latest_meta = {}

            prev_info = latest_meta.get(domain)
            prev_dt = prev_info.get("latest_trd_dt") if prev_info else None

            if (not prev_dt) or (trd_dt > prev_dt):
                latest_meta[domain] = {
                    "latest_trd_dt": trd_dt,
                    "meta_file": meta_file
                }
                with open(meta_path, "w", encoding="utf-8") as f:
                    json.dump(latest_meta, f, indent=2, ensure_ascii=False)
                print(f"ğŸ§­ [UPDATED] latest_meta ({self.layer}): {domain} â†’ {trd_dt}")
            else:
                print(f"â„¹ï¸ Skipped meta update for {domain} ({self.layer}) (existing={prev_dt}, new={trd_dt})")
