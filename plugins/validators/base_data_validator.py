"""
Base Data Validator (Layer Unified with Vendor Support)
--------------------------------------------------------
âœ… ê¸°ëŠ¥ ìš”ì•½
1ï¸âƒ£ Lake / Warehouse / Mart ê³µí†µ Pandera + Soda Core(DuckDB) ê²€ì¦ ì—”ì§„
2ï¸âƒ£ ëª¨ë“  ê²½ë¡œëŠ” DataPathResolver ê¸°ë°˜ (constants.py ë‹¨ìˆœí™”)
3ï¸âƒ£ ê²€ì¦ ì‹¤íŒ¨ ì‹œ Airflow Task ì‹¤íŒ¨ + validated ê²½ë¡œì— ë©”íƒ€íŒŒì¼ ì €ì¥
"""

import os, yaml, tempfile, gc, json, logging, duckdb
import pandas as pd
import pandera.pandas as pa
from pandera import DataFrameSchema, Column
from pathlib import Path
from datetime import datetime, timezone
from typing import Dict, Any, Optional
from soda.scan import Scan
from filelock import FileLock
from plugins.config import constants as C
from plugins.config.constants import WAREHOUSE_DOMAINS
from plugins.utils.path_manager import DataPathResolver  # âœ… ìƒˆ ê²½ë¡œ ê´€ë¦¬ ìœ í‹¸

class BaseDataValidator:
    def __init__(
        self,
        domain: str,
        layer: str,
        trd_dt: Optional[str] = None,
        vendor: Optional[str] = "eodhd",
        exchange_code: Optional[str] = None,
        country_code: Optional[str] = None,
        allow_empty: bool = False,
        domain_group: Optional[str] = None,
        **kwargs,
    ):
        self.domain = domain
        self.domain_group = domain_group or "equity"
        self.layer = layer.lower()
        self.trd_dt = trd_dt
        self.vendor = vendor
        self.exchange_code = exchange_code or "ALL"
        self.country_code = country_code
        self.allow_empty = allow_empty
        self.log = logging.getLogger(f"{__name__}.{self.__class__.__name__}")

        # ---------------------------------------------------------------------
        # âœ… ê²½ë¡œ ìë™ êµ¬ì„± (DataPathResolver ì‚¬ìš©)
        # ---------------------------------------------------------------------
        if self.layer == "lake":
            self.data_root = C.DATA_LAKE_ROOT
            self.schema_root = C.VALIDATOR_SCHEMA_LAKE / self.domain_group / vendor.lower()
            self.check_root = C.VALIDATOR_CHECKS_LAKE / self.domain_group / vendor.lower()

            # ì›ì²œ(raw) ë°ì´í„° ì½ê¸°ìš©
            self.dataset_path = DataPathResolver.lake_raw(
                self.domain_group, self.domain, self.vendor, self.exchange_code, self.trd_dt
            )

            # ê²€ì¦ ê²°ê³¼(validated) ì €ì¥ìš©
            self.validated_dir = DataPathResolver.lake_validated(
                self.domain_group, self.domain, self.vendor, self.exchange_code, self.trd_dt
            )

        elif self.layer == "warehouse":
            self.data_root = C.DATA_WAREHOUSE_ROOT
            self.schema_root = C.VALIDATOR_SCHEMA_WAREHOUSE / self.domain_group
            self.check_root = C.VALIDATOR_CHECKS_WAREHOUSE / self.domain_group

            # Warehouse ìŠ¤ëƒ…ìƒ· ì½ê¸°ìš©
            self.dataset_path = DataPathResolver.warehouse_snapshot(
                self.domain_group, self.domain, self.country_code, self.trd_dt
            )

            # ê²€ì¦ ê²°ê³¼(validated) ì €ì¥ìš©
            self.validated_dir = DataPathResolver.warehouse_validated(
                self.domain_group, self.domain, self.country_code, self.trd_dt
            )

        else:
            raise ValueError(f"Unsupported layer type: {self.layer}")

        self.log.info(f"ğŸ“¦ Validator dataset path: {self.dataset_path}")
        self.log.info(f"ğŸ“ Validation results will be saved in: {self.validated_dir}")


    def _aggregate_status(self, checks: Dict[str, Any]) -> str:
        if not checks:
            return "skipped"
        if any(not c.get("passed", False) for c in checks.values()):
            return "failed"
        return "success"

    # -------------------------------------------------------------------------
    # 1ï¸âƒ£ Main Validation
    # -------------------------------------------------------------------------
    def validate(self, context: Optional[dict] = None) -> Dict[str, Any]:
        df = self._load_dataset()

        # âœ… ë°ì´í„° ì—†ì„ ê²½ìš° ì²˜ë¦¬
        if df.empty:
            if self.allow_empty:
                result = {
                    "dataset": self.domain,
                    "layer": self.layer,
                    "vendor": self.vendor,
                    "exchange_code": self.exchange_code,
                    "country_code": self.country_code,
                    "trd_dt": self.trd_dt,
                    "status": "skipped",
                    "record_count": 0,
                    "checks": {},
                    "validated_source": str(self.dataset_path),
                    "validated_at": datetime.now(timezone.utc).isoformat(),
                    "message": "No data found (allow_empty=True)",
                }
                validated_dir = self._save_result(result, df)
                self.log.info(f"ğŸ§¾ Skipped validation saved: {validated_dir}")
                return result
            else:
                raise ValueError(f"âŒ No data found for {self.domain} (allow_empty=False)")

        # âœ… Pandera + Soda ê²€ì¦ ìˆ˜í–‰
        checks = self._define_checks(df)
        status = self._aggregate_status(checks)

        result = {
            "dataset": self.domain,
            "layer": self.layer,
            "vendor": self.vendor,
            "country_code": self.country_code,
            "exchange_code": self.exchange_code,
            "trd_dt": self.trd_dt,
            "status": status,
            "record_count": len(df),
            "checks": checks,
            "validated_source": str(self.dataset_path),
            "validated_at": datetime.now(timezone.utc).isoformat(),
        }

        # âœ… ê²°ê³¼ ì €ì¥
        validated_dir = self._save_result(result, df)
        if status != "success":
            raise ValueError(f"âŒ Validation failed â€” see {validated_dir}/_last_validated.json")

        # âœ… ë©”íƒ€ ê°±ì‹ 
        self._update_latest_snapshot_meta(self.domain, self.trd_dt, str(self.dataset_path))
        self.log.info(f"âœ… Validation SUCCESS â€” saved to {validated_dir}")
        return result

    # -------------------------------------------------------------------------
    # 2ï¸âƒ£ Dataset Load
    # -------------------------------------------------------------------------
    def _load_dataset(self) -> pd.DataFrame:
        if not self.dataset_path.exists():
            self.log.warning(f"âš ï¸ Dataset path not found: {self.dataset_path}")
            return pd.DataFrame()

        if self.dataset_path.is_dir():
            json_files = [
                f
                for f in list(self.dataset_path.glob("*.json")) + list(self.dataset_path.glob("*.jsonl"))
                if not f.name.startswith("_")  # âœ… ë©”íƒ€ íŒŒì¼ ì œì™¸
            ]

            if not json_files:
                self.log.warning(f"âš ï¸ No JSON files in {self.dataset_path}")
                raise

            dfs = []
            for f in json_files:
                try:
                    if f.suffix == ".jsonl":
                        df_flat = pd.read_json(f, lines=True, dtype=False)
                    else:
                        data = json.load(open(f, "r", encoding="utf-8"))
                        if isinstance(data, list):
                            df_flat = pd.json_normalize(data, sep="_")
                        else:
                            df_flat = pd.json_normalize([data], sep="_")

                    # âœ… DataFrame ì¡´ì¬ ì—¬ë¶€ëŠ” emptyë¡œ ì²´í¬í•´ì•¼ í•¨
                    if df_flat is not None and not df_flat.empty:
                        df_flat = df_flat.dropna(how="all").reset_index(drop=True)
                        dfs.append(df_flat)
                    else:
                        self.log.warning(f"âš ï¸ {f.name} has no valid rows")

                except Exception as e:
                    self.log.warning(f"âš ï¸ Failed to load {f.name}: {e}")

            if not dfs:
                self.log.warning(f"âš ï¸ No valid dataframes loaded from {self.dataset_path}")
                return pd.DataFrame()

            combined = pd.concat(dfs, ignore_index=True)
            self.log.info(f"âœ… Loaded {len(combined)} rows after filtering invalid JSON files")
            return combined

        # âœ… ë‹¨ì¼ parquet íŒŒì¼ ì²˜ë¦¬
        if self.dataset_path.suffix.lower() == ".parquet":
            df = pd.read_parquet(self.dataset_path)
            df = df.dropna(how="all").reset_index(drop=True)
            return df

        raise ValueError(f"âŒ Unsupported file type: {self.dataset_path.suffix}")

    # -------------------------------------------------------------------------
    # 3ï¸âƒ£ Validation Logic (Pandera + Soda)
    # -------------------------------------------------------------------------
    def _define_checks(self, df: pd.DataFrame) -> Dict[str, Any]:
        checks = {}

        schema_path = self.schema_root / f"{self.domain}.json"
        soda_path = self.check_root / f"{self.domain}.yml"

        if self.layer == 'warehouse':
            schema_path = self.schema_root / f"{WAREHOUSE_DOMAINS[self.domain]}.json"
            soda_path = self.check_root / f"{WAREHOUSE_DOMAINS[self.domain]}.yml"

        # Pandera
        if schema_path.exists():
            with open(schema_path, "r", encoding="utf-8") as f:
                schema_def = json.load(f)
            checks["pandera"] = self._validate_with_pandera(df, schema_def)
        else:
            self.log.warning(f"âš ï¸ Pandera schema not found: {schema_path}")

        # Soda
        if soda_path.exists():
            checks["soda_core"] = self._run_soda_duckdb_validation(df, soda_path)
        else:
            self.log.warning(f"âš ï¸ Soda YAML not found: {soda_path}")

        return checks

    # -------------------------------------------------------------------------
    # 4ï¸âƒ£ Pandera Validation
    # -------------------------------------------------------------------------
    def _validate_with_pandera(self, df: pd.DataFrame, schema_def: dict) -> Dict[str, Any]:
        # 1) ìŠ¤í‚¤ë§ˆ â†’ pandera dtype ë§¤í•‘
        type_map = {
            "String": pa.String,
            "Int": pa.Int,
            "Float": pa.Float,
            "DateTime": pa.DateTime,
            "Datetime": pa.DateTime,
            "Bool": pa.Bool,
        }

        try:
            # 2) í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
            defined_cols = [c["name"] for c in schema_def.get("columns", [])]
            missing_cols = [c for c in defined_cols if c not in df.columns]
            if missing_cols:
                raise ValueError(f"âŒ Missing required columns in dataset: {missing_cols}")

            # 3) ì‚¬ì „ ì •ê·œí™”: ê³µë°±â†’NaN, ë¬¸ìì—´ íŠ¸ë¦¼
            for c in df.columns:
                if df[c].dtype == object:
                    df[c] = df[c].apply(lambda x: x.strip() if isinstance(x, str) else x)
                    df[c] = df[c].replace({"": None})

            # 4) ìŠ¤í‚¤ë§ˆ ê¸°ë°˜ íƒ€ì… ê°•ì œ(coerce)
            #    - ìˆ«ì/ë‚ ì§œê°€ ë¬¸ìì—´ì´ì–´ë„ ì˜¬ë°”ë¥¸ dtypeìœ¼ë¡œ ë³€í™˜
            for col_def in schema_def.get("columns", []):
                name = col_def["name"]
                typ = (col_def.get("type") or "String").lower()
                if name not in df.columns:
                    continue
                try:
                    if typ in ("float",):
                        df[name] = pd.to_numeric(df[name], errors="coerce")
                    elif typ in ("int",):
                        df[name] = pd.to_numeric(df[name], errors="coerce").astype("Int64")
                    elif typ in ("datetime", "datetime"):
                        # ë‚ ì§œëŠ” í‘œì¤€ YYYY-MM-DD, YYYY-MM-DDTHH:MM:SS ëª¨ë‘ í—ˆìš©
                        df[name] = pd.to_datetime(df[name], errors="coerce", utc=False)
                    elif typ in ("bool",):
                        # "true"/"false"/1/0 ë“± ë¬¸ìì—´ë„ ì²˜ë¦¬
                        df[name] = df[name].astype(str).str.lower().map(
                            {"true": True, "false": False, "1": True, "0": False}
                        ).astype("boolean")
                    else:
                        df[name] = df[name].astype("string")
                except Exception as e:
                    self.log.warning(f"âš ï¸ Coercion failed for {name}({typ}): {e}")

            # 5) pandera Column ì •ì˜ (nullable ë°˜ì˜)
            columns = {
                c["name"]: Column(
                    type_map.get(c["type"], pa.String),
                    nullable=c.get("nullable", True),
                    coerce=True,  # ì»¬ëŸ¼ ë‹¨ìœ„ coerce
                )
                for c in schema_def.get("columns", [])
            }

            # 6) ìŠ¤í‚¤ë§ˆ ê²€ì¦ (ìŠ¤í‚¤ë§ˆ ë ˆë²¨ coerce ì¶”ê°€)
            schema = DataFrameSchema(columns, coerce=True)
            schema.validate(df, lazy=True)

            # 7) ì¶”ê°€ constraints (íŒ¨í„´/NOT NULL ë“±)
            cons = schema_def.get("constraints", {})
            if "patterns" in cons:
                for col, pattern in cons["patterns"].items():
                    if col in df.columns:
                        invalid_mask = df[col].dropna().astype(str).str.match(pattern) == False
                        if invalid_mask.any():
                            bad = df.loc[invalid_mask, col].head(5).tolist()
                            raise ValueError(f"âŒ Pattern mismatch in '{col}': samples={bad}")

            if "non_nullable" in cons:
                for col in cons["non_nullable"]:
                    if col in df.columns and df[col].isna().any():
                        cnt = int(df[col].isna().sum())
                        raise ValueError(f"âŒ Null values found in non-nullable column '{col}' (rows={cnt})")

            return {"passed": True, "message": "Pandera schema validation passed"}

        except Exception as e:
            # ë” í’ë¶€í•œ ë””ë²„ê¹… ì •ë³´ë¥¼ ë¡œê·¸ì— ë‚¨ê¹€
            self.log.error(f"[Pandera] {e}")
            # ì‹¤íŒ¨ ì»¬ëŸ¼ ìœ„ì£¼ë¡œ ê°„ë‹¨ í”„ë¡œíŒŒì¼
            try:
                debug_cols = [c["name"] for c in schema_def.get("columns", [])]
                preview = df[debug_cols].head(3).to_dict(orient="records")
                self.log.error(f"[Pandera] Sample rows: {preview}")
            except Exception:
                pass
            return {"passed": False, "message": str(e)}

    # -------------------------------------------------------------------------
    # 5ï¸âƒ£ Soda Validation
    # -------------------------------------------------------------------------
    def _run_soda_duckdb_validation(self, df: pd.DataFrame, soda_path: Path) -> Dict[str, Any]:
        checks = {}
        db_path = None
        tmp_config_path = None
        scan = Scan()

        try:
            # âœ… ë°ì´í„°ê°€ ë¹„ì–´ìˆìœ¼ë©´ Soda ìŠ¤í‚µ
            if df is None or df.empty:
                self.log.warning("âš ï¸ Skip Soda validation â€” dataframe is empty.")
                return {"passed": True, "message": "No data to validate (skipped)"}

            # âœ… ì„ì‹œ DuckDB íŒŒì¼ ìƒì„±
            with tempfile.NamedTemporaryFile(suffix=".duckdb", delete=False) as db_file:
                db_path = db_file.name
            if os.path.exists(db_path):
                os.unlink(db_path)

            con = duckdb.connect(database=db_path)

            # âœ… Pandas DataFrameì„ DuckDB í…Œì´ë¸”ë¡œ ì•ˆì „í•˜ê²Œ ë“±ë¡
            con.register("df_view", df)
            con.execute(f"CREATE TABLE {self.domain} AS SELECT * FROM df_view")
            con.unregister("df_view")
            con.close()

            # âœ… Soda ì„¤ì • íŒŒì¼ ì„ì‹œ ìƒì„±
            tmp_config = {"data_source my_duckdb": {"type": "duckdb", "path": db_path}}
            with tempfile.NamedTemporaryFile(mode="w", suffix=".yml", delete=False) as tmp_file:
                yaml.dump(tmp_config, tmp_file)
                tmp_config_path = tmp_file.name

            # âœ… Soda ì‹¤í–‰
            scan.set_data_source_name("my_duckdb")
            scan.add_configuration_yaml_file(tmp_config_path)
            scan.add_sodacl_yaml_files(str(soda_path))

            start = datetime.now(timezone.utc)
            exit_code = scan.execute()
            end = datetime.now(timezone.utc)

            checks["passed"] = exit_code == 0
            checks["message"] = "All Soda rules passed" if exit_code == 0 else "Some Soda rules failed"
            checks["execution_time"] = (end - start).total_seconds()

        except Exception as e:
            msg = f"Soda validation error: {e}"
            self.log.error(msg)
            checks = {"passed": False, "message": msg}

        finally:
            for p in [tmp_config_path, db_path, f"{db_path}.wal"]:
                if p and os.path.exists(p):
                    os.unlink(p)
            del scan
            gc.collect()

        return checks

    # -------------------------------------------------------------------------
    # 6ï¸âƒ£ ê²°ê³¼ ì €ì¥
    # -------------------------------------------------------------------------
    def _save_result(self, result: dict, df: pd.DataFrame) -> Path:
        self.validated_dir.mkdir(parents=True, exist_ok=True)
        df.to_parquet(self.validated_dir / f"{self.domain}.parquet", index=False)
        with open(self.validated_dir / "_last_validated.json", "w", encoding="utf-8") as f:
            json.dump(result, f, indent=2, ensure_ascii=False)
        self.log.info(f"âœ… Validation results saved â†’ {self.validated_dir}")
        return self.validated_dir

    # -------------------------------------------------------------------------
    # 7ï¸âƒ£ Meta update
    # -------------------------------------------------------------------------
    def _update_latest_snapshot_meta(self, domain: str, trd_dt: str, meta_file: str):
        meta_path = C.LATEST_VALIDATED_META_PATH if self.layer == "lake" else C.LATEST_SNAPSHOT_META_PATH
        lock_path = C.LATEST_VALIDATED_META_LOCK if self.layer == "lake" else C.LATEST_SNAPSHOT_META_LOCK
        meta_path.parent.mkdir(parents=True, exist_ok=True)

        with FileLock(str(lock_path)):
            latest_meta = json.load(open(meta_path, "r")) if meta_path.exists() else {}
            prev_info = latest_meta.get(domain)
            prev_dt = prev_info.get("latest_trd_dt") if prev_info else None
            if (not prev_dt) or (trd_dt > prev_dt):
                latest_meta[domain] = {"latest_trd_dt": trd_dt, "meta_file": meta_file}
                json.dump(latest_meta, open(meta_path, "w"), indent=2, ensure_ascii=False)
                self.log.info(f"ğŸ§­ Updated latest meta for {domain}: {trd_dt}")
            else:
                self.log.info(f"â„¹ï¸ Skipped meta update for {domain} (existing={prev_dt}, new={trd_dt})")
