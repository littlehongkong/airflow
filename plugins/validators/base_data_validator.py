"""
Base Data Validator (Layer Unified with Vendor Support)
--------------------------------------------------------
‚úÖ Í∏∞Îä• ÏöîÏïΩ
1Ô∏è‚É£ Lake / Warehouse / Mart Í≥µÌÜµ Pandera + Soda Core(DuckDB) Í≤ÄÏ¶ù ÏóîÏßÑ
2Ô∏è‚É£ Î™®Îì† Í≤ΩÎ°úÎäî constants.py Í∏∞Î∞ò (vendor Ìè¨Ìï®)
3Ô∏è‚É£ Í≤ÄÏ¶ù Ïã§Ìå® Ïãú Airflow Task Ïã§Ìå® + validated Í≤ΩÎ°úÏóê Î©îÌÉÄÌååÏùº Ï†ÄÏû•
"""

import os, yaml, tempfile
from pathlib import Path
from datetime import datetime, timezone
from typing import Dict, Any, Optional
import pandas as pd
import pandera.pandas as pa
from pandera import DataFrameSchema, Column
from soda.scan import Scan
from filelock import FileLock
import logging
import gc
import json
import psutil
import duckdb
import re
# ‚úÖ import Í≤ΩÎ°úÎ•º ÌòÑÏû¨ Íµ¨Ï°∞Ïóê ÎßûÍ≤å ÏàòÏ†ï
from plugins.config import constants as C



class BaseDataValidator:
    def __init__(
        self,
        domain: str,
        layer: str,
        trd_dt: Optional[str] = None,
        dataset_path: Optional[str] = None,
        vendor: Optional[str] = "eodhd",
        exchange_code: Optional[str] = "ALL",
        allow_empty: bool = False,
        domain_group: Optional[str] = None,
        **kwargs,
    ):
        self.domain = domain
        self.domain_group = domain_group
        self.layer = layer.lower()
        self.trd_dt = trd_dt
        self.vendor = vendor
        self.exchange_code = exchange_code
        self.allow_empty = allow_empty
        self.dataset_path = Path(dataset_path)
        self.log = logging.getLogger(f"{__name__}.{self.__class__.__name__}")

        # ‚úÖ constants Í∏∞Î∞ò Í≤ΩÎ°ú ÏÑ§Ï†ï
        if self.layer == "lake":
            self.schema_root = C.VALIDATOR_SCHEMA_LAKE / domain_group / vendor.lower()
            self.check_root = C.VALIDATOR_CHECKS_LAKE / domain_group / vendor.lower()
            self.data_root = C.DATA_LAKE_ROOT

        elif self.layer == "warehouse":
            # ‚úÖ equity ÎèÑÎ©îÏù∏ Ìè¥ÎçîÎ•º Ìè¨Ìï®ÌïòÎèÑÎ°ù ÏàòÏ†ï
            self.schema_root = C.VALIDATOR_SCHEMA_WAREHOUSE / domain_group
            self.check_root = C.VALIDATOR_CHECKS_WAREHOUSE / domain_group
            self.data_root = C.DATA_WAREHOUSE_ROOT
        #
        # else:
        #     # ‚úÖ mart ÎèÑ ÎèôÏùºÌïòÍ≤å ÎèÑÎ©îÏù∏ Îã®ÏúÑ Ìè¥Îçî Ï∂îÍ∞Ä
        #     self.schema_root = VALIDATOR_SCHEMA_MART / domain_group
        #     self.check_root = VALIDATOR_CHECKS_MART / domain_group
        #     self.data_root = DATA_MART_ROOT
    # -------------------------------------------------------------------------
    # 1Ô∏è‚É£ Main Validation
    # -------------------------------------------------------------------------
    def validate(self, context: Optional[dict] = None) -> Dict[str, Any]:
        df = self._load_dataset()

        if df.empty:
            if self.allow_empty:
                print(f"‚úÖ No data found for {self.domain} (allow_empty=True) ‚Üí SKIP validation.")

                result = {
                    "dataset": self.domain,
                    "layer": self.layer,
                    "vendor": self.vendor,
                    "exchange_code": getattr(self, "exchange_code", None),
                    "trd_dt": self.trd_dt,
                    "status": "skipped",
                    "record_count": 0,
                    "checks": {},
                    "validated_source": str(self.dataset_path),
                    "validated_at": datetime.now(timezone.utc).isoformat(),
                    "message": "No data found (allow_empty=True)",
                }

                validated_dir = self._save_result(result, df)
                print(f"üßæ Skipped validation result saved: {validated_dir}")
                return result

            else:
                raise ValueError(f"‚ùå No data found for {self.domain} (allow_empty=False)")

        checks = self._define_checks(df)
        status = self._aggregate_status(checks)

        result = {
            "dataset": self.domain,
            "layer": self.layer,
            "vendor": self.vendor,
            "trd_dt": self.trd_dt,
            "status": status,
            "record_count": len(df),
            "checks": checks,
            "validated_source": str(self.dataset_path),
            "validated_at": datetime.now(timezone.utc).isoformat(),
        }

        validated_dir = self._save_result(result, df)
        if status != "success":
            raise ValueError(f"‚ùå Validation failed ‚Äî see {validated_dir}/_last_validated.json")

        meta_file_path = str(self.dataset_path)
        self._update_latest_snapshot_meta(self.domain, self.trd_dt, meta_file_path)

        print(f"‚úÖ Validation SUCCESS ‚Äî saved to {validated_dir}")
        return result

    # -------------------------------------------------------------------------
    # 2Ô∏è‚É£ Load Dataset
    # -------------------------------------------------------------------------
    def _load_dataset(self) -> pd.DataFrame:
        """
        ‚úÖ Îç∞Ïù¥ÌÑ∞ÏÖã Î°úÎçî (ÎåÄÏö©Îüâ ÏïàÏ†Ñ Î≤ÑÏ†Ñ)
        - fundamentals: ÌååÏùºÎ≥Ñ Ïä§Ìä∏Î¶¨Î∞ç Í≤ÄÏ¶ù ÏßÄÏõê
        - Í∏∞ÌÉÄ: parquet/jsonl Îã®Ïùº ÌååÏùº Î°úÎìú
        """


        if self.dataset_path.is_dir():
            files = list(self.dataset_path.glob("*.json"))
            if not files:
                raise FileNotFoundError(f"‚ùå {self.dataset_path} ÎÇ¥Ïóê JSON ÌååÏùºÏù¥ ÏóÜÏäµÎãàÎã§.")

            dfs = []
            for f in files:
                try:
                    # ‚úÖ JSON ÏùΩÍ∏∞
                    with open(f, "r", encoding="utf-8") as fp:
                        data = json.load(fp)

                    # ‚úÖ flatten Ï≤òÎ¶¨ - sep='_' ÏÇ¨Ïö©!
                    df_flat = pd.json_normalize(data, sep='_')

                    dfs.append(df_flat)
                except Exception as e:
                    self.log.warning(f"‚ö†Ô∏è {f.name} Î°úÎìú Ïã§Ìå®: {e}")
                    continue

            combined = pd.concat(dfs, ignore_index=True)

            # ‚úÖ ÌòπÏãú Î™®Î•º . Ï†úÍ±∞ (2Ï∞® ÏïàÏ†ÑÏû•Ïπò)
            combined.columns = [c.replace(".", "_") for c in combined.columns]

            # ‚úÖ Í≤ÄÏ¶ù
            assert all("." not in col for col in combined.columns), \
                f"‚ùå Ïª¨ÎüºÎ™ÖÏóê Ïó¨Ï†ÑÌûà . Ï°¥Ïû¨: {[c for c in combined.columns if '.' in c][:5]}"

            self.log.info(f"üîç Î≥ÄÌôòÎêú Ïª¨Îüº ÏÉòÌîå: {list(combined.columns[:10])}")
            self.log.info(f"‚úÖ Flatten ÏôÑÎ£å: {len(combined):,}Ìñâ, {len(combined.columns)}Ïó¥")

            return combined


        # ‚úÖ Îã®Ïùº ÌååÏùº Ï≤òÎ¶¨
        if self.dataset_path is None or not self.dataset_path.exists():
            return pd.DataFrame()

        ext = self.dataset_path.suffix.lower()
        if ext == ".parquet":
            return pd.read_parquet(self.dataset_path)
        elif ext in [".json", ".jsonl"]:
            return duckdb.query(f"SELECT * FROM read_json_auto('{self.dataset_path}')").to_df()
        else:
            raise ValueError(f"‚ùå Unsupported file format: {ext}")

    # -------------------------------------------------------------------------
    # 3Ô∏è‚É£ Pandera + Soda Core Validation
    # -------------------------------------------------------------------------
    def _define_checks(self, df: pd.DataFrame) -> Dict[str, Any]:
        checks = {}

        # ‚úÖ numeric Ïª¨Îüº ÌÉÄÏûÖ Î≥¥Ï†ï (Pandera Float ÎåÄÏùë)
        for col in ["active_tickers", "previous_day_updated_tickers", "updated_tickers"]:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors="coerce")

        # ‚úÖ Pandera Í≤ÄÏ¶ù
        self.log.info("Pandera Í≤ÄÏ¶ù ÏãúÏûë")
        schema_path = self.schema_root / f"{self.domain}.json"
        if schema_path.exists():
            with open(schema_path, "r", encoding="utf-8") as f:
                schema_def = json.load(f)
            checks["pandera_schema"] = self._validate_with_pandera(df, schema_def)
            self.log.info(f"Pandera Í≤ÄÏ¶ù ÏôÑÎ£å : {checks['pandera_schema']}")
        else:
            print(f"‚ö†Ô∏è Pandera schema not found: {schema_path}")

        # ‚úÖ Soda Core Í≤ÄÏ¶ù (DuckDB)
        if self.domain == "fundamentals":
            # ‚úÖ Type ÌïÑÎìú ÌÉêÏÉâ (General.Type or General.Type.value)
            type_col = None
            for cand in ["General.Type", "Type", "General.Type.value"]:
                if cand in df.columns:
                    type_col = cand
                    break

            # Í∏∞Î≥∏Í∞í Common Stock
            fund_type = "stock"
            if type_col and "ETF" in df[type_col].astype(str).unique():
                fund_type = "etf"

            soda_filename = f"{self.domain}_{fund_type}.yml"
            soda_path = self.check_root / soda_filename

            if soda_path.exists():
                print(f"‚úÖ Using Soda checks: {soda_path.name}")

                # print(df.columns)
                # print(df.head())

                checks.update(self._run_soda_duckdb_validation(df, soda_path))
            else:
                print(f"‚ö†Ô∏è Soda check file not found: {soda_path}")

        else:
            # ‚úÖ ÏùºÎ∞ò ÎèÑÎ©îÏù∏ Ï≤òÎ¶¨
            soda_path = self.check_root / f"{self.domain}.yml"
            if soda_path.exists():
                checks.update(self._run_soda_duckdb_validation(df, soda_path))
            else:
                print(f"‚ö†Ô∏è Soda check file not found: {soda_path}")

        return checks

    # -------------------------------------------------------------------------
    # 4Ô∏è‚É£ Pandera Validation
    # -------------------------------------------------------------------------
    def _validate_with_pandera(self, df: pd.DataFrame, schema_def: dict) -> Dict[str, Any]:
        # 1) Ïä§ÌÇ§Îßà ‚Üí pandera dtype Îß§Ìïë
        type_map = {
            "String": pa.String,
            "Int": pa.Int,
            "Float": pa.Float,
            "DateTime": pa.DateTime,
            "Datetime": pa.DateTime,
            "Bool": pa.Bool,
        }

        try:
            # 2) ÌïÑÏàò Ïª¨Îüº ÌôïÏù∏
            defined_cols = [c["name"] for c in schema_def.get("columns", [])]
            missing_cols = [c for c in defined_cols if c not in df.columns]
            if missing_cols:
                raise ValueError(f"‚ùå Missing required columns in dataset: {missing_cols}")

            # 3) ÏÇ¨Ï†Ñ Ï†ïÍ∑úÌôî: Í≥µÎ∞±‚ÜíNaN, Î¨∏ÏûêÏó¥ Ìä∏Î¶º
            for c in df.columns:
                if df[c].dtype == object:
                    df[c] = df[c].apply(lambda x: x.strip() if isinstance(x, str) else x)
                    df[c] = df[c].replace({"": None})

            # 4) Ïä§ÌÇ§Îßà Í∏∞Î∞ò ÌÉÄÏûÖ Í∞ïÏ†ú(coerce)
            #    - Ïà´Ïûê/ÎÇ†ÏßúÍ∞Ä Î¨∏ÏûêÏó¥Ïù¥Ïñ¥ÎèÑ Ïò¨Î∞îÎ•∏ dtypeÏúºÎ°ú Î≥ÄÌôò
            for col_def in schema_def.get("columns", []):
                name = col_def["name"]
                typ = (col_def.get("type") or "String").lower()
                if name not in df.columns:
                    continue
                try:
                    if typ in ("float",):
                        df[name] = pd.to_numeric(df[name], errors="coerce")
                    elif typ in ("int",):
                        df[name] = pd.to_numeric(df[name], errors="coerce").astype("Int64")
                    elif typ in ("datetime", "datetime"):
                        # ÎÇ†ÏßúÎäî ÌëúÏ§Ä YYYY-MM-DD, YYYY-MM-DDTHH:MM:SS Î™®Îëê ÌóàÏö©
                        df[name] = pd.to_datetime(df[name], errors="coerce", utc=False)
                    elif typ in ("bool",):
                        # "true"/"false"/1/0 Îì± Î¨∏ÏûêÏó¥ÎèÑ Ï≤òÎ¶¨
                        df[name] = df[name].astype(str).str.lower().map(
                            {"true": True, "false": False, "1": True, "0": False}
                        ).astype("boolean")
                    else:
                        df[name] = df[name].astype("string")
                except Exception as e:
                    self.log.warning(f"‚ö†Ô∏è Coercion failed for {name}({typ}): {e}")

            # 5) pandera Column Ï†ïÏùò (nullable Î∞òÏòÅ)
            columns = {
                c["name"]: Column(
                    type_map.get(c["type"], pa.String),
                    nullable=c.get("nullable", True),
                    coerce=True,  # Ïª¨Îüº Îã®ÏúÑ coerce
                )
                for c in schema_def.get("columns", [])
            }

            # 6) Ïä§ÌÇ§Îßà Í≤ÄÏ¶ù (Ïä§ÌÇ§Îßà Î†àÎ≤® coerce Ï∂îÍ∞Ä)
            schema = DataFrameSchema(columns, coerce=True)
            schema.validate(df, lazy=True)

            # 7) Ï∂îÍ∞Ä constraints (Ìå®ÌÑ¥/NOT NULL Îì±)
            cons = schema_def.get("constraints", {})
            if "patterns" in cons:
                for col, pattern in cons["patterns"].items():
                    if col in df.columns:
                        invalid_mask = df[col].dropna().astype(str).str.match(pattern) == False
                        if invalid_mask.any():
                            bad = df.loc[invalid_mask, col].head(5).tolist()
                            raise ValueError(f"‚ùå Pattern mismatch in '{col}': samples={bad}")

            if "non_nullable" in cons:
                for col in cons["non_nullable"]:
                    if col in df.columns and df[col].isna().any():
                        cnt = int(df[col].isna().sum())
                        raise ValueError(f"‚ùå Null values found in non-nullable column '{col}' (rows={cnt})")

            return {"passed": True, "message": "Pandera schema validation passed"}

        except Exception as e:
            # Îçî ÌíçÎ∂ÄÌïú ÎîîÎ≤ÑÍπÖ Ï†ïÎ≥¥Î•º Î°úÍ∑∏Ïóê ÎÇ®ÍπÄ
            self.log.error(f"[Pandera] {e}")
            # Ïã§Ìå® Ïª¨Îüº ÏúÑÏ£ºÎ°ú Í∞ÑÎã® ÌîÑÎ°úÌååÏùº
            try:
                debug_cols = [c["name"] for c in schema_def.get("columns", [])]
                preview = df[debug_cols].head(3).to_dict(orient="records")
                self.log.error(f"[Pandera] Sample rows: {preview}")
            except Exception:
                pass
            return {"passed": False, "message": str(e)}

    # -------------------------------------------------------------------------
    # 5Ô∏è‚É£ Soda Core Validation (DuckDB)
    # -------------------------------------------------------------------------
    def _run_soda_duckdb_validation(self, df: pd.DataFrame, soda_path: Path) -> Dict[str, Any]:

        checks = {}
        db_path = None
        tmp_config_path = None
        scan = Scan()
        try:

            # üîç ÎîîÎ≤ÑÍπÖ: DuckDB Îì±Î°ù Ï†Ñ Ïª¨ÎüºÎ™Ö ÌôïÏù∏
            # self.log.info("=" * 80)
            # self.log.info(f"üîç DuckDB Îì±Î°ù ÏßÅÏ†Ñ Ïª¨ÎüºÎ™Ö (Ï≤òÏùå 10Í∞ú):")
            # self.log.info(df.columns.tolist()[:10])
            # self.log.info(f"üîç General Í¥ÄÎ†® Ïª¨Îüº:")
            # self.log.info([c for c in df.columns if 'General' in c][:10])
            # self.log.info("=" * 80)

            with tempfile.NamedTemporaryFile(suffix=".duckdb", delete=False) as db_file:
                db_path = db_file.name
            if os.path.exists(db_path):
                os.unlink(db_path)

            con = duckdb.connect(database=db_path)
            con.execute(f"CREATE TABLE {self.domain} AS SELECT * FROM df")
            con.close()

            del con

            tmp_config = {
                "data_source my_duckdb": {"type": "duckdb", "path": db_path}
            }

            with tempfile.NamedTemporaryFile(mode="w", suffix=".yml", delete=False) as tmp_file:
                yaml.dump(tmp_config, tmp_file)
                tmp_config_path = tmp_file.name

            scan.set_data_source_name("my_duckdb")
            scan.add_configuration_yaml_file(tmp_config_path)
            scan.add_sodacl_yaml_files(str(soda_path))

            scan_start = datetime.now(timezone.utc)
            exit_code = scan.execute()
            scan_end = datetime.now(timezone.utc)

            checks["soda_core"] = {
                "passed": exit_code == 0,
                "message": "All Soda rules passed" if exit_code == 0 else "Some Soda rules failed",
                "execution_time": (scan_end - scan_start).total_seconds(),
            }

        except Exception as e:
            msg = f"Soda Core validation error: {e}"
            checks["soda_core"] = {"passed": False, "message": msg}
            raise RuntimeError(msg)


        finally:
            for path in [tmp_config_path, db_path, f"{db_path}.wal"]:
                if path and os.path.exists(path):
                    os.unlink(path)
            del scan
            gc.collect()

        return checks

    def _flatten_json_column(self, df: pd.DataFrame) -> pd.DataFrame:
        dict_cols = [c for c in df.columns if df[c].apply(lambda x: isinstance(x, dict)).any()]
        for col in dict_cols:
            expanded = pd.json_normalize(df[col])
            expanded.columns = [f"{col}.{sub}" for sub in expanded.columns]
            df = pd.concat([df.drop(columns=[col]), expanded], axis=1)
        # üîÅ recursive flatten
        if any(df[c].apply(lambda x: isinstance(x, dict)).any() for c in df.columns):
            return self._flatten_json_column(df)
        return df

    # -------------------------------------------------------------------------
    # 6Ô∏è‚É£ Save Validation Result
    # -------------------------------------------------------------------------
    def _save_result(self, result: dict, df: pd.DataFrame) -> Path:
        """
        ‚úÖ Î™®Îì† ÎèÑÎ©îÏù∏ Í≥µÌÜµ Ï†ÄÏû• Î°úÏßÅ
        - Í∏∞Î≥∏Ï†ÅÏúºÎ°ú parquet Ï†ÄÏû•
        - validated ÎîîÎ†âÌÜ†Î¶¨ Íµ¨Ï°∞: data_root/validated/domain_group/domain/vendor=.../exchange_code=.../trd_dt=...
        """

        validated_dir = (
                self.data_root
                / "validated"
                / self.domain_group
                / self.domain
                / f"vendor={self.vendor}"
                / f"exchange_code={self.exchange_code}"
                / f"trd_dt={self.trd_dt}"
        )
        validated_dir.mkdir(parents=True, exist_ok=True)

        # ‚úÖ parquet Ï†ÄÏû•
        parquet_path = validated_dir / f"{self.domain}.parquet"
        df.to_parquet(parquet_path, index=False)
        self.log.info(f"‚úÖ Parquet Ï†ÄÏû• ÏôÑÎ£å: {parquet_path} ({len(df):,}Ìñâ)")

        # ‚úÖ _last_validated.json ÏÉùÏÑ±
        meta_path = validated_dir / "_last_validated.json"
        with open(meta_path, "w", encoding="utf-8") as f:
            json.dump(result, f, indent=2, ensure_ascii=False)

        return validated_dir

    # -------------------------------------------------------------------------
    # 7Ô∏è‚É£ Utility
    # -------------------------------------------------------------------------
    def _aggregate_status(self, checks: Dict[str, Any]) -> str:
        if not checks:
            return "skipped"
        if any(not c.get("passed", False) for c in checks.values()):
            return "failed"
        return "success"


    # -------------------------------------------------------------------------
    # ‚úÖ ÏµúÏã† snapshot Î©îÌÉÄ ÏóÖÎç∞Ïù¥Ìä∏
    # -------------------------------------------------------------------------
    def _update_latest_snapshot_meta(self, domain: str, trd_dt: str, meta_file: str):
        """
        ‚úÖ ÏµúÏã† Ïä§ÎÉÖÏÉ∑ Î©îÌÉÄÌååÏùº ÏóÖÎç∞Ïù¥Ìä∏
        - backfill Í≥†Î†§: Í∏∞Ï°¥ ÎÇ†ÏßúÎ≥¥Îã§ ÏµúÏã†Ïùº Í≤ΩÏö∞Îßå Í∞±Ïã†
        - Ïó¨Îü¨ ÌîÑÎ°úÏÑ∏Ïä§ Ï†ëÍ∑º Ïãú file lockÏúºÎ°ú ÎèôÏãúÏÑ± Ï†úÏñ¥
        """

        C.LATEST_SNAPSHOT_META_PATH.parent.mkdir(parents=True, exist_ok=True)
        with FileLock(str(C.LATEST_SNAPSHOT_META_LOCK)):
            # Í∏∞Ï°¥ Î©îÌÉÄÌååÏùº Î°úÎìú
            if C.LATEST_SNAPSHOT_META_PATH.exists():
                try:
                    with open(C.LATEST_SNAPSHOT_META_PATH, "r", encoding="utf-8") as f:
                        latest_meta = json.load(f)
                except json.JSONDecodeError:
                    latest_meta = {}
            else:
                latest_meta = {}

            prev_info = latest_meta.get(domain)
            prev_dt = prev_info.get("latest_trd_dt") if prev_info else None

            # Ïù¥Ï†Ñ ÎÇ†ÏßúÎ≥¥Îã§ ÏµúÏã†Ïù¥Î©¥ Í∞±Ïã†
            if (not prev_dt) or (trd_dt > prev_dt):
                latest_meta[domain] = {
                    "latest_trd_dt": trd_dt,
                    "meta_file": meta_file
                }
                with open(C.LATEST_SNAPSHOT_META_PATH, "w", encoding="utf-8") as f:
                    json.dump(latest_meta, f, indent=2, ensure_ascii=False)
                print(f"üß≠ [UPDATED] latest_snapshot_meta: {domain} ‚Üí {trd_dt}")
            else:
                print(f"‚ÑπÔ∏è Skipped meta update for {domain} (existing={prev_dt}, new={trd_dt})")