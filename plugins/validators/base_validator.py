# plugins/validators/base_validator.py
import os
import json
import pandas as pd
import pandera.pandas as pa
from soda.scan import Scan
import tempfile
import yaml
import duckdb
from typing import Optional, Tuple, Any, Dict
from pandera import DataFrameSchema
from plugins.pipelines.base_equity_pipeline import LOCAL_DATA_LAKE_PATH
from datetime import datetime, timezone

class BaseDataValidator:
    """
    ëª¨ë“  ë°ì´í„°ì…‹ ê²€ì¦ì˜ ë² ì´ìŠ¤ í´ë˜ìŠ¤ (ì¬ì‚¬ìš©ì„± ê·¹ëŒ€í™” ë²„ì „)
    ------------------------------------------------------------
    âœ… Pandera + Soda Core ì§€ì›
    âœ… Hive íŒŒí‹°ì…˜ ê²½ë¡œ ìë™ íƒìƒ‰
    âœ… layer ì „í™˜(raw â†’ validated ë“±)
    âœ… Exchange / Date ê¸°ë°˜ ê²½ë¡œ ìë™ ê²°ì •
    """

    # ê° validatorë³„ë¡œ override í•´ì•¼ í•¨
    schema: Optional[DataFrameSchema] = None
    soda_check_file: Optional[str] = None  # ì˜ˆ: "/opt/airflow/plugins/soda/checks/equity_price_checks.yml"

    def __init__(self, exchange_code: str, trd_dt: str, data_domain: str, layer: str = "raw", **kwargs):
        self.exchange_code = exchange_code
        self.trd_dt = trd_dt
        self.layer = layer
        self.data_domain=data_domain

        # âœ… DAGì—ì„œ ë„˜ê²¨ì£¼ëŠ” ì¶”ê°€ ì¸ì ì²˜ë¦¬
        self.allow_empty = kwargs.get("allow_empty", False)

    # ----------------------------------------------------------------------
    # âœ… 1ï¸âƒ£ Data Lake ê²½ë¡œ í—¬í¼
    # ----------------------------------------------------------------------
    def _get_lake_path(self, layer: Optional[str] = None) -> str:
        """Hive-style ê²½ë¡œ ë°˜í™˜ (/data_lake/{layer}/{domain}/exchange_code=..../trd_dt=...)"""
        layer = layer or self.layer
        base_path = os.path.join(
            LOCAL_DATA_LAKE_PATH,
            "data_lake",
            layer,
            self.data_domain,
            f"exchange_code={self.exchange_code}",
            f"trd_dt={self.trd_dt}"
        )
        os.makedirs(base_path, exist_ok=True)
        return base_path


    # ----------------------------------------------------------------------
    # âœ… 3ï¸âƒ£ Pandera ê²€ì¦
    # ----------------------------------------------------------------------
    def _run_pandera(self, df: pd.DataFrame) -> Tuple[bool, Any]:
        if not self.schema:
            raise NotImplementedError("í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ schemaë¥¼ ì •ì˜í•´ì•¼ í•©ë‹ˆë‹¤.")

        try:
            self.schema.validate(df, lazy=True)
            return True, None
        except pa.errors.SchemaErrors as err:
            return False, err.failure_cases

    # ----------------------------------------------------------------------
    # âœ… 4ï¸âƒ£ Soda ê²€ì¦
    # ----------------------------------------------------------------------
    def _run_soda(self, layer: str = "raw", dag_run_id: str = None, task_id: str = None, allow_empty: bool = False) -> Dict:
        """Soda Core ê²€ì¦ë§Œ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ ë°˜í™˜"""

        self.soda_check_file = self._get_soda_check_path()
        if not self.soda_check_file or not os.path.exists(self.soda_check_file):
            print("âš ï¸ Soda ê²€ì¦ì„ ê±´ë„ˆëœë‹ˆë‹¤ (ì²´í¬íŒŒì¼ ì—†ìŒ).")
            return {"status": "skipped", "reason": "no_check_file"}

        # âœ… _get_lake_path()ê°€ ì´ë¯¸ íŒŒí‹°ì…˜ ê²½ë¡œê¹Œì§€ í¬í•¨í•˜ë¯€ë¡œ íŒŒì¼ëª…ë§Œ ì¶”ê°€
        raw_dataset_path = os.path.join(
            self._get_lake_path(layer),
            f"{self.data_domain}.jsonl"
        )

        # âœ… ë°ì´í„° íŒŒì¼ ì¡´ì¬ í™•ì¸
        if not os.path.exists(raw_dataset_path):
            msg = f"âš ï¸ ê²€ì¦ ëŒ€ìƒ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {raw_dataset_path}"
            if allow_empty:
                print(msg + " â†’ allow_empty=True, skip ì²˜ë¦¬")
                return {"status": "skipped", "reason": "no_data_file"}
            raise FileNotFoundError(msg)


        # âœ… íŒŒì¼ í¬ê¸° 0ì¸ ê²½ìš°
        if os.path.getsize(raw_dataset_path) == 0:
            msg = f"âš ï¸ íŒŒì¼ ë¹„ì–´ ìˆìŒ: {raw_dataset_path}"
            if allow_empty:
                print(msg + " â†’ allow_empty=True, skip ì²˜ë¦¬")
                return {"status": "skipped", "reason": "empty_file"}
            raise ValueError(msg)


        # âœ… JSONL ì²«ì¤„ ê²€ì‚¬
        try:
            with open(raw_dataset_path, "r") as f:
                first_line = next(f, None)
                if not first_line:
                    msg = f"âš ï¸ ë°ì´í„° ë‚´ìš© ì—†ìŒ: {raw_dataset_path}"
                    if allow_empty:
                        print(msg + " â†’ allow_empty=True, skip ì²˜ë¦¬")
                        return {"status": "skipped", "reason": "empty_content"}
                    raise ValueError(msg)
        except Exception as e:
            if allow_empty:
                print(f"âš ï¸ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜ ({e}) â†’ skip ì²˜ë¦¬")
                return {"status": "skipped", "reason": "read_error"}
            raise

        return self._execute_soda(raw_dataset_path, layer, dag_run_id, task_id)

        # ----------------------------------------------------------------------
        # âœ… Soda ê²€ì¦ ì‹¤í–‰ (ê¸°ì¡´ ë¡œì§ ê·¸ëŒ€ë¡œ ë³„ë„ í•¨ìˆ˜ë¡œ ë¶„ë¦¬)
        # ----------------------------------------------------------------------
    def _execute_soda(self, raw_dataset_path: str, layer: str, dag_run_id: str, task_id: str):

        tmp_config = {
            "data_source my_duckdb": {
                "type": "duckdb",
                "path": ":memory:",
            }
        }

        with tempfile.NamedTemporaryFile(mode="w", suffix=".yml", delete=False) as tmp_file:
            yaml.dump(tmp_config, tmp_file)
            tmp_config_path = tmp_file.name

        try:
            scan = Scan()
            scan.set_data_source_name("my_duckdb")
            scan.add_configuration_yaml_file(tmp_config_path)
            scan.add_sodacl_yaml_files(self.soda_check_file)

            data_source = scan._data_source_manager.get_data_source("my_duckdb")
            data_source.connection.execute(
                f"CREATE TABLE {self.data_domain} AS SELECT * FROM read_json_auto('{raw_dataset_path}')"
            )

            scan_start_time = datetime.now(timezone.utc)
            exit_code = scan.execute()
            scan_end_time = datetime.now(timezone.utc)

            # ê²€ì¦ ê²°ê³¼ ìˆ˜ì§‘
            validation_result = {
                "scan_metadata": {
                    "dataset": self.data_domain,
                    "layer": layer,
                    "source_path": raw_dataset_path,
                    "scan_timestamp": scan_start_time.isoformat(),
                    "scan_duration_seconds": (scan_end_time - scan_start_time).total_seconds(),
                    "exit_code": exit_code,
                    "dag_run_id": dag_run_id,
                    "task_id": task_id,
                },
                "checks": [],
                "errors": [],
                "summary": {
                    "total_checks": 0,
                    "passed": 0,
                    "failed": 0,
                    "warned": 0,
                    "errored": 0,
                },
                "final_status": "pending",
                "log_file": None,
            }

            has_failures = False
            has_errors = False

            if hasattr(scan, '_checks'):
                for check in scan._checks:
                    validation_result["summary"]["total_checks"] += 1

                    # âœ… CheckOutcome enumì„ ë¬¸ìì—´ë¡œ ë³€í™˜
                    outcome_value = getattr(check, 'outcome', 'unknown')
                    if hasattr(outcome_value, 'value'):
                        # Enum íƒ€ì…ì¸ ê²½ìš° .valueë¡œ ë¬¸ìì—´ ì¶”ì¶œ
                        outcome_str = outcome_value.value
                    else:
                        # ì´ë¯¸ ë¬¸ìì—´ì¸ ê²½ìš°
                        outcome_str = str(outcome_value)

                    check_info = {
                        "name": getattr(check, 'name', 'Unknown check'),
                        "outcome": outcome_str,  # âœ… ë¬¸ìì—´ë¡œ ì €ì¥
                    }

                    if outcome_str == 'pass':
                        validation_result["summary"]["passed"] += 1
                    elif outcome_str == 'fail':
                        validation_result["summary"]["failed"] += 1
                        has_failures = True
                        print(f"  âŒ FAILED: {check_info['name']}")
                    elif outcome_str == 'warn':
                        validation_result["summary"]["warned"] += 1

                    if hasattr(check, 'check_value'):
                        check_value = getattr(check, 'check_value', None)
                        # âœ… check_valueë„ JSON ì§ë ¬í™” ê°€ëŠ¥í•˜ë„ë¡ ë³€í™˜
                        if check_value is not None:
                            try:
                                json.dumps(check_value)  # ì§ë ¬í™” ê°€ëŠ¥ ì—¬ë¶€ í…ŒìŠ¤íŠ¸
                                check_info["check_value"] = check_value
                            except (TypeError, ValueError):
                                check_info["check_value"] = str(check_value)

                    validation_result["checks"].append(check_info)

            if hasattr(scan, '_logs') and hasattr(scan._logs, 'logs'):
                for log in scan._logs.logs:
                    if log.level == 'error':
                        has_errors = True
                        validation_result["summary"]["errored"] += 1
                        error_info = {
                            "level": log.level,
                            "message": log.message,
                        }
                        validation_result["errors"].append(error_info)
                        print(f"  âš ï¸ ERROR: {log.message}")

            # âœ… validated ë©”íƒ€ë°ì´í„° ë””ë ‰í† ë¦¬ (íŒŒí‹°ì…˜ ì—†ì´ ìµœìƒìœ„ì— ì €ì¥)
            validated_base = os.path.join(LOCAL_DATA_LAKE_PATH, "data_lake", "validated")
            validated_metadata_dir = os.path.join(
                validated_base,
                "_metadata",
                "validation_logs"
            )
            os.makedirs(validated_metadata_dir, exist_ok=True)

            # ë¡œê·¸ íŒŒì¼ëª… ìƒì„±
            timestamp_str = scan_start_time.strftime('%Y%m%d_%H%M%S')
            if dag_run_id:
                safe_dag_run_id = dag_run_id.replace(':', '-').replace('+', '_')
                log_filename = f"{self.data_domain}_{timestamp_str}_{safe_dag_run_id}.json"
            else:
                log_filename = f"{self.data_domain}_{timestamp_str}.json"

            validation_log_file = os.path.join(validated_metadata_dir, log_filename)

            # ì‹¤íŒ¨/ì—ëŸ¬ íŒë‹¨
            validation_result["final_status"] = "success"
            validation_result["log_file"] = validation_log_file

            if exit_code >= 2 or has_failures:
                validation_result["final_status"] = "failed"
                with open(validation_log_file, 'w', encoding='utf-8') as f:
                    json.dump(validation_result, f, indent=2, ensure_ascii=False)
                print(f"ğŸ“„ ê²€ì¦ ì‹¤íŒ¨ ë¡œê·¸ ì €ì¥: {validation_log_file}")
                raise AssertionError(f"âŒ Soda ê²€ì¦ ì‹¤íŒ¨ (exit_code: {exit_code})")

            if exit_code == 3 or has_errors:
                validation_result["final_status"] = "error"
                with open(validation_log_file, 'w', encoding='utf-8') as f:
                    json.dump(validation_result, f, indent=2, ensure_ascii=False)
                print(f"ğŸ“„ ê²€ì¦ ì—ëŸ¬ ë¡œê·¸ ì €ì¥: {validation_log_file}")
                raise AssertionError(f"âŒ Soda ê²€ì¦ ì¤‘ ì—ëŸ¬ ë°œìƒ (exit_code: {exit_code})")

            if exit_code == 1:
                validation_result["final_status"] = "warning"
                print(f"âš ï¸ Soda ê²€ì¦ ê²½ê³  ìˆìŒ (exit_code: {exit_code}) - ê³„ì† ì§„í–‰")

            # ê²€ì¦ ë¡œê·¸ ì €ì¥
            with open(validation_log_file, 'w', encoding='utf-8') as f:
                json.dump(validation_result, f, indent=2, ensure_ascii=False)
            print(f"ğŸ“„ ê²€ì¦ ë¡œê·¸ ì €ì¥: {validation_log_file}")
            print(f"âœ… Soda ê²€ì¦ í†µê³¼ (exit_code: {exit_code})")

            return validation_result

        finally:
            if os.path.exists(tmp_config_path):
                os.unlink(tmp_config_path)

    # ----------------------------------------------------------------------
    # âœ… 5ï¸âƒ£ ì „ì²´ ì‹¤í–‰
    # ----------------------------------------------------------------------
    def run(self, context: Dict = None, allow_empty: bool = False) -> Dict:
        """Airflow DAG ë‚´ì—ì„œ ì‹¤í–‰ë˜ëŠ” ë°ì´í„° ê²€ì¦ ì‹¤í–‰"""

        # --- context íŒŒì‹± ---
        airflow_ctx = context.get("context") if isinstance(context, dict) else None
        dag_run_id = airflow_ctx.get("run_id")
        task_id = getattr(airflow_ctx.get("task_instance"), "task_id", None)
        print(f"ğŸ“˜ [Validator Context] dag_run_id={dag_run_id}, task_id={task_id}")

        # --- ê²€ì¦ ì‹¤í–‰ ---
        validation_result = self._run_soda(
            layer="raw",
            dag_run_id=dag_run_id,
            task_id=task_id,
            allow_empty=allow_empty,
        )

        if not validation_result:
            print("âš ï¸ validation_result is None â†’ skip ë°˜í™˜")
            return {
                "data_domain": self.data_domain,
                "exchange_code": self.exchange_code,
                "trd_dt": self.trd_dt,
                "status": "skipped",
                "record_count": 0,
            }

        # --- ê²€ì¦ í†µê³¼ ì‹œ validated ì €ì¥ ---
        self._save_to_validated(validation_result, dag_run_id, task_id)

        # --- row_count ê³„ì‚° ---
        record_count = 0
        try:
            conn = duckdb.connect()
            parquet_path = os.path.join(
                self._get_lake_path("validated"),
                f"{self.data_domain}.parquet"
            )
            result = conn.execute(f"SELECT COUNT(*) FROM read_parquet('{parquet_path}')").fetchone()
            record_count = result[0] if result else 0
            conn.close()
        except Exception as e:
            print(f"âš ï¸ Row count ê³„ì‚° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

        # --- ê³µí†µ ë°˜í™˜ êµ¬ì¡° ---
        return {
            "data_domain": self.data_domain,
            "exchange_code": self.exchange_code,
            "trd_dt": self.trd_dt,
            "status": validation_result.get("final_status", "success"),
            "record_count": record_count,
            "log_file": validation_result.get("log_file"),
        }

    def _save_to_validated(self, validation_result: Dict, dag_run_id: str = None, task_id: str = None) -> None:
        """ê²€ì¦ í†µê³¼ ë°ì´í„°ë¥¼ validated ê³„ì¸µì— Parquetë¡œ ì €ì¥"""

        raw_dataset_path = validation_result["scan_metadata"]["source_path"]

        # âœ… validated ê³„ì¸µ ê²½ë¡œ (íŒŒí‹°ì…˜ í¬í•¨)
        validated_parquet_path = os.path.join(
            self._get_lake_path("validated"),
            f"{self.data_domain}.parquet"
        )

        # âœ… JSONL â†’ Parquet ë³€í™˜ (DuckDB ì‚¬ìš©)
        conn = duckdb.connect(':memory:')
        conn.execute(f"""
            COPY (SELECT * FROM read_json_auto('{raw_dataset_path}'))
            TO '{validated_parquet_path}' (FORMAT PARQUET)
        """)
        conn.close()

        print(f"ğŸ“¦ Validated ë°ì´í„° ì €ì¥: {validated_parquet_path}")

        # _last_validated.json ë©”íƒ€ë°ì´í„° ì €ì¥
        last_validated_meta = {
            "dataset": self.data_domain,
            "last_validated_timestamp": validation_result["scan_metadata"]["scan_timestamp"],
            "validation_log_file": os.path.basename(validation_result.get("log_file", "")),
            "source_file": raw_dataset_path,
            "validated_file": validated_parquet_path,
            "dag_run_id": dag_run_id,
            "task_id": task_id,
            "status": validation_result["final_status"],
            "checks_summary": validation_result["summary"]
        }

        validated_dir = self._get_lake_path("validated")
        last_validated_path = os.path.join(validated_dir, "_last_validated.json")
        with open(last_validated_path, 'w', encoding='utf-8') as f:
            json.dump(last_validated_meta, f, indent=2, ensure_ascii=False)

        print(f"ğŸ“‹ ë©”íƒ€ë°ì´í„° ì €ì¥: {last_validated_path}")

    # âœ…  Soda ì²´í¬íŒŒì¼ ê²½ë¡œ ìë™ ì¶”ë¡ 
    def _get_soda_check_path(self) -> Optional[str]:
        """
        data_domain ê¸°ë°˜ìœ¼ë¡œ Soda ì²´í¬íŒŒì¼ ê²½ë¡œë¥¼ ìë™ íƒìƒ‰í•©ë‹ˆë‹¤.
        ì˜ˆ: equity_prices â†’ /opt/airflow/plugins/soda/checks/equity_prices_checks.yml
        """
        base_dir = "/opt/airflow/plugins/soda/checks"
        file_name = f"{self.data_domain}_checks.yml"
        file_path = os.path.join(base_dir, file_name)

        assert os.path.exists(file_path), f"âš ï¸ Soda ì²´í¬íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {file_path}"

        return file_path