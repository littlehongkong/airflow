# plugins/validators/base_validator.py
import os
import json
import pandas as pd
import pandera.pandas as pa
from soda.scan import Scan
import tempfile
import yaml
import duckdb
from typing import Optional, Tuple, Any, Dict
from pandera import DataFrameSchema
from plugins.pipelines.base_equity_pipeline import LOCAL_DATA_LAKE_PATH
from datetime import datetime, timezone

class BaseDataValidator:
    """
    ëª¨ë“  ë°ì´í„°ì…‹ ê²€ì¦ì˜ ë² ì´ìŠ¤ í´ë˜ìŠ¤ (ì¬ì‚¬ìš©ì„± ê·¹ëŒ€í™” ë²„ì „)
    ------------------------------------------------------------
    âœ… Pandera + Soda Core ì§€ì›
    âœ… Hive íŒŒí‹°ì…˜ ê²½ë¡œ ìë™ íƒìƒ‰
    âœ… layer ì „í™˜(raw â†’ validated ë“±)
    âœ… Exchange / Date ê¸°ë°˜ ê²½ë¡œ ìë™ ê²°ì •
    """

    # ê° validatorë³„ë¡œ override í•´ì•¼ í•¨
    schema: Optional[DataFrameSchema] = None
    soda_check_file: Optional[str] = None  # ì˜ˆ: "/opt/airflow/plugins/soda/checks/equity_price_checks.yml"

    def __init__(self, exchange_code: str, trd_dt: str, data_domain: str, layer: str = "raw"):
        self.exchange_code = exchange_code
        self.trd_dt = trd_dt
        self.layer = layer
        self.data_domain=data_domain

    # ----------------------------------------------------------------------
    # âœ… 1ï¸âƒ£ Data Lake ê²½ë¡œ í—¬í¼
    # ----------------------------------------------------------------------
    def _get_lake_path(self, layer: Optional[str] = None) -> str:
        """Hive-style ê²½ë¡œ ë°˜í™˜ (/data_lake/{layer}/{domain}/exchange_code=..../trd_dt=...)"""
        layer = layer or self.layer
        base_path = os.path.join(
            LOCAL_DATA_LAKE_PATH,
            "data_lake",
            layer,
            self.data_domain,
            f"exchange_code={self.exchange_code}",
            f"trd_dt={self.trd_dt}"
        )
        os.makedirs(base_path, exist_ok=True)
        return base_path

    # ----------------------------------------------------------------------
    # âœ… 2ï¸âƒ£ ë°ì´í„° ë¡œë“œ (ìë™ íŒŒì¼ íƒìƒ‰)
    # ----------------------------------------------------------------------
    def _load_records(self, layer: Optional[str] = None) -> pd.DataFrame:
        """JSONL, JSON, Parquet ìë™ íƒìƒ‰ í›„ ë¡œë“œ"""
        target_dir = self._get_lake_path(layer)
        files = [
            f for f in os.listdir(target_dir)
            if f.endswith((".jsonl", ".parquet", ".json"))
        ]

        if not files:
            raise AssertionError(f"âš ï¸ ê²€ì¦ ëŒ€ìƒ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {target_dir}")

        file_path = os.path.join(target_dir, files[0])
        print(f"ğŸ“‚ ë¡œë“œ íŒŒì¼: {file_path}")

        if file_path.endswith(".jsonl"):
            with open(file_path, "r", encoding="utf-8") as f:
                data = [json.loads(line) for line in f if line.strip()]
        elif file_path.endswith(".json"):
            # í‹°ì»¤ë³„ JSONì¼ ê²½ìš° ì—¬ëŸ¬ ê°œë¥¼ ë³‘í•©
            data = []
            for f_name in files:
                with open(os.path.join(target_dir, f_name), "r", encoding="utf-8") as f:
                    data.append(json.load(f))
        elif file_path.endswith(".parquet"):
            return pd.read_parquet(file_path)
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {file_path}")

        return pd.DataFrame(data)

    # ----------------------------------------------------------------------
    # âœ… 3ï¸âƒ£ Pandera ê²€ì¦
    # ----------------------------------------------------------------------
    def _run_pandera(self, df: pd.DataFrame) -> Tuple[bool, Any]:
        if not self.schema:
            raise NotImplementedError("í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ schemaë¥¼ ì •ì˜í•´ì•¼ í•©ë‹ˆë‹¤.")

        try:
            self.schema.validate(df, lazy=True)
            return True, None
        except pa.errors.SchemaErrors as err:
            return False, err.failure_cases

    # ----------------------------------------------------------------------
    # âœ… 4ï¸âƒ£ Soda ê²€ì¦
    # ----------------------------------------------------------------------
    def _run_soda(self, layer: str = "raw", dag_run_id: str = None, task_id: str = None) -> Dict:
        """Soda Core ê²€ì¦ë§Œ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ ë°˜í™˜"""

        self.soda_check_file = self._get_soda_check_path()
        if not self.soda_check_file or not os.path.exists(self.soda_check_file):
            print("âš ï¸ Soda ê²€ì¦ì„ ê±´ë„ˆëœë‹ˆë‹¤ (ì²´í¬íŒŒì¼ ì—†ìŒ).")
            return None

        # âœ… _get_lake_path()ê°€ ì´ë¯¸ íŒŒí‹°ì…˜ ê²½ë¡œê¹Œì§€ í¬í•¨í•˜ë¯€ë¡œ íŒŒì¼ëª…ë§Œ ì¶”ê°€
        raw_dataset_path = os.path.join(
            self._get_lake_path(layer),
            f"{self.data_domain}.jsonl"
        )

        if not os.path.exists(raw_dataset_path):
            raise FileNotFoundError(f"âš ï¸ ê²€ì¦ ëŒ€ìƒ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {raw_dataset_path}")

        tmp_config = {
            "data_source my_duckdb": {
                "type": "duckdb",
                "path": ":memory:",
            }
        }

        with tempfile.NamedTemporaryFile(mode="w", suffix=".yml", delete=False) as tmp_file:
            yaml.dump(tmp_config, tmp_file)
            tmp_config_path = tmp_file.name

        try:
            scan = Scan()
            scan.set_data_source_name("my_duckdb")
            scan.add_configuration_yaml_file(tmp_config_path)
            scan.add_sodacl_yaml_files(self.soda_check_file)

            data_source = scan._data_source_manager.get_data_source("my_duckdb")
            data_source.connection.execute(
                f"CREATE TABLE {self.data_domain} AS SELECT * FROM read_json_auto('{raw_dataset_path}')"
            )

            scan_start_time = datetime.now(timezone.utc)
            exit_code = scan.execute()
            scan_end_time = datetime.now(timezone.utc)

            # ê²€ì¦ ê²°ê³¼ ìˆ˜ì§‘
            validation_result = {
                "scan_metadata": {
                    "dataset": self.data_domain,
                    "layer": layer,
                    "source_path": raw_dataset_path,
                    "scan_timestamp": scan_start_time.isoformat(),
                    "scan_duration_seconds": (scan_end_time - scan_start_time).total_seconds(),
                    "exit_code": exit_code,
                    "dag_run_id": dag_run_id,
                    "task_id": task_id,
                },
                "checks": [],
                "errors": [],
                "summary": {
                    "total_checks": 0,
                    "passed": 0,
                    "failed": 0,
                    "warned": 0,
                    "errored": 0,
                }
            }

            has_failures = False
            has_errors = False

            if hasattr(scan, '_checks'):
                for check in scan._checks:
                    validation_result["summary"]["total_checks"] += 1

                    # âœ… CheckOutcome enumì„ ë¬¸ìì—´ë¡œ ë³€í™˜
                    outcome_value = getattr(check, 'outcome', 'unknown')
                    if hasattr(outcome_value, 'value'):
                        # Enum íƒ€ì…ì¸ ê²½ìš° .valueë¡œ ë¬¸ìì—´ ì¶”ì¶œ
                        outcome_str = outcome_value.value
                    else:
                        # ì´ë¯¸ ë¬¸ìì—´ì¸ ê²½ìš°
                        outcome_str = str(outcome_value)

                    check_info = {
                        "name": getattr(check, 'name', 'Unknown check'),
                        "outcome": outcome_str,  # âœ… ë¬¸ìì—´ë¡œ ì €ì¥
                    }

                    if outcome_str == 'pass':
                        validation_result["summary"]["passed"] += 1
                    elif outcome_str == 'fail':
                        validation_result["summary"]["failed"] += 1
                        has_failures = True
                        print(f"  âŒ FAILED: {check_info['name']}")
                    elif outcome_str == 'warn':
                        validation_result["summary"]["warned"] += 1

                    if hasattr(check, 'check_value'):
                        check_value = getattr(check, 'check_value', None)
                        # âœ… check_valueë„ JSON ì§ë ¬í™” ê°€ëŠ¥í•˜ë„ë¡ ë³€í™˜
                        if check_value is not None:
                            try:
                                json.dumps(check_value)  # ì§ë ¬í™” ê°€ëŠ¥ ì—¬ë¶€ í…ŒìŠ¤íŠ¸
                                check_info["check_value"] = check_value
                            except (TypeError, ValueError):
                                check_info["check_value"] = str(check_value)

                    validation_result["checks"].append(check_info)

            if hasattr(scan, '_logs') and hasattr(scan._logs, 'logs'):
                for log in scan._logs.logs:
                    if log.level == 'error':
                        has_errors = True
                        validation_result["summary"]["errored"] += 1
                        error_info = {
                            "level": log.level,
                            "message": log.message,
                        }
                        validation_result["errors"].append(error_info)
                        print(f"  âš ï¸ ERROR: {log.message}")

            # âœ… validated ë©”íƒ€ë°ì´í„° ë””ë ‰í† ë¦¬ (íŒŒí‹°ì…˜ ì—†ì´ ìµœìƒìœ„ì— ì €ì¥)
            validated_base = os.path.join(LOCAL_DATA_LAKE_PATH, "data_lake", "validated")
            validated_metadata_dir = os.path.join(
                validated_base,
                "_metadata",
                "validation_logs"
            )
            os.makedirs(validated_metadata_dir, exist_ok=True)

            # ë¡œê·¸ íŒŒì¼ëª… ìƒì„±
            timestamp_str = scan_start_time.strftime('%Y%m%d_%H%M%S')
            if dag_run_id:
                safe_dag_run_id = dag_run_id.replace(':', '-').replace('+', '_')
                log_filename = f"{self.data_domain}_{timestamp_str}_{safe_dag_run_id}.json"
            else:
                log_filename = f"{self.data_domain}_{timestamp_str}.json"

            validation_log_file = os.path.join(validated_metadata_dir, log_filename)

            # ì‹¤íŒ¨/ì—ëŸ¬ íŒë‹¨
            validation_result["final_status"] = "success"
            validation_result["log_file"] = validation_log_file

            if exit_code >= 2 or has_failures:
                validation_result["final_status"] = "failed"
                with open(validation_log_file, 'w', encoding='utf-8') as f:
                    json.dump(validation_result, f, indent=2, ensure_ascii=False)
                print(f"ğŸ“„ ê²€ì¦ ì‹¤íŒ¨ ë¡œê·¸ ì €ì¥: {validation_log_file}")
                raise AssertionError(f"âŒ Soda ê²€ì¦ ì‹¤íŒ¨ (exit_code: {exit_code})")

            if exit_code == 3 or has_errors:
                validation_result["final_status"] = "error"
                with open(validation_log_file, 'w', encoding='utf-8') as f:
                    json.dump(validation_result, f, indent=2, ensure_ascii=False)
                print(f"ğŸ“„ ê²€ì¦ ì—ëŸ¬ ë¡œê·¸ ì €ì¥: {validation_log_file}")
                raise AssertionError(f"âŒ Soda ê²€ì¦ ì¤‘ ì—ëŸ¬ ë°œìƒ (exit_code: {exit_code})")

            if exit_code == 1:
                validation_result["final_status"] = "warning"
                print(f"âš ï¸ Soda ê²€ì¦ ê²½ê³  ìˆìŒ (exit_code: {exit_code}) - ê³„ì† ì§„í–‰")

            # ê²€ì¦ ë¡œê·¸ ì €ì¥
            with open(validation_log_file, 'w', encoding='utf-8') as f:
                json.dump(validation_result, f, indent=2, ensure_ascii=False)
            print(f"ğŸ“„ ê²€ì¦ ë¡œê·¸ ì €ì¥: {validation_log_file}")
            print(f"âœ… Soda ê²€ì¦ í†µê³¼ (exit_code: {exit_code})")

            return validation_result

        finally:
            if os.path.exists(tmp_config_path):
                os.unlink(tmp_config_path)

    # ----------------------------------------------------------------------
    # âœ… 5ï¸âƒ£ ì „ì²´ ì‹¤í–‰
    # ----------------------------------------------------------------------
    def run(self, context: Dict = None) -> None:
        """ì‹¤ì œ ê²€ì¦ ë¡œì§ ì‹¤í–‰"""

        # Airflow contextì—ì„œ DAG ì‹¤í–‰ ì •ë³´ ì¶”ì¶œ
        dag_run_id = None
        task_id = None

        if context:
            dag_run_id = context.get('dag_run').run_id if context.get('dag_run') else None
            task_id = context.get('task_instance').task_id if context.get('task_instance') else None

        # 1. ê²€ì¦ ì‹¤í–‰
        validation_result = self._run_soda(layer="raw", dag_run_id=dag_run_id, task_id=task_id)

        if not validation_result:
            return

        # 2. ê²€ì¦ í†µê³¼ ì‹œ validated ê³„ì¸µìœ¼ë¡œ ì €ì¥ (ê¸°ì¡´ ì½”ë“œ í™œìš©)
        self._save_to_validated(validation_result, dag_run_id, task_id)

    def _save_to_validated(self, validation_result: Dict, dag_run_id: str = None, task_id: str = None) -> None:
        """ê²€ì¦ í†µê³¼ ë°ì´í„°ë¥¼ validated ê³„ì¸µì— Parquetë¡œ ì €ì¥"""

        raw_dataset_path = validation_result["scan_metadata"]["source_path"]

        # âœ… validated ê³„ì¸µ ê²½ë¡œ (íŒŒí‹°ì…˜ í¬í•¨)
        validated_parquet_path = os.path.join(
            self._get_lake_path("validated"),
            f"{self.data_domain}.parquet"
        )

        # âœ… JSONL â†’ Parquet ë³€í™˜ (DuckDB ì‚¬ìš©)
        conn = duckdb.connect(':memory:')
        conn.execute(f"""
            COPY (SELECT * FROM read_json_auto('{raw_dataset_path}'))
            TO '{validated_parquet_path}' (FORMAT PARQUET)
        """)
        conn.close()

        print(f"ğŸ“¦ Validated ë°ì´í„° ì €ì¥: {validated_parquet_path}")

        # _last_validated.json ë©”íƒ€ë°ì´í„° ì €ì¥
        last_validated_meta = {
            "dataset": self.data_domain,
            "last_validated_timestamp": validation_result["scan_metadata"]["scan_timestamp"],
            "validation_log_file": os.path.basename(validation_result.get("log_file", "")),
            "source_file": raw_dataset_path,
            "validated_file": validated_parquet_path,
            "dag_run_id": dag_run_id,
            "task_id": task_id,
            "status": validation_result["final_status"],
            "checks_summary": validation_result["summary"]
        }

        validated_dir = self._get_lake_path("validated")
        last_validated_path = os.path.join(validated_dir, "_last_validated.json")
        with open(last_validated_path, 'w', encoding='utf-8') as f:
            json.dump(last_validated_meta, f, indent=2, ensure_ascii=False)

        print(f"ğŸ“‹ ë©”íƒ€ë°ì´í„° ì €ì¥: {last_validated_path}")

    # âœ…  Soda ì²´í¬íŒŒì¼ ê²½ë¡œ ìë™ ì¶”ë¡ 
    def _get_soda_check_path(self) -> Optional[str]:
        """
        data_domain ê¸°ë°˜ìœ¼ë¡œ Soda ì²´í¬íŒŒì¼ ê²½ë¡œë¥¼ ìë™ íƒìƒ‰í•©ë‹ˆë‹¤.
        ì˜ˆ: equity_prices â†’ /opt/airflow/plugins/soda/checks/equity_prices_checks.yml
        """
        base_dir = "/opt/airflow/plugins/soda/checks"
        file_name = f"{self.data_domain}_checks.yml"
        file_path = os.path.join(base_dir, file_name)

        assert os.path.exists(file_path), f"âš ï¸ Soda ì²´í¬íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {file_path}"

        return file_path