# plugins/validators/base_validator.py
import os
import json
import pandas as pd
import pandera.pandas as pa
from soda.scan import Scan
import tempfile
import yaml
import duckdb
import logging
from typing import Optional, Tuple, Any, Dict
from pandera import DataFrameSchema
from plugins.config.constants import DATA_LAKE_ROOT
from datetime import datetime, timezone

class BaseDataValidator:
    """
    ëª¨ë“  ë°ì´í„°ì…‹ ê²€ì¦ì˜ ë² ì´ìŠ¤ í´ë˜ìŠ¤ (ì¬ì‚¬ìš©ì„± ê·¹ëŒ€í™” ë²„ì „)
    ------------------------------------------------------------
    âœ… Pandera + Soda Core ì§€ì›
    âœ… Hive íŒŒí‹°ì…˜ ê²½ë¡œ ìë™ íƒìƒ‰
    âœ… layer ì „í™˜(raw â†’ validated ë“±)
    âœ… Exchange / Date ê¸°ë°˜ ê²½ë¡œ ìë™ ê²°ì •
    """

    # ê° validatorë³„ë¡œ override í•´ì•¼ í•¨
    schema: Optional[DataFrameSchema] = None
    soda_check_file: Optional[str] = None  # ì˜ˆ: "/opt/airflow/plugins/soda/checks/equity_price_checks.yml"

    def __init__(self, exchange_code: str, trd_dt: str, data_domain: str, layer: str = "raw", vendor: str = None, **kwargs):
        self.exchange_code = exchange_code
        self.trd_dt = trd_dt
        self.layer = layer
        self.data_domain=data_domain
        self.log = logging.getLogger(f"{__name__}.{self.__class__.__name__}")

        # âœ… DAGì—ì„œ ë„˜ê²¨ì£¼ëŠ” ì¶”ê°€ ì¸ì ì²˜ë¦¬
        self.allow_empty = kwargs.get("allow_empty", False)

        self.vendor = (vendor or "").lower()
        if not self.vendor:
            raise ValueError(
                f"âŒ vendor ê°’ì´ ì§€ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. "
                f"Validator ì´ˆê¸°í™” ì‹œ vendor ì¸ìê°€ í•„ìˆ˜ì…ë‹ˆë‹¤ "
                f"(data_domain={data_domain}, exchange_code={exchange_code})."
            )

    # ----------------------------------------------------------------------
    # âœ… 1ï¸âƒ£ Data Lake ê²½ë¡œ í—¬í¼
    # ----------------------------------------------------------------------
    def _get_lake_path(self, layer: Optional[str] = None) -> str:
        """Hive-style ê²½ë¡œ ë°˜í™˜ (/data_lake/{layer}/{domain}/exchange_code=..../trd_dt=...)"""
        layer = layer or self.layer
        base_path = os.path.join(
            DATA_LAKE_ROOT,
            layer,
            self.data_domain,
            f"vendor={self.vendor}",
            f"exchange_code={self.exchange_code}",
            f"trd_dt={self.trd_dt}"
        )
        os.makedirs(base_path, exist_ok=True)
        return base_path

    # ----------------------------------------------------------------------
    # âœ… 3ï¸âƒ£ Pandera ê²€ì¦
    # ----------------------------------------------------------------------
    def _run_pandera(self, df: pd.DataFrame) -> Tuple[bool, Any]:
        if not self.schema:
            raise NotImplementedError("í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ schemaë¥¼ ì •ì˜í•´ì•¼ í•©ë‹ˆë‹¤.")

        try:
            self.schema.validate(df, lazy=True)
            return True, None
        except pa.errors.SchemaErrors as err:
            return False, err.failure_cases

    # ----------------------------------------------------------------------
    # âœ… 4ï¸âƒ£ Soda ê²€ì¦
    # ----------------------------------------------------------------------
    def _run_soda(self, layer: str = "raw", dag_run_id: str = None, task_id: str = None, allow_empty: bool = False) -> Dict:
        """Soda Core ê²€ì¦ë§Œ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ ë°˜í™˜"""

        self.soda_check_file = self._get_soda_check_path()
        if not self.soda_check_file or not os.path.exists(self.soda_check_file):
            print("âš ï¸ Soda ê²€ì¦ì„ ê±´ë„ˆëœë‹ˆë‹¤ (ì²´í¬íŒŒì¼ ì—†ìŒ).")
            return {"status": "skipped", "reason": "no_check_file"}

        # âœ… _get_lake_path()ê°€ ì´ë¯¸ íŒŒí‹°ì…˜ ê²½ë¡œê¹Œì§€ í¬í•¨í•˜ë¯€ë¡œ íŒŒì¼ëª…ë§Œ ì¶”ê°€
        raw_dataset_path = os.path.join(
            self._get_lake_path(layer),
            f"{self.data_domain}.jsonl"
        )

        # âœ… ë°ì´í„° íŒŒì¼ ì¡´ì¬ í™•ì¸
        if not os.path.exists(raw_dataset_path):
            msg = f"âš ï¸ ê²€ì¦ ëŒ€ìƒ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {raw_dataset_path}"
            if allow_empty:
                print(msg + " â†’ allow_empty=True, skip ì²˜ë¦¬")
                return {"status": "skipped", "reason": "no_data_file"}
            raise FileNotFoundError(msg)


        # âœ… íŒŒì¼ í¬ê¸° 0ì¸ ê²½ìš°
        if os.path.getsize(raw_dataset_path) == 0:
            msg = f"âš ï¸ íŒŒì¼ ë¹„ì–´ ìˆìŒ: {raw_dataset_path}"
            if allow_empty:
                print(msg + " â†’ allow_empty=True, skip ì²˜ë¦¬")
                return {"status": "skipped", "reason": "empty_file"}
            raise ValueError(msg)


        # âœ… JSONL ì²«ì¤„ ê²€ì‚¬
        try:
            with open(raw_dataset_path, "r") as f:
                first_line = next(f, None)
                if not first_line:
                    msg = f"âš ï¸ ë°ì´í„° ë‚´ìš© ì—†ìŒ: {raw_dataset_path}"
                    if allow_empty:
                        print(msg + " â†’ allow_empty=True, skip ì²˜ë¦¬")
                        return {"status": "skipped", "reason": "empty_content"}
                    raise ValueError(msg)
        except Exception as e:
            if allow_empty:
                print(f"âš ï¸ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜ ({e}) â†’ skip ì²˜ë¦¬")
                return {"status": "skipped", "reason": "read_error"}
            raise

        return self._execute_soda(raw_dataset_path, layer, dag_run_id, task_id)

        # ----------------------------------------------------------------------
        # âœ… Soda ê²€ì¦ ì‹¤í–‰ (ê¸°ì¡´ ë¡œì§ ê·¸ëŒ€ë¡œ ë³„ë„ í•¨ìˆ˜ë¡œ ë¶„ë¦¬)
        # ----------------------------------------------------------------------
    def _execute_soda(self, raw_dataset_path: str, layer: str, dag_run_id: str, task_id: str):

        tmp_config = {
            "data_source my_duckdb": {
                "type": "duckdb",
                "path": ":memory:",
            }
        }

        with tempfile.NamedTemporaryFile(mode="w", suffix=".yml", delete=False) as tmp_file:
            yaml.dump(tmp_config, tmp_file)
            tmp_config_path = tmp_file.name

        try:
            scan = Scan()
            scan.set_data_source_name("my_duckdb")
            scan.add_configuration_yaml_file(tmp_config_path)
            scan.add_sodacl_yaml_files(self.soda_check_file)

            data_source = scan._data_source_manager.get_data_source("my_duckdb")
            data_source.connection.execute(
                f"CREATE TABLE {self.data_domain} AS SELECT * FROM read_json_auto('{raw_dataset_path}')"
            )

            scan_start_time = datetime.now(timezone.utc)
            exit_code = scan.execute()
            scan_end_time = datetime.now(timezone.utc)

            # ê²€ì¦ ê²°ê³¼ ìˆ˜ì§‘
            validation_result = {
                "scan_metadata": {
                    "dataset": self.data_domain,
                    "layer": layer,
                    "source_path": raw_dataset_path,
                    "scan_timestamp": scan_start_time.isoformat(),
                    "scan_duration_seconds": (scan_end_time - scan_start_time).total_seconds(),
                    "exit_code": exit_code,
                    "dag_run_id": dag_run_id,
                    "task_id": task_id,
                },
                "checks": [],
                "errors": [],
                "summary": {
                    "total_checks": 0,
                    "passed": 0,
                    "failed": 0,
                    "warned": 0,
                    "errored": 0,
                },
                "final_status": "pending",
                "log_file": None,
            }

            has_failures = False
            has_errors = False

            if hasattr(scan, '_checks'):
                for check in scan._checks:
                    validation_result["summary"]["total_checks"] += 1

                    # âœ… CheckOutcome enumì„ ë¬¸ìì—´ë¡œ ë³€í™˜
                    outcome_value = getattr(check, 'outcome', 'unknown')
                    if hasattr(outcome_value, 'value'):
                        # Enum íƒ€ì…ì¸ ê²½ìš° .valueë¡œ ë¬¸ìì—´ ì¶”ì¶œ
                        outcome_str = outcome_value.value
                    else:
                        # ì´ë¯¸ ë¬¸ìì—´ì¸ ê²½ìš°
                        outcome_str = str(outcome_value)

                    check_info = {
                        "name": getattr(check, 'name', 'Unknown check'),
                        "outcome": outcome_str,  # âœ… ë¬¸ìì—´ë¡œ ì €ì¥
                    }

                    if outcome_str == 'pass':
                        validation_result["summary"]["passed"] += 1
                    elif outcome_str == 'fail':
                        validation_result["summary"]["failed"] += 1
                        has_failures = True
                        print(f"  âŒ FAILED: {check_info['name']}")
                    elif outcome_str == 'warn':
                        validation_result["summary"]["warned"] += 1

                    if hasattr(check, 'check_value'):
                        check_value = getattr(check, 'check_value', None)
                        # âœ… check_valueë„ JSON ì§ë ¬í™” ê°€ëŠ¥í•˜ë„ë¡ ë³€í™˜
                        if check_value is not None:
                            try:
                                json.dumps(check_value)  # ì§ë ¬í™” ê°€ëŠ¥ ì—¬ë¶€ í…ŒìŠ¤íŠ¸
                                check_info["check_value"] = check_value
                            except (TypeError, ValueError):
                                check_info["check_value"] = str(check_value)

                    validation_result["checks"].append(check_info)

            if hasattr(scan, '_logs') and hasattr(scan._logs, 'logs'):
                for log in scan._logs.logs:
                    if log.level == 'error':
                        has_errors = True
                        validation_result["summary"]["errored"] += 1
                        error_info = {
                            "level": log.level,
                            "message": log.message,
                        }
                        validation_result["errors"].append(error_info)
                        print(f"  âš ï¸ ERROR: {log.message}")

            # âœ… validated ë©”íƒ€ë°ì´í„° ë””ë ‰í† ë¦¬ (íŒŒí‹°ì…˜ ì—†ì´ ìµœìƒìœ„ì— ì €ì¥)
            validated_base = os.path.join(DATA_LAKE_ROOT, "validated")
            validated_metadata_dir = os.path.join(
                validated_base,
                "_metadata",
                "validation_logs"
            )
            os.makedirs(validated_metadata_dir, exist_ok=True)

            # ë¡œê·¸ íŒŒì¼ëª… ìƒì„±
            timestamp_str = scan_start_time.strftime('%Y%m%d_%H%M%S')
            if dag_run_id:
                safe_dag_run_id = dag_run_id.replace(':', '-').replace('+', '_')
                log_filename = f"{self.data_domain}_{timestamp_str}_{safe_dag_run_id}.json"
            else:
                log_filename = f"{self.data_domain}_{timestamp_str}.json"

            validation_log_file = os.path.join(validated_metadata_dir, log_filename)

            # ì‹¤íŒ¨/ì—ëŸ¬ íŒë‹¨
            validation_result["final_status"] = "success"
            validation_result["log_file"] = validation_log_file

            if exit_code >= 2 or has_failures:
                validation_result["final_status"] = "failed"
                with open(validation_log_file, 'w', encoding='utf-8') as f:
                    json.dump(validation_result, f, indent=2, ensure_ascii=False)
                print(f"ğŸ“„ ê²€ì¦ ì‹¤íŒ¨ ë¡œê·¸ ì €ì¥: {validation_log_file}")
                raise AssertionError(f"âŒ Soda ê²€ì¦ ì‹¤íŒ¨ (exit_code: {exit_code})")

            if exit_code == 3 or has_errors:
                validation_result["final_status"] = "error"
                with open(validation_log_file, 'w', encoding='utf-8') as f:
                    json.dump(validation_result, f, indent=2, ensure_ascii=False)
                print(f"ğŸ“„ ê²€ì¦ ì—ëŸ¬ ë¡œê·¸ ì €ì¥: {validation_log_file}")
                raise AssertionError(f"âŒ Soda ê²€ì¦ ì¤‘ ì—ëŸ¬ ë°œìƒ (exit_code: {exit_code})")

            if exit_code == 1:
                validation_result["final_status"] = "warning"
                print(f"âš ï¸ Soda ê²€ì¦ ê²½ê³  ìˆìŒ (exit_code: {exit_code}) - ê³„ì† ì§„í–‰")

            # ê²€ì¦ ë¡œê·¸ ì €ì¥
            with open(validation_log_file, 'w', encoding='utf-8') as f:
                json.dump(validation_result, f, indent=2, ensure_ascii=False)
            print(f"ğŸ“„ ê²€ì¦ ë¡œê·¸ ì €ì¥: {validation_log_file}")
            print(f"âœ… Soda ê²€ì¦ í†µê³¼ (exit_code: {exit_code})")

            return validation_result

        finally:
            if os.path.exists(tmp_config_path):
                os.unlink(tmp_config_path)

    # ----------------------------------------------------------------------
    # âœ… 5ï¸âƒ£ ì „ì²´ ì‹¤í–‰
    # ----------------------------------------------------------------------
    def run(self, context: Dict = None, allow_empty: bool = False) -> Dict:
        """Airflow DAG ë‚´ì—ì„œ ì‹¤í–‰ë˜ëŠ” ë°ì´í„° ê²€ì¦ ì‹¤í–‰"""

        airflow_ctx = context.get("context")
        dag_run_id = airflow_ctx.get("run_id")
        task_id = getattr(airflow_ctx.get("task_instance"), "task_id", None)

        print(f"ğŸ“˜ [Validator Context] dag_run_id={dag_run_id}, task_id={task_id}")

        validation_result = self._run_soda(
            layer="raw",
            dag_run_id=dag_run_id,
            task_id=task_id,
            allow_empty=allow_empty,
        )

        # âœ… allow_empty=True ì‹œì—ë„ _last_validated.jsonì€ í•­ìƒ ë‚¨ê¸´ë‹¤.
        if not validation_result or validation_result.get("status") == "skipped":
            print("âš ï¸ ë°ì´í„° ì—†ìŒ ë˜ëŠ” skip ì²˜ë¦¬ë¨ â€” ë¹ˆ ë©”íƒ€ë°ì´í„° ê¸°ë¡ ì¤‘...")

            # ë¹ˆ Parquet íŒŒì¼ë„ í•¨ê»˜ ìƒì„± (schema ìœ ì§€)
            validated_dir = self._get_lake_path("validated")
            validated_parquet_path = os.path.join(
                self._get_lake_path("validated"),  # vendor-aware ê²½ë¡œ ìë™ ìƒì„±
                f"{self.data_domain}.parquet"
            )

            import pyarrow as pa
            import pyarrow.parquet as pq
            empty_table = pa.Table.from_pandas(pd.DataFrame())
            pq.write_table(empty_table, validated_parquet_path)

            print(f"ğŸ“¦ Empty Validated ë°ì´í„° ì €ì¥: {validated_parquet_path}")

            # _last_validated.json ì‘ì„±
            last_validated_meta = {
                "dataset": self.data_domain,
                "vendor": self.vendor,
                "last_validated_timestamp": datetime.now(timezone.utc).isoformat(),
                "validation_log_file": None,
                "source_file": None,
                "validated_file": validated_parquet_path,
                "dag_run_id": dag_run_id,
                "task_id": task_id,
                "status": "skipped",
                "checks_summary": {
                    "total_checks": 0,
                    "passed": 0,
                    "failed": 0,
                    "warned": 0,
                    "errored": 0
                },
                "record_count": 0,
                "reason": validation_result.get("reason", "empty_file" if validation_result else "unknown")
            }

            last_validated_path = os.path.join(validated_dir, "_last_validated.json")
            with open(last_validated_path, 'w', encoding='utf-8') as f:
                json.dump(last_validated_meta, f, indent=2, ensure_ascii=False)

            print(f"ğŸ“‹ Empty ë©”íƒ€ë°ì´í„° ì €ì¥: {last_validated_path}")
            return last_validated_meta

            # --- ê²€ì¦ í†µê³¼ ì‹œ validated ì €ì¥ ---
        self._save_to_validated(validation_result, dag_run_id, task_id)

        # --- row_count ê³„ì‚° ---
        record_count = 0
        try:
            conn = duckdb.connect()
            parquet_path = os.path.join(
                self._get_lake_path("validated"),
                f"{self.data_domain}.parquet"
            )
            result = conn.execute(f"SELECT COUNT(*) FROM read_parquet('{parquet_path}')").fetchone()
            record_count = result[0] if result else 0
            conn.close()
        except Exception as e:
            print(f"âš ï¸ Row count ê³„ì‚° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

        # --- ê³µí†µ ë°˜í™˜ êµ¬ì¡° ---
        return {
            "data_domain": self.data_domain,
            "vendor": self.vendor,
            "exchange_code": self.exchange_code,
            "trd_dt": self.trd_dt,
            "status": validation_result.get("final_status", "success"),
            "record_count": record_count,
            "log_file": validation_result.get("log_file"),
        }

    def _save_to_validated(self, validation_result: Dict, dag_run_id: str = None, task_id: str = None) -> None:
        """ê²€ì¦ í†µê³¼ ë°ì´í„°ë¥¼ validated ê³„ì¸µì— Parquetë¡œ ì €ì¥"""

        raw_dataset_path = validation_result["scan_metadata"]["source_path"]

        # âœ… validated ê³„ì¸µ ê²½ë¡œ (íŒŒí‹°ì…˜ í¬í•¨)
        validated_parquet_path = os.path.join(
            self._get_lake_path("validated"),
            f"{self.data_domain}.parquet"
        )

        # âœ… 1ï¸âƒ£ Raw ë‹¨ê³„ì˜ ì›ì²œ ë©”íƒ€íŒŒì¼ ê²½ë¡œ íƒìƒ‰
        raw_dir = os.path.dirname(raw_dataset_path)
        source_meta_path = os.path.join(raw_dir, "_source_meta.json")
        source_meta_data = None

        if os.path.exists(source_meta_path):
            try:
                with open(source_meta_path, "r", encoding="utf-8") as f:
                    source_meta_data = json.load(f)
            except Exception as e:
                print(f"âš ï¸ _source_meta.json ë¡œë“œ ì‹¤íŒ¨: {e}")

        # âœ… JSONL â†’ Parquet ë³€í™˜ (DuckDB ì‚¬ìš©)
        conn = duckdb.connect(':memory:')
        conn.execute(f"""
            COPY (SELECT * FROM read_json_auto('{raw_dataset_path}'))
            TO '{validated_parquet_path}' (FORMAT PARQUET)
        """)
        conn.close()

        print(f"ğŸ“¦ Validated ë°ì´í„° ì €ì¥: {validated_parquet_path}")

        # _last_validated.json ë©”íƒ€ë°ì´í„° ì €ì¥
        last_validated_meta = {
            "dataset": self.data_domain,
            "last_validated_timestamp": validation_result["scan_metadata"]["scan_timestamp"],
            "validation_log_file": os.path.basename(validation_result.get("log_file", "")),
            "source_file": raw_dataset_path,
            "validated_file": validated_parquet_path,
            "dag_run_id": dag_run_id,
            "task_id": task_id,
            "status": validation_result["final_status"],
            "checks_summary": validation_result["summary"]
        }

        # âœ… 4ï¸âƒ£ source_meta ë³‘í•©
        if source_meta_data:
            last_validated_meta["source_meta"] = source_meta_data
            print("ğŸ”— ì›ì²œ ë©”íƒ€ë°ì´í„° ë³‘í•© ì™„ë£Œ (_source_meta.json â†’ _last_validated.json)")
        else:
            print("âš ï¸ ì›ì²œ ë©”íƒ€ë°ì´í„° íŒŒì¼(_source_meta.json)ì´ ì—†ì–´ ë³‘í•©ì„ ê±´ë„ˆëœ€")

        validated_dir = self._get_lake_path("validated")
        last_validated_path = os.path.join(validated_dir, "_last_validated.json")
        with open(last_validated_path, 'w', encoding='utf-8') as f:
            json.dump(last_validated_meta, f, indent=2, ensure_ascii=False)

        print(f"ğŸ“‹ ë©”íƒ€ë°ì´í„° ì €ì¥: {last_validated_path}")

    # âœ…  Soda ì²´í¬íŒŒì¼ ê²½ë¡œ ìë™ ì¶”ë¡ 
    def _get_soda_check_path(self) -> Optional[str]:
        """
        data_domain ê¸°ë°˜ìœ¼ë¡œ Soda ì²´í¬íŒŒì¼ ê²½ë¡œë¥¼ ìë™ íƒìƒ‰í•©ë‹ˆë‹¤.
        ì˜ˆ: equity_prices â†’ /opt/airflow/plugins/soda/checks/prices_checks.yml
        """
        base_dir = "/opt/airflow/plugins/soda/checks"
        file_name = f"{self.data_domain}_checks.yml"
        file_path = os.path.join(base_dir, file_name)

        assert os.path.exists(file_path), f"âš ï¸ Soda ì²´í¬íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {file_path}"

        return file_path



    def _format_soda_result(
        self,
        scan,
        layer: str,
        dag_run_id: str,
        task_id: str,
        scan_start,
        scan_end,
        exit_code: int,
    ):
        """Soda Scan ê²°ê³¼ë¥¼ í‘œì¤€ í¬ë§·(dict)ìœ¼ë¡œ ì •ë¦¬"""
        has_failures = False
        has_errors = False

        summary = {
            "total_checks": 0,
            "passed": 0,
            "failed": 0,
            "warned": 0,
            "errored": 0,
        }
        checks = []
        errors = []

        # âœ… ì²´í¬ ê²°ê³¼ ì§‘ê³„
        if hasattr(scan, "_checks"):
            for c in scan._checks:
                summary["total_checks"] += 1
                outcome_val = getattr(c, "outcome", "unknown")
                outcome = outcome_val.value if hasattr(outcome_val, "value") else str(outcome_val)

                if outcome == "pass":
                    summary["passed"] += 1
                elif outcome == "fail":
                    summary["failed"] += 1
                    has_failures = True
                elif outcome == "warn":
                    summary["warned"] += 1

                checks.append({
                    "name": getattr(c, "name", "unknown"),
                    "outcome": outcome,
                })

        # âœ… ë¡œê·¸ ì˜¤ë¥˜ ì§‘ê³„
        if hasattr(scan, "_logs") and hasattr(scan._logs, "logs"):
            for lg in scan._logs.logs:
                if lg.level == "error":
                    has_errors = True
                    summary["errored"] += 1
                    errors.append({"level": lg.level, "message": lg.message})

        # âœ… ìµœì¢… ìƒíƒœ ê²°ì •
        final_status = "success"
        if exit_code >= 2 or has_failures:
            final_status = "failed"
        elif exit_code == 3 or has_errors:
            final_status = "error"
        elif exit_code == 1:
            final_status = "warning"

        # âœ… ë¡œê·¸ íŒŒì¼ ê²½ë¡œ ìƒì„±
        validated_base = os.path.join(DATA_LAKE_ROOT, "data_lake", "validated")
        log_dir = os.path.join(validated_base, "_metadata", "validation_logs")
        os.makedirs(log_dir, exist_ok=True)

        ts = scan_start.strftime("%Y%m%d_%H%M%S")
        safe_run_id = (dag_run_id or "").replace(":", "-").replace("+", "_")
        log_name = f"{self.data_domain}_{ts}_{safe_run_id}.json" if safe_run_id else f"{self.data_domain}_{ts}.json"
        log_path = os.path.join(log_dir, log_name)

        # âœ… í‘œì¤€ ë°˜í™˜ dict
        result = {
            "scan_metadata": {
                "dataset": self.data_domain,
                "layer": layer,
                "scan_timestamp": scan_start.isoformat(),
                "scan_duration_seconds": (scan_end - scan_start).total_seconds(),
                "exit_code": exit_code,
                "dag_run_id": dag_run_id,
                "task_id": task_id,
            },
            "checks": checks,
            "errors": errors,
            "summary": summary,
            "final_status": final_status,
            "log_file": log_path,
        }

        # âœ… ë¡œê·¸ íŒŒì¼ ì €ì¥
        with open(log_path, "w", encoding="utf-8") as f:
            json.dump(result, f, indent=2, ensure_ascii=False)

        print(f"ğŸ“‹ Soda ê²€ì¦ ë¡œê·¸ ì €ì¥: {log_path}")

        return result