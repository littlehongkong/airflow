from plugins.validators.base_data_validator import BaseDataValidator
from plugins.config import constants as C
from plugins.utils.name_utils import normalize_field_names   # âœ… ìƒˆ ìœ í‹¸ ì‚¬ìš©
from datetime import datetime, timezone
from typing import Optional, Dict, Any
import pandas as pd, json, logging
import gc
import shutil
import psutil, tracemalloc
import traceback
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock
_parquet_lock = Lock()

process = psutil.Process()
tracemalloc.start()

class FundamentalDataValidator(BaseDataValidator):
    """
    ðŸ§© Fundamental ì „ìš© Validator
    - íŒŒì¼ ë‹¨ìœ„ ê²€ì¦ (OOM ë°©ì§€)
    - ê° JSONë³„ Pandera/Soda ì ìš©
    - validated ê²°ê³¼ JSONL append
    """

    def __init__(
        self,
        trd_dt: str,
        domain: str,
        exchange_code: str,
        vendor: str = "eodhd",
        domain_group: str = "equity",
    ):
        self.dataset_path = (
            C.DATA_LAKE_ROOT
            / "raw"
            / domain_group
            / domain
            / f"vendor={vendor}"
            / f"exchange_code={exchange_code}"
            / f"trd_dt={trd_dt}"
        )

        super().__init__(
            domain=domain,
            layer="lake",
            trd_dt=trd_dt,
            vendor=vendor,
            dataset_path=self.dataset_path,
            exchange_code=exchange_code,
            domain_group=domain_group,
        )


        self.output_dir = (
            C.DATA_LAKE_ROOT
            / "validated"
            / domain_group
            / "fundamentals"
            / f"vendor={vendor}"
        )

        self.log = logging.getLogger(__name__)

    def _finalize_general_parquet(self):
        general_output_dir = (
                C.DATA_LAKE_VALIDATED /
                self.domain_group / "fundamentals" /
                f"vendor={self.vendor}" /
                f"exchange_code={self.exchange_code}"
        )
        jsonl_path = general_output_dir / f"fundamentals_general_{self.trd_dt}.jsonl"
        parquet_path = general_output_dir / f"fundamentals_general_{self.trd_dt}.parquet"
        latest_path = general_output_dir / "fundamentals_general_latest.parquet"

        if not jsonl_path.exists():
            self.log.warning(f"âš ï¸ No JSONL found for parquet conversion: {jsonl_path}")
            return

        df = pd.read_json(jsonl_path, lines=True)
        df.drop_duplicates(subset=["ticker"], keep="last", inplace=True)
        df.to_parquet(parquet_path, index=False)
        shutil.copyfile(parquet_path, latest_path)
        self.log.info(f"âœ… Converted JSONL â†’ Parquet: {parquet_path}")

    def _append_general_to_jsonl(self, general_dict: Dict[str, Any]) -> None:
        """
        âœ… ë³‘ë ¬ ê²€ì¦ìš© ìž„ì‹œ append-safe JSONL ë¡œê¹…
        - ìœ„ì¹˜: validated/fundamentals/.../fundamentals_general_{trd_dt}.jsonl
        """
        if not general_dict:
            return

        try:
            record = {
                "exchange_code": self.exchange_code,
                "ticker": general_dict.get("Code"),
                "Name": general_dict.get("Name"),
                "security_type": general_dict.get("Type"),
                "Sector": general_dict.get("Sector"),
                "Industry": general_dict.get("Industry"),
                "country_code": general_dict.get("CountryISO"),
                "CurrencyCode": general_dict.get("CurrencyCode"),
                "GicSector": general_dict.get("GicSector"),
                "GicGroup": general_dict.get("GicGroup"),
                "GicIndustry": general_dict.get("GicIndustry"),
                "GicSubIndustry": general_dict.get("GicSubIndustry"),
                "IPODate": general_dict.get("IPODate"),
                "IsDelisted": general_dict.get("IsDelisted"),
                "isin": general_dict.get("ISIN"),
                "cusip": general_dict.get("CUSIP"),
                "lei": general_dict.get("LEI"),
                "OpenFigi": general_dict.get("OpenFigi"),
                'fiscal_year_end': general_dict.get("FiscalYearEnd"),
                "PrimaryTicker": general_dict.get("PrimaryTicker"),
                "cik": general_dict.get("CIK"),
                "last_fundamental_update": general_dict.get("UpdatedAt"),
                "logo_url": general_dict.get("LogoURL"),
                "trd_dt": self.trd_dt,
                "validated_at": datetime.now(timezone.utc).isoformat(),
            }

            record = normalize_field_names(record)

            general_output_dir = (
                    C.DATA_LAKE_VALIDATED /
                    self.domain_group / "fundamentals" /
                    f"vendor={self.vendor}" /
                    f"exchange_code={self.exchange_code}"
            )
            general_output_dir.mkdir(parents=True, exist_ok=True)

            jsonl_path = general_output_dir / f"fundamentals_general_{self.trd_dt}.jsonl"

            with open(jsonl_path, "a", encoding="utf-8") as f:
                f.write(json.dumps(record, ensure_ascii=False) + "\n")

        except Exception as e:
            self.log.warning(f"âš ï¸ General JSONL append ì‹¤íŒ¨: {e}")


    def _detect_fund_type(self, df: pd.DataFrame) -> str:
        """íŒŒì¼ë³„ íƒ€ìž… ìžë™ ê°ì§€ (ETF or Stock)"""
        type_col = next(
            (c for c in ["General_Type", "Type", "General.Type", "General_Type.value"] if c in df.columns),
            None
        )
        fund_type = "etf"
        if type_col and df[type_col].astype(str).str.contains("Common Stock", case=False, na=False).any():
            fund_type = "stock"
        return fund_type

    def _define_checks(self, df: pd.DataFrame) -> Dict[str, Any]:
        """ETF/Stockë³„ Pandera schema ì ìš©"""
        checks = {}
        fund_type = self._detect_fund_type(df)

        schema_path = self.schema_root / f"fundamentals_{fund_type}.json"
        self.log.info(f"ðŸ§© schema_path: {schema_path}")

        if schema_path.exists():
            with open(schema_path, "r", encoding="utf-8") as f:
                schema_def = json.load(f)
            checks["pandera_schema"] = self._validate_with_pandera(df, schema_def)
        else:
            self.log.warning(f"âš ï¸ Pandera schema not found: {schema_path}")

        return checks


    def validate_file(self, file_path: Path) -> Dict[str, Any]:
        """ë‹¨ì¼ JSON íŒŒì¼ ê²€ì¦"""
        with open(file_path, "r", encoding="utf-8") as infile:
            data = json.load(infile)

        general_data = data.get("General", {})
        if not general_data:
            return {"file": file_path.name, "status": "skipped", "reason": "No 'General' key"}

        df = pd.json_normalize(general_data, sep="_")
        df.columns = [c.replace(".", "_") for c in df.columns]
        df.columns = [f"General_{c}" if not c.startswith("General_") else c for c in df.columns]

        checks = self._define_checks(df)
        status = self._aggregate_status(checks)

        return {"file": file_path.name, "status": status, "checks": checks}

    # -------------------------------------------------------------------------
    # 1ï¸âƒ£ ë©”ì¸ ê²€ì¦ (Base êµ¬ì¡° ìœ ì§€)
    # -------------------------------------------------------------------------
    def validate(self, context: Optional[dict] = None) -> Dict[str, Any]:
        """
        âš™ï¸ fundamentals ì „ìš© ê²€ì¦ ë¡œì§ (Lake ë ˆë²¨)
        - General ë¸”ë¡ë§Œ ê²€ì¦ (ETF/Stock ìžë™ êµ¬ë¶„)
        - íŒŒì¼ ë‹¨ìœ„ ë³‘ë ¬ ê²€ì¦ (ThreadPoolExecutor)
        - ì„±ê³µ ì‹œ ì›ë³¸ JSON validatedë¡œ ë³µì‚¬ + General parquet append
        """
        files = [
            f for f in self.dataset_path.glob("*.json")
            if not f.name.startswith("_")
        ]

        total_files = len(files)

        if not files:
            if self.allow_empty:
                return self._skip_empty_result()
            raise FileNotFoundError(f"âŒ No fundamental files found: {self.dataset_path}")

        self.log.info(f"ðŸ“‚ {len(files)} fundamentals íŒŒì¼ ë³‘ë ¬ ê²€ì¦ ì‹œìž‘")

        validated_dir = (
                self.data_root
                / "validated"
                / self.domain_group
                / self.domain
                / f"vendor={self.vendor}"
                / f"exchange_code={self.exchange_code}"
                / f"trd_dt={self.trd_dt}"
        )
        validated_dir.mkdir(parents=True, exist_ok=True)

        jsonl_path = validated_dir / f"{self.domain}_validated.jsonl"

        # âœ… ì‹¤í–‰ ì „ ì´ˆê¸°í™” (íŒŒì¼ ìžˆìœ¼ë©´ ì‚­ì œ)
        if jsonl_path.exists():
            jsonl_path.unlink()

        last_validated_path = validated_dir / "_last_validated.json"

        total_passed, total_failed = 0, 0
        failed_symbols = []
        records = []

        # âœ… ì§„í–‰ë¥  ë° ë½ ê´€ë¦¬
        progress_lock = Lock()
        completed = 0
        # ---------------------------------------------------------------
        # âœ… ë³‘ë ¬ íŒŒì¼ ê²€ì¦ (ETF/Stock ìžë™ ë¶„ê¸° í¬í•¨)
        # ---------------------------------------------------------------
        def _validate_file(file_path):
            nonlocal completed
            try:
                with open(file_path, "r", encoding="utf-8") as infile:
                    full_data = json.load(infile)
                general_data = full_data.get("General", {})
                if not general_data:
                    return {"file": file_path.name, "status": "skipped", "reason": "No General key"}

                df = pd.json_normalize(general_data, sep="_")
                df.columns = [c.replace(".", "_") for c in df.columns]
                df.columns = [f"General_{c}" if not c.startswith("General_") else c for c in df.columns]

                checks = self._define_checks(df)
                status = self._aggregate_status(checks)

                # âœ… progress ì¹´ìš´í„° ì¦ê°€ (thread-safe)
                with progress_lock:
                    completed += 1
                    progress_pct = (completed / total_files) * 100
                    self.log.info(
                        f"ðŸ” [{completed}/{total_files} | {progress_pct:.1f}%] "
                        f"ê²€ì¦ ì¤‘: {file_path.name} (status={status})"
                    )

                if status == "success":
                    validated_json_path = validated_dir / f"{file_path.stem}.json"
                    with open(validated_json_path, "w", encoding="utf-8") as out_f:
                        json.dump(full_data, out_f, ensure_ascii=False, indent=2)
                    self._append_general_to_jsonl(full_data.get("General", {}))

                return {"file": file_path.name, "status": status, "checks": checks}

            except Exception as e:
                with progress_lock:
                    completed += 1
                    progress_pct = (completed / total_files) * 100
                    self.log.warning(
                        f"âš ï¸ [{completed}/{total_files} | {progress_pct:.1f}%] "
                        f"{file_path.name} ê²€ì¦ ì‹¤íŒ¨: {e}"
                    )
                return {"file": file_path.name, "status": "failed", "error": str(e)}

        # ---------------------------------------------------------------
        # âœ… ThreadPoolExecutorë¡œ ë³‘ë ¬ ì²˜ë¦¬
        # ---------------------------------------------------------------
        with ThreadPoolExecutor(max_workers=8) as executor:
            futures = {executor.submit(_validate_file, f): f for f in files}
            for future in as_completed(futures):
                result = future.result()
                records.append(result)
                if result["status"] == "success":
                    total_passed += 1
                elif result["status"] == "failed":
                    total_failed += 1
                    failed_symbols.append(result["file"])

        # âœ… ìµœì¢… ìš”ì•½
        self.log.info(f"ðŸŽ¯ ê²€ì¦ ì™„ë£Œ â€” ì„±ê³µ {total_passed:,}, ì‹¤íŒ¨ {total_failed:,}, ì´ {total_files:,}")

        # ---------------------------------------------------------------
        # âœ… JSONL ë¡œê·¸ ì €ìž¥
        # ---------------------------------------------------------------
        with open(jsonl_path, "w", encoding="utf-8") as out_f:
            for rec in records:
                out_f.write(json.dumps(rec, ensure_ascii=False) + "\n")

        # ---------------------------------------------------------------
        # âœ… ê²°ê³¼ ìš”ì•½ ì €ìž¥
        # ---------------------------------------------------------------
        summary = {
            "dataset": self.domain,
            "layer": self.layer,
            "vendor": self.vendor,
            "exchange_code": self.exchange_code,
            "trd_dt": self.trd_dt,
            "record_count": len(files),
            "passed_files": total_passed,
            "failed_files": total_failed,
            "failed_symbols": failed_symbols,
            "status": "success" if total_failed == 0 else "failed",
            "validated_source": str(self.dataset_path),
            "validated_file": str(jsonl_path),
            "validated_at": datetime.now(timezone.utc).isoformat(),
        }

        with open(last_validated_path, "w", encoding="utf-8") as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)

        self._update_latest_snapshot_meta(
            domain=self.domain,
            trd_dt=self.trd_dt,
            meta_file=str(last_validated_path),
        )

        if total_failed > 0:
            self.log.error(f"âŒ {total_failed:,}ê°œ ì¢…ëª© ê²€ì¦ ì‹¤íŒ¨ â€” details: {last_validated_path}")
            raise ValueError(f"Fundamentals validation failed â€” {total_failed:,} files failed")

        self.log.info("fundamentals_general íŒŒì¼ jsonlì—ì„œ parquetìœ¼ë¡œ ë³€í™˜")
        self._finalize_general_parquet()

        self.log.info(f"ðŸŽ¯ Fundamentals ê²€ì¦ ì™„ë£Œ â€” ì„±ê³µ {total_passed:,}ê±´, ê²°ê³¼ ì €ìž¥: {last_validated_path}")
        return summary

    # -------------------------------------------------------------------------
    # 4ï¸âƒ£ empty ê²°ê³¼ ìŠ¤í‚µìš© ìœ í‹¸ (Baseì™€ ì¼ê´€ì„± ìœ ì§€)
    # -------------------------------------------------------------------------
    def _skip_empty_result(self) -> Dict[str, Any]:
        result = {
            "dataset": self.domain,
            "layer": self.layer,
            "vendor": self.vendor,
            "exchange_code": self.exchange_code,
            "trd_dt": self.trd_dt,
            "status": "skipped",
            "record_count": 0,
            "validated_source": str(self.dataset_path),
            "validated_at": datetime.now(timezone.utc).isoformat(),
            "message": "No data found (allow_empty=True)",
        }

        self.log.info(f"âœ… No data found for {self.domain} â†’ SKIPPED")
        return result