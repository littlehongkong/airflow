from plugins.validators.base_data_validator import BaseDataValidator
from plugins.config import constants as C
from plugins.utils.name_utils import normalize_field_names   # âœ… ìƒˆ ìœ í‹¸ ì‚¬ìš©
from datetime import datetime, timezone
from typing import Optional, Dict, Any
import pandas as pd, json, logging
import gc
import shutil
import psutil, tracemalloc
import traceback

process = psutil.Process()
tracemalloc.start()

class FundamentalDataValidator(BaseDataValidator):
    """
    ðŸ§© Fundamental ì „ìš© Validator
    - íŒŒì¼ ë‹¨ìœ„ ê²€ì¦ (OOM ë°©ì§€)
    - ê° JSONë³„ Pandera/Soda ì ìš©
    - validated ê²°ê³¼ JSONL append
    """

    def __init__(
        self,
        trd_dt: str,
        domain: str,
        exchange_code: str,
        vendor: str = "eodhd",
        domain_group: str = "equity",
    ):
        self.dataset_path = (
            C.DATA_LAKE_ROOT
            / "raw"
            / domain_group
            / domain
            / f"vendor={vendor}"
            / f"exchange_code={exchange_code}"
            / f"trd_dt={trd_dt}"
        )

        super().__init__(
            domain=domain,
            layer="lake",
            trd_dt=trd_dt,
            vendor=vendor,
            dataset_path=self.dataset_path,
            exchange_code=exchange_code,
            domain_group=domain_group,
        )


        self.output_dir = (
            C.DATA_LAKE_ROOT
            / "validated"
            / domain_group
            / "fundamentals"
            / f"vendor={vendor}"
        )

        self.log = logging.getLogger(__name__)


    def _append_general_to_parquet(self, general_dict: Dict[str, Any]) -> None:
        """
        âœ… fundamentals General-only ì •ë³´ë¥¼ ê±°ëž˜ì†Œ ë‹¨ìœ„ parquetì— append
        - íŒŒì¼ëª…: fundamentals_general_{trd_dt}.parquet
        - ìœ„ì¹˜: validated/fundamentals/exchange_code={exchange_code}/
        """
        try:

            if not general_dict:
                return

            record = {
                "exchange_code": self.exchange_code,
                "ticker": general_dict.get("Code"),
                "Name": general_dict.get("Name"),
                "security_type": general_dict.get("Type"),
                "Sector": general_dict.get("Sector"),
                "Industry": general_dict.get("Industry"),
                "country_code": general_dict.get("CountryISO"),
                "CurrencyCode": general_dict.get("CurrencyCode"),
                "GicSector": general_dict.get("GicSector"),
                "GicGroup": general_dict.get("GicGroup"),
                "GicIndustry": general_dict.get("GicIndustry"),
                "GicSubIndustry": general_dict.get("GicSubIndustry"),
                "IPODate": general_dict.get("IPODate"),
                "IsDelisted": general_dict.get("IsDelisted"),
                "isin": general_dict.get("ISIN"),
                "cusip": general_dict.get("CUSIP"),
                "lei": general_dict.get("LEI"),
                "OpenFigi": general_dict.get("OpenFigi"),
                'fiscal_year_end': general_dict.get("FiscalYearEnd"),
                "PrimaryTicker": general_dict.get("PrimaryTicker"),
                "cik": general_dict.get("CIK"),
                "last_fundamental_update": general_dict.get("UpdatedAt"),
                "logo_url": general_dict.get("LogoURL"),
                "trd_dt": self.trd_dt,
                "validated_at": datetime.now(timezone.utc).isoformat(),
            }

            record = normalize_field_names(record)

            df = pd.DataFrame([record])

            general_output_dir = (
                C.DATA_LAKE_VALIDATED
                / self.domain_group
                / "fundamentals"
                / f"vendor={self.vendor}"
                / f"exchange_code={self.exchange_code}"
            )
            general_output_dir.mkdir(parents=True, exist_ok=True)

            parquet_path = general_output_dir / f"fundamentals_general_{self.trd_dt}.parquet"

            if parquet_path.exists():
                existing = pd.read_parquet(parquet_path)
                merged = pd.concat([existing, df], ignore_index=True)
                merged.drop_duplicates(subset=["ticker"], keep="last", inplace=True)
                merged.to_parquet(parquet_path, index=False)
            else:
                df.to_parquet(parquet_path, index=False)

            # ìµœì‹ ë³¸ ë³µì‚¬
            latest_path = general_output_dir / "fundamentals_general_latest.parquet"
            shutil.copyfile(parquet_path, latest_path)

            self.log.debug(f"ðŸ“¦ General parquet updated: {parquet_path.name}")

        except Exception as e:
            self.log.warning(f"âš ï¸ General parquet append ì‹¤íŒ¨: {e}")


    def _define_checks(self, df: pd.DataFrame) -> Dict[str, Any]:
        """
        fundamentals ì „ìš© Pandera + Soda Core ê²€ì¦
        - ETF/Stock ìžë™ ë¶„ê¸°
        - flatten ì»¬ëŸ¼ ì „ì²˜ë¦¬
        """
        checks = {}

        # âœ… 1. ê¸°ë³¸ ì „ì²˜ë¦¬
        for col in df.columns:
            if df[col].dtype == object:
                df[col] = df[col].apply(lambda x: None if isinstance(x, str) and x.strip() == "" else x)

        for c in ["General_UpdatedAt", "General_IPODate", "General_ReportDate"]:
            if c in df.columns:
                df[c] = pd.to_datetime(df[c], errors="coerce")

        # âœ… 2. ETF/Stock ë¶„ê¸°
        type_col = next((c for c in ["General_Type", "Type", "General.Type", "General_Type.value"] if c in df.columns),
                        None)

        fund_type = "etf"
        if type_col and df[type_col].astype(str).str.contains("Common Stock", case=False, na=False).any():
            fund_type = "stock"

        # âœ… 3. Pandera Schema ì ìš©
        schema_path = self.schema_root / f"fundamentals_{fund_type}.json"
        self.log.info(f"schema_path : {schema_path}")
        if schema_path.exists():
            with open(schema_path, "r", encoding="utf-8") as f:
                schema_def = json.load(f)
            checks["pandera_schema"] = self._validate_with_pandera(df, schema_def)
        else:
            self.log.warning(f"âš ï¸ Pandera schema not found: {schema_path}")

        # âœ… 4. Soda Core ì ìš©
        soda_path = self.check_root / f"fundamentals_{fund_type}.yml"
        if soda_path.exists():
            checks.update(self._run_soda_duckdb_validation(df, soda_path))
        else:
            self.log.warning(f"âš ï¸ Soda check file not found: {soda_path}")

        return checks


    # -------------------------------------------------------------------------
    # 1ï¸âƒ£ ë©”ì¸ ê²€ì¦ (Base êµ¬ì¡° ìœ ì§€)
    # -------------------------------------------------------------------------
    def validate(self, context: Optional[dict] = None) -> Dict[str, Any]:
        """
        fundamentals ì „ìš© ê²€ì¦ ë¡œì§ (íŒŒì¼ ë‹¨ìœ„)
        - General ë¸”ë¡ë§Œ ê²€ì¦í•˜ë˜, ë‚˜ë¨¸ì§€ ë¸”ë¡(Financials ë“±)ì€ ê·¸ëŒ€ë¡œ ìœ ì§€
        - ê²€ì¦ ì„±ê³µ ì‹œ ì›ë³¸ ì „ì²´ JSONì„ validatedë¡œ ë³µì‚¬
        """
        files = [
            f for f in self.dataset_path.glob("*.json")
            if not f.name.startswith("_")
        ]

        if not files:
            if self.allow_empty:
                return self._skip_empty_result()
            raise FileNotFoundError(f"âŒ No fundamental files found: {self.dataset_path}")

        self.log.info(f"ðŸ“ {len(files)}ê°œ fundamentals íŒŒì¼ ê²€ì¦ ì‹œìž‘")

        validated_dir = (
                self.data_root
                / "validated"
                / self.domain_group
                / self.domain
                / f"vendor={self.vendor}"
                / f"exchange_code={self.exchange_code}"
                / f"trd_dt={self.trd_dt}"
        )
        validated_dir.mkdir(parents=True, exist_ok=True)

        jsonl_path = validated_dir / f"{self.domain}_validated.jsonl"
        last_validated_path = validated_dir / "_last_validated.json"

        total_passed, total_failed = 0, 0
        failed_symbols = []

        for i, f in enumerate(files, 1):
            try:
                # âœ… ì „ì²´ JSON ë¡œë“œ (General ì™¸ key í¬í•¨)
                with open(f, "r", encoding="utf-8") as infile:
                    full_data = json.load(infile)

                # âœ… General í‚¤ë§Œ ê²€ì¦ìš©ìœ¼ë¡œ ì¶”ì¶œ
                general_data = full_data.get("General", {})
                if not general_data:
                    self.log.warning(f"âš ï¸ No 'General' key found in {f.name}")
                    continue

                df = pd.json_normalize(general_data, sep="_")
                df.columns = [c.replace(".", "_") for c in df.columns]
                df.columns = [f"General_{c}" if not c.startswith("General_") else c for c in df.columns]

                # âœ… ê²€ì¦ ìˆ˜í–‰
                checks = self._define_checks(df)
                status = self._aggregate_status(checks)

                record = {
                    "file": f.name,
                    "status": status,
                    "checks": checks,
                }
                with open(jsonl_path, "a", encoding="utf-8") as out_f:
                    out_f.write(json.dumps(record, ensure_ascii=False) + "\n")

                if status == "success":
                    total_passed += 1

                    # âœ… ì›ë³¸ ì „ì²´ JSON (Financials í¬í•¨)ì„ validatedë¡œ ê·¸ëŒ€ë¡œ ë³µì‚¬
                    validated_json_path = validated_dir / f"{f.stem}.json"
                    with open(validated_json_path, "w", encoding="utf-8") as out_f:
                        json.dump(full_data, out_f, ensure_ascii=False, indent=2)

                    # âœ… General-only parquet append
                    self._append_general_to_parquet(full_data.get("General", {}))

                else:
                    total_failed += 1
                    failed_symbols.append(f.stem)

            except Exception as e:
                total_failed += 1
                failed_symbols.append(f.stem)
                self.log.warning(f"âš ï¸ {f.name} ê²€ì¦ ì‹¤íŒ¨: {e}")
                self.log.info(traceback.format_exc())
                continue

            finally:
                # âœ… ë©”ëª¨ë¦¬ ì •ë¦¬
                for var in ["df", "full_data", "general_data", "checks"]:
                    if var in locals():
                        del locals()[var]
                gc.collect()
                tracemalloc.clear_traces()

        # ---------------------------------------------------------------------
        # 2ï¸âƒ£ ê²°ê³¼ ìš”ì•½
        # ---------------------------------------------------------------------
        summary = {
            "dataset": self.domain,
            "layer": self.layer,
            "vendor": self.vendor,
            "exchange_code": self.exchange_code,
            "trd_dt": self.trd_dt,
            "record_count": len(files),
            "passed_files": total_passed,
            "failed_files": total_failed,
            "failed_symbols": failed_symbols,
            "status": "success" if total_failed == 0 else "failed",
            "validated_source": str(self.dataset_path),
            "validated_file": str(jsonl_path),
            "validated_at": datetime.now(timezone.utc).isoformat(),
        }

        # âœ… `_last_validated.json` ì €ìž¥
        with open(last_validated_path, "w", encoding="utf-8") as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)

        # âœ… snapshot ë©”íƒ€ ì—…ë°ì´íŠ¸
        self._update_latest_snapshot_meta(
            domain=self.domain,
            trd_dt=self.trd_dt,
            meta_file=str(last_validated_path)
        )

        if total_failed > 0:
            self.log.error(f"âŒ {total_failed:,}ê°œ ì¢…ëª© ê²€ì¦ ì‹¤íŒ¨ â€” details: {last_validated_path}")
            raise ValueError(f"Fundamentals validation failed â€” {total_failed:,} files failed")

        self.log.info(f"ðŸŽ¯ Fundamentals ê²€ì¦ ì™„ë£Œ â€” ì„±ê³µ {total_passed:,}ê±´, ê²°ê³¼ ì €ìž¥: {last_validated_path}")
        return summary

    # -------------------------------------------------------------------------
    # 4ï¸âƒ£ empty ê²°ê³¼ ìŠ¤í‚µìš© ìœ í‹¸ (Baseì™€ ì¼ê´€ì„± ìœ ì§€)
    # -------------------------------------------------------------------------
    def _skip_empty_result(self) -> Dict[str, Any]:
        result = {
            "dataset": self.domain,
            "layer": self.layer,
            "vendor": self.vendor,
            "exchange_code": self.exchange_code,
            "trd_dt": self.trd_dt,
            "status": "skipped",
            "record_count": 0,
            "validated_source": str(self.dataset_path),
            "validated_at": datetime.now(timezone.utc).isoformat(),
            "message": "No data found (allow_empty=True)",
        }

        self.log.info(f"âœ… No data found for {self.domain} â†’ SKIPPED")
        return result