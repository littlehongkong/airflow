from plugins.validators.base_data_validator import BaseDataValidator
from plugins.config import constants as C
import pandas as pd
import json
from pathlib import Path


class ExchangeDetailValidator(BaseDataValidator):
    """
    ğŸ“… Exchange Detail Validator
    - DataPathResolver ê¸°ë°˜ in/out ê²½ë¡œ ìë™í™”
    - nested dict í•„ë“œ(JSON ì§ë ¬í™”) ì²˜ë¦¬
    """

    def __init__(
        self,
        domain: str,
        domain_group: str,
        trd_dt: str,
        exchange_code: str,
        vendor: str,
        allow_empty: bool = False,
        **kwargs,
    ):
        """
        Data Lake êµ¬ì¡° ì˜ˆì‹œ:
        /opt/airflow/data/data_lake/raw/equity/exchange_detail/
            vendor=eodhd/exchange_code=US/trd_dt=2025-11-11/exchange_detail.jsonl
        """
        self.allow_empty = allow_empty
        self.vendor = vendor.lower()
        self.exchange_code = exchange_code
        self.domain_group = domain_group
        self.domain = domain

        # âœ… ì›ì²œ(raw) ê²½ë¡œ
        dataset_path = (
            C.DATA_LAKE_ROOT
            / "raw"
            / domain_group
            / domain
            / f"vendor={self.vendor}"
            / f"exchange_code={exchange_code}"
            / f"trd_dt={trd_dt}"
            / f"{domain}.jsonl"
        )

        # âœ… ê²€ì¦ê²°ê³¼(validated) ê²½ë¡œëŠ” BaseDataValidatorì—ì„œ ìë™ ìƒì„±
        super().__init__(
            domain=domain,
            domain_group=domain_group,
            layer="lake",
            trd_dt=trd_dt,
            vendor=self.vendor,
            exchange_code=exchange_code,
            allow_empty=allow_empty,
            **kwargs,
        )

        self.dataset_path = dataset_path  # ì›ì²œ JSONL íŒŒì¼ ê²½ë¡œ
        self.log.info(f"ğŸ“¦ ExchangeHolidayValidator dataset_path: {self.dataset_path}")

    # -------------------------------------------------------------------------
    # 1ï¸âƒ£ JSONL ë¡œë”
    # -------------------------------------------------------------------------
    def _load_dataset(self) -> pd.DataFrame:
        """
        âœ… JSONL â†’ DataFrame ë¡œë“œ
        - ë¹ˆ íŒŒì¼/ë””ë ‰í„°ë¦¬ ì˜ˆì™¸ì²˜ë¦¬
        - nested dict â†’ ë¬¸ìì—´ ë³€í™˜
        """
        try:
            # ğŸ“ dataset_pathê°€ ë””ë ‰í† ë¦¬ë©´ ì‹¤ì œ íŒŒì¼ ì°¾ê¸°
            if Path(self.dataset_path).is_dir():
                candidate = list(Path(self.dataset_path).glob("*.jsonl"))
                if not candidate:
                    self.log.error(f"ğŸš« No JSONL found under {self.dataset_path}")
                    return pd.DataFrame()
                self.dataset_path = candidate[0]  # ì²« ë²ˆì§¸ íŒŒì¼ ì„ íƒ

            if not Path(self.dataset_path).exists():
                self.log.error(f"ğŸš« Missing source file: {self.dataset_path}")
                return pd.DataFrame()

            self.log.info(f"ğŸ“‚ Loading JSONL file â†’ {self.dataset_path}")
            df = pd.read_json(self.dataset_path, lines=True)

        except Exception as e:
            self.log.error(f"âŒ Failed to read JSONL: {e}")
            return pd.DataFrame()

        if df.empty:
            self.log.warning(f"âš ï¸ Empty data for {self.exchange_code}")
            return df

        df = self._normalize_nested_fields(df)
        return df

    # -------------------------------------------------------------------------
    # 2ï¸âƒ£ Nested dict â†’ ë¬¸ìì—´ ì§ë ¬í™”
    # -------------------------------------------------------------------------
    def _normalize_nested_fields(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        PyArrow Parquet ì €ì¥ ì‹œ struct íƒ€ì… ì¶©ëŒ ë°©ì§€
        â†’ nested dictë¥¼ JSON ë¬¸ìì—´ë¡œ ì§ë ¬í™”
        """
        nested_cols = ["ExchangeHolidays", "ExchangeEarlyCloseDays", "TradingHours"]
        for col in nested_cols:
            if col in df.columns:
                df[col] = df[col].apply(
                    lambda x: (
                        json.dumps(x if x else {}, ensure_ascii=False)
                        if isinstance(x, (dict, list))
                        else x
                    )
                )
        return df
