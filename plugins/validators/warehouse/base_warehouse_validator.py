"""
Warehouse Base Validator (Refactored)
-----------------------------------------
‚úÖ Î™©Ï†Å:
- Warehouse Ï†ÅÏû¨ ÌõÑ Ï†ïÌï©ÏÑ± Í≤ÄÏ¶ùÏùÑ ÏúÑÌïú Í≥µÌÜµ Î≤†Ïù¥Ïä§ ÌÅ¥ÎûòÏä§
- BaseWarehousePipelineÍ≥º ÎèôÏùºÌïú ÎèÑÎ©îÏù∏ Íµ¨Ï°∞Î°ú ÎèôÏûë
- ÌïòÏúÑ ÌÅ¥ÎûòÏä§ÏóêÏÑúÎäî `_define_checks()`Îßå Íµ¨ÌòÑ

Í∏∞Îä• ÏöîÏïΩ:
1Ô∏è‚É£ Parquet ÌååÏùº Î°úÎìú (snapshot Í∏∞Î∞ò)
2Ô∏è‚É£ Pandera / Soda Core / Custom Rule Í∏∞Î∞ò Í≤ÄÏ¶ù
3Ô∏è‚É£ Í≤ÄÏ¶ù Í≤∞Í≥º Ï†ÄÏû• Î∞è ÏÉÅÌÉú ÏöîÏïΩ
"""

import duckdb
import pandas as pd
import json
import numpy as np
from pathlib import Path
from typing import Dict, Any, Optional, List
from plugins.validators.base_validator_interface import BaseValidatorInterface
from plugins.config.constants import DATA_WAREHOUSE_ROOT
from datetime import datetime, timezone


class BaseWarehouseValidator(BaseValidatorInterface):
    def __init__(
        self,
        domain: str,
        snapshot_dt: str,
        partition_key: str = "snapshot_dt",
        partition_value: Optional[str] = None,
        warehouse_root: Path = Path("/opt/airflow/data/data_warehouse"),
        **kwargs,
    ):
        super().__init__(**kwargs)
        self.domain = domain
        self.snapshot_dt = snapshot_dt
        self.partition_key = partition_key
        self.partition_value = partition_value  # ‚úÖ Ïù¥Ï†ú optional
        self.warehouse_root = warehouse_root
        self.duckdb_path = warehouse_root / "duckdb" / f"{domain}.duckdb"
        self.conn = None

    # -------------------------------------------------------------------------
    # 1Ô∏è‚É£ Validation Entry Point
    # -------------------------------------------------------------------------
    def validate(self, context: Optional[Dict] = None, **kwargs) -> Dict[str, Any]:
        """
        Í≤ÄÏ¶ù Î©îÏù∏ Ìï®Ïàò (Airflow OperatorÏóêÏÑú ÏßÅÏ†ë Ìò∏Ï∂ú)
        """
        dag_run_id, task_id = self._extract_airflow_context(context)
        parquet_path = self._get_parquet_path()

        if not parquet_path.exists():
            self.log.warning(f"‚ùå Parquet file not found: {parquet_path}")
            return self._generate_result(
                status="skipped",
                checks={},
                record_count=0,
                message="File not found",
                dag_run_id=dag_run_id,
                task_id=task_id,
            )

        # ‚úÖ Load data
        df = pd.read_parquet(parquet_path)
        self.log.info(f"üì¶ Loaded {len(df):,} rows from {parquet_path}")

        # ‚úÖ Run checks
        checks = self._define_checks(df)
        status = self._aggregate_status(checks)
        self.log.info(f"‚úÖ Validation complete | Status: {status}")

        # ‚úÖ Build result
        result = self._generate_result(
            status=status,
            checks=checks,
            record_count=len(df),
            snapshot_dt=self.snapshot_dt,
            parquet_path=str(parquet_path),
            dag_run_id=dag_run_id,
            task_id=task_id,
        )

        # ‚úÖ Save result JSON
        saved_path = self._save_validation_result(result)
        result["result_path"] = str(saved_path)
        return result

    # -------------------------------------------------------------------------
    # 2Ô∏è‚É£ Abstracts for Subclasses
    # -------------------------------------------------------------------------
    def _get_parquet_path(self) -> Path:
        """
        snapshot Í∏∞Ï§Ä warehouse Í≤ΩÎ°ú Î∞òÌôò
        ÌïòÏúÑ ÌÅ¥ÎûòÏä§ÏóêÏÑú domain Í∏∞Ï§ÄÏúºÎ°ú ÏûêÎèô Ï†ëÍ∑º
        """
        return self.warehouse_root / self.domain / f"snapshot_dt={self.snapshot_dt}" / f"{self.domain}.parquet"

    def _define_checks(self, df: pd.DataFrame) -> Dict[str, Any]:
        """ÌïòÏúÑ ÌÅ¥ÎûòÏä§ÏóêÏÑú Í≤ÄÏ¶ù Í∑úÏπô Ï†ïÏùò"""
        raise NotImplementedError("‚ùå _define_checks() must be implemented in subclass")

    # -------------------------------------------------------------------------
    # 3Ô∏è‚É£ Utility Checks
    # -------------------------------------------------------------------------
    def _check_row_count(self, df: pd.DataFrame, min_rows: int = 1) -> Dict[str, Any]:
        value = int(len(df))
        return {
            "passed": bool(value >= min_rows),
            "value": value,
            "expected": f">={min_rows}",
            "message": f"Row count = {value}",
        }

    def _check_no_nulls(self, df: pd.DataFrame, column: str) -> Dict[str, Any]:
        null_count = int(df[column].isnull().sum())
        return {
            "passed": bool(null_count == 0),
            "value": null_count,
            "expected": 0,
            "message": f"Null count in {column}: {null_count}",
        }

    def _check_no_duplicates(self, df: pd.DataFrame, column: str) -> Dict[str, Any]:
        dup_count = int(df[column].duplicated().sum())
        return {
            "passed": bool(dup_count == 0),
            "value": dup_count,
            "expected": 0,
            "message": f"Duplicate count in {column}: {dup_count}",
        }

    def _check_foreign_key(self, df: pd.DataFrame, ref_df: pd.DataFrame, fk_column: str, ref_column: str) -> Dict[
        str, Any]:
        missing_count = int(len(df[~df[fk_column].isin(ref_df[ref_column])]))
        return {
            "passed": bool(missing_count == 0),
            "value": missing_count,
            "expected": 0,
            "message": f"Missing foreign key values in {fk_column}: {missing_count}",
        }

    def _aggregate_status(self, checks: Dict[str, Any]) -> str:
        if not checks:
            return "skipped"
        if any(not c.get("passed", False) for c in checks.values()):
            return "failed"
        return "success"

    # -------------------------------------------------------------------------
    # 4Ô∏è‚É£ Result Handling
    # -------------------------------------------------------------------------
    def _generate_result(
        self,
        status: str,
        checks: Dict[str, Any],
        record_count: int,
        snapshot_dt: Optional[str] = None,
        parquet_path: Optional[str] = None,
        dag_run_id: Optional[str] = None,
        task_id: Optional[str] = None,
        message: Optional[str] = None,
    ) -> Dict[str, Any]:
        """Í≤ÄÏ¶ù Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Í≤∞Í≥º ÏÉùÏÑ±"""
        result = {
            "dataset": self.domain,
            "snapshot_dt": snapshot_dt or self.snapshot_dt,
            "status": status,
            "record_count": record_count,
            "checks": checks,
            "validated_file": parquet_path,
            "validated_at": datetime.now(timezone.utc).isoformat(),
        }
        if message:
            result["message"] = message
        if dag_run_id:
            result["dag_run_id"] = dag_run_id
        if task_id:
            result["task_id"] = task_id
        return result

    def _save_validation_result(self, result: Dict[str, Any], **kwargs) -> Path:

        def _make_json_serializable(obj):
            import numpy as np

            if isinstance(obj, (np.bool_, bool)):
                return bool(obj)
            elif isinstance(obj, (np.integer, int)):
                return int(obj)
            elif isinstance(obj, (np.floating, float)):
                return float(obj)
            elif isinstance(obj, dict):
                return {k: _make_json_serializable(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [_make_json_serializable(v) for v in obj]
            else:
                return obj

        # ‚úÖ Î≥ÄÌôò ÏàòÌñâ
        serializable_result = _make_json_serializable(result)

        # ‚úÖ Ï†ÄÏû•
        result_dir = self.warehouse_root / self.domain / "validation"
        result_dir.mkdir(parents=True, exist_ok=True)

        # ‚úÖ ÌååÏùºÎ™Ö Í∑úÏπô: {domain}_{partition(optional)}_{snapshot_dt}_validation.json
        if self.partition_value:
            filename = f"{self.domain}_{self.partition_value}_{self.snapshot_dt}_validation.json"
        else:
            filename = f"{self.domain}_{self.snapshot_dt}_validation.json"

        output_path = result_dir / filename
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(serializable_result, f, indent=2, ensure_ascii=False)

        self.log.info(f"üíæ Saved validation result: {output_path}")
        return output_path

    # -------------------------------------------------------------------------
    # 5Ô∏è‚É£ Optional: Pandera / Soda Core Ïó∞Îèô (ÌôïÏû• Ìè¨Ïù∏Ìä∏)
    # -------------------------------------------------------------------------
    def _validate_with_pandera(self, df: pd.DataFrame, schema_def: dict) -> Dict[str, Any]:
        """
        Optional: Pandera Ïä§ÌÇ§Îßà Í∏∞Î∞ò Í≤ÄÏ¶ù (warehouse_schemas/{domain}.json)
        """
        import pandera as pa
        from pandera import DataFrameSchema, Column

        try:
            columns = {
                c["name"]: Column(
                    eval(f"pa.{c['type'].capitalize()}"),
                    nullable=c.get("nullable", True),
                )
                for c in schema_def.get("columns", [])
            }
            schema = DataFrameSchema(columns)
            schema.validate(df)
            self.log.info(f"‚úÖ Pandera validation passed for {self.domain}")
            return {"passed": True, "message": "Pandera schema validation passed"}
        except Exception as e:
            self.log.error(f"‚ùå Pandera validation failed: {e}")
            return {"passed": False, "message": str(e)}

    def _validate_with_soda(self, df: pd.DataFrame, checks: Optional[List[str]] = None):
        """
        Optional: Soda Core Í∏∞Î∞ò Í≤ÄÏ¶ù (Îç∞Ïù¥ÌÑ∞ ÌíàÏßà Î¶¨Ìè¨Ìä∏ ÏÉùÏÑ±)
        """
        try:
            from soda.scan import Scan
            scan = Scan()
            scan.add_pandas_df(df, self.domain)
            if checks:
                for rule in checks:
                    scan.add_check(rule)
            scan.execute()
            if scan.has_check_failures():
                raise ValueError(f"Soda checks failed for {self.domain}")
            self.log.info(f"‚úÖ Soda validation passed for {self.domain}")
        except Exception as e:
            self.log.error(f"‚ùå Soda validation failed: {e}")
            raise e

    # -------------------------------------------------------------------------
    # 6Ô∏è‚É£ Cleanup
    # -------------------------------------------------------------------------
    def cleanup(self):
        """Î¶¨ÏÜåÏä§ Ï†ïÎ¶¨"""
        if self.conn:
            self.conn.close()
            self.conn = None
