import json
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime, timezone
from typing import Dict, Any
import pandas as pd
import logging

from plugins.validators.base_data_validator import BaseDataValidator
from plugins.config.constants import DATA_WAREHOUSE_ROOT, WAREHOUSE_DOMAINS


class FundamentalsWarehouseValidator(BaseDataValidator):
    """
    ğŸ§© Fundamentals Warehouse Validator (constants ê¸°ë°˜ ê°œì„ íŒ)
    ---------------------------------------------------------------
    âœ… ê°œì„  í¬ì¸íŠ¸:
      - constants.WAREHOUSE_DOMAINS ê¸°ë°˜ ê²½ë¡œ ìë™ êµ¬ì„±
      - sectionë³„ Pandera ê²€ì¦ (Typeë³„ ìŠ¤í‚¤ë§ˆ ìë™ ì„ íƒ)
      - Snapshot ë‹¨ìœ„ Soda Core ê²€ì¦ (optional)
      - ë³‘ë ¬ ì²˜ë¦¬ (security_id ë‹¨ìœ„)
    """

    def __init__(
        self,
        trd_dt: str,
        country_code: str,
        domain_group: str,
        vendor: str = "eodhd",
        allow_empty: bool = False,
    ):
        super().__init__(
            domain="fundamental",
            layer="warehouse",
            trd_dt=trd_dt,
            vendor=vendor,
            domain_group=domain_group,
            allow_empty=allow_empty,
        )

        self.country_code = country_code
        self.log = logging.getLogger(__name__)

        # âœ… constants ê¸°ë°˜ dataset_path êµ¬ì„±
        domain_conf = WAREHOUSE_DOMAINS["fundamental"]
        relative_path = domain_conf["path"].format(
            country_code=country_code,
            trd_dt=trd_dt,
        )
        self.dataset_path = Path(DATA_WAREHOUSE_ROOT) / relative_path

        self.log.info(f"ğŸ“‚ Fundamentals dataset path: {self.dataset_path}")

    # ------------------------------------------------------------------
    def _detect_fund_type(self, general_data: dict) -> str:
        """General ë¸”ë¡ì˜ Type ê¸°ì¤€ìœ¼ë¡œ ì¹´í…Œê³ ë¦¬ ìë™ ê°ì§€"""
        t = (general_data.get("Type") or general_data.get("General_Type") or "").lower()
        if "etf" in t:
            return "etf"
        elif "fund" in t or "mutual" in t:
            return "fund"
        return "stock"

    # ------------------------------------------------------------------
    def _validate_security_folder(self, security_dir: Path) -> Dict[str, Any]:
        """
        âœ… ë‹¨ì¼ security_id í´ë” ê²€ì¦
        - General.json ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        - Type ê°ì§€
        - ê° Section íŒŒì¼ë³„ Pandera ê²€ì¦ ìˆ˜í–‰
        """
        try:
            general_path = security_dir / "General.json"
            if not general_path.exists():
                return {"security_id": security_dir.name, "status": "skipped", "reason": "No General.json"}

            with open(general_path, "r", encoding="utf-8") as f:
                general_data = json.load(f)

            fund_type = self._detect_fund_type(general_data)
            schema_dir = self.schema_root / "fundamentals"  # e.g. /schemas/warehouse/equity/fundamentals
            check_dir = self.check_root / "fundamentals"

            errors = []
            sections = [p for p in security_dir.glob("*.json")]

            for section_file in sections:
                section = section_file.stem
                # âœ… Typeë³„ ìŠ¤í‚¤ë§ˆ ë¡œë“œ
                schema_path = schema_dir / f"{fund_type}_{section}.json"
                if not schema_path.exists():
                    self.log.debug(f"âš ï¸ No schema for {fund_type}/{section}, skipping")
                    continue

                try:
                    df = pd.json_normalize(json.load(open(section_file, "r", encoding="utf-8")), sep="_")
                    with open(schema_path, "r", encoding="utf-8") as f:
                        schema_def = json.load(f)

                    result = self._validate_with_pandera(df, schema_def)
                    if not result.get("passed", False):
                        errors.append(f"{section}: {result.get('message')}")

                except Exception as e:
                    errors.append(f"{section}: {str(e)}")

            # âœ… ê²°ê³¼ ìš”ì•½
            status = "success" if not errors else "failed"
            return {
                "security_id": security_dir.name,
                "fund_type": fund_type,
                "status": status,
                "errors": errors,
            }

        except Exception as e:
            return {"security_id": security_dir.name, "status": "failed", "errors": [str(e)]}

    # ------------------------------------------------------------------
    def validate(self, context: Dict[str, Any] = None) -> Dict[str, Any]:
        """
        âš™ï¸ Warehouse ì „ì²´ ê²€ì¦ í”„ë¡œì„¸ìŠ¤
        1ï¸âƒ£ ë³‘ë ¬ security_id ë‹¨ìœ„ Pandera ê²€ì¦
        2ï¸âƒ£ Snapshot ë‹¨ìœ„ Soda Core ê²€ì¦ (typeë³„)
        """
        total_checked, failed_tickers = 0, []
        all_results = []

        if not self.dataset_path.exists():
            raise FileNotFoundError(f"âŒ Dataset path not found: {self.dataset_path}")

        security_dirs = [p for p in self.dataset_path.glob("security_id=*") if p.is_dir()]
        if not security_dirs:
            self.log.warning("âš ï¸ No fundamentals security_id folders found")
            return {"status": "empty", "checked_tickers": 0}

        self.log.info(f"ğŸ“Š Validating {len(security_dirs)} fundamentals records")

        # âœ… ë³‘ë ¬ ì‹¤í–‰
        with ThreadPoolExecutor(max_workers=8) as executor:
            futures = {executor.submit(self._validate_security_folder, d): d for d in security_dirs}
            for future in as_completed(futures):
                res = future.result()
                all_results.append(res)
                total_checked += 1
                if res["status"] != "success":
                    failed_tickers.append(res["security_id"])

        # âœ… Snapshot ë‹¨ìœ„ Soda ê²€ì¦ (typeë³„)
        df_summary = pd.DataFrame(all_results)
        soda_results = {}

        for fund_type in df_summary["fund_type"].dropna().unique():
            soda_path = self.check_root / f"fundamentals_{fund_type}.yml"
            if not soda_path.exists():
                self.log.warning(f"âš ï¸ No Soda check found for {fund_type}")
                continue
            self.log.info(f"ğŸ§ª Running Soda validation for {fund_type}")
            soda_results[fund_type] = self._run_soda_duckdb_validation(df_summary, soda_path)

        # âœ… ê²°ê³¼ ì €ì¥
        failed = len(failed_tickers)
        result = {
            "dataset": self.domain,
            "layer": self.layer,
            "country_code": self.country_code,
            "trd_dt": self.trd_dt,
            "checked_tickers": total_checked,
            "failed_tickers": failed_tickers,
            "failed_count": failed,
            "status": "failed" if failed > 0 else "success",
            "validated_at": datetime.now(timezone.utc).isoformat(),
        }

        meta_path = self.dataset_path / "_last_validated.json"
        with open(meta_path, "w", encoding="utf-8") as f:
            json.dump(result, f, indent=2, ensure_ascii=False)

        self.log.info(
            f"ğŸ¯ Fundamentals warehouse validation complete | total={total_checked} | failed={failed} | path={meta_path}"
        )

        if failed > 0:
            raise ValueError(f"âŒ Fundamentals warehouse validation failed: {failed} tickers")

        return result
