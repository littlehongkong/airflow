import os
import json
import tempfile
import pandas as pd
import pandera.pandas as pa
from pandera import Column, Check
from plugins.validators.base_validator import BaseDataValidator
from soda.scan import Scan
import duckdb
import yaml


class FundamentalValidator(BaseDataValidator):
    """ì£¼ì‹/ETF ì¬ë¬´ì œí‘œ í†µí•© ê²€ì¦ Validator"""

    BATCH_SIZE = 1000

    def __init__(self, exchange_code: str, trd_dt: str, data_domain: str = "fundamentals"):
        super().__init__(exchange_code, trd_dt, data_domain)
        self.stock_schema = self._get_stock_schema()
        self.etf_schema = self._get_etf_schema()

    # ============================================================
    # 1ï¸âƒ£ JSON flatten í—¬í¼
    # ============================================================
    @staticmethod
    def _to_float(v):
        if v in [None, "", "NA", "NaN", "N/A", "null", "None"]:
            return None
        try:
            return float(v)
        except (TypeError, ValueError):
            try:
                return float(str(v).replace(",", ""))
            except Exception:
                return None

    def _extract_financials(self, record: dict):
        fin = record.get("Financials", {})
        general = record.get("General", {})
        code = general.get("Code")
        stock_type = general.get("Type", "").lower()

        data = {"code": code, "stock_type": stock_type}

        if stock_type == 'common stock':

            # ------------------------
            # ğŸ“Š ì†ìµê³„ì‚°ì„œ (Income Statement)
            # ------------------------
            income_q = fin.get("Income_Statement", {}).get("quarterly", {})
            if income_q:
                latest_q = max(income_q.keys())
                item = income_q[latest_q]
                data["totalRevenue"] = self._to_float(item.get("totalRevenue"))
                data["operatingIncome"] = self._to_float(item.get("operatingIncome"))
                data["netIncome"] = self._to_float(item.get("netIncome"))
            else:
                data["totalRevenue"] = None
                data["operatingIncome"] = None
                data["netIncome"] = None

            # ------------------------
            # ğŸ’° ëŒ€ì°¨ëŒ€ì¡°í‘œ (Balance Sheet)
            # ------------------------
            bs_q = fin.get("Balance_Sheet", {}).get("quarterly", {})
            if bs_q:
                latest_q = max(bs_q.keys())
                item = bs_q[latest_q]
                data["totalAssets"] = self._to_float(item.get("totalAssets"))
                data["totalLiab"] = self._to_float(item.get("totalLiab"))
                data["totalStockholderEquity"] = self._to_float(item.get("totalStockholderEquity"))
            else:
                data["totalAssets"] = None
                data["totalLiab"] = None
                data["totalStockholderEquity"] = None

            # ------------------------
            # ğŸ’µ í˜„ê¸ˆíë¦„í‘œ (Cash Flow)
            # ------------------------
            cf_q = fin.get("Cash_Flow", {}).get("quarterly", {})
            if cf_q:
                latest_q = max(cf_q.keys())
                item = cf_q[latest_q]

                # âœ… ì˜ì—…í™œë™ í˜„ê¸ˆíë¦„
                data["cashflow_operating"] = self._to_float(
                    item.get("totalCashFromOperatingActivities")
                    or item.get("cashFlowsOtherOperating")
                    or item.get("freeCashFlow")
                )

                # âœ… íˆ¬ìí™œë™ í˜„ê¸ˆíë¦„
                data["cashflow_investing"] = self._to_float(
                    item.get("totalCashflowsFromInvestingActivities")
                    or item.get("investments")
                    or item.get("capitalExpenditures")
                    or item.get("otherCashflowsFromInvestingActivities")
                    or item.get("salePurchaseOfStock")
                )

                # âœ… ì¬ë¬´í™œë™ í˜„ê¸ˆíë¦„
                data["cashflow_financing"] = self._to_float(
                    item.get("totalCashFromFinancingActivities")
                    or item.get("dividendsPaid")
                    or item.get("netBorrowings")
                    or item.get("issuanceOfCapitalStock")
                    or item.get("otherCashflowsFromFinancingActivities")
                )
            else:
                data["cashflow_operating"] = None
                data["cashflow_investing"] = None
                data["cashflow_financing"] = None

        elif "ETF_Data" in record:

            etf_data = record.get('ETF_Data', {})

            data["etf_total_assets"] = self._to_float(etf_data.get('TotalAssets'))
            data["etf_holdings_count"] = self._to_float(etf_data.get('Holdings_Count'))
            data["etf_expense_ratio"] = self._to_float(etf_data.get('NetExpenseRatio'))
            data["etf_yield"] = self._to_float(etf_data.get('Yield'))

        return data

    # ============================================================
    # 2ï¸âƒ£ Pandera Schema ì •ì˜
    # ============================================================
    def _get_stock_schema(self):
        return pa.DataFrameSchema(
            columns={
                "totalRevenue": Column(float, nullable=True),
                "operatingIncome": Column(float, nullable=True),
                "netIncome": Column(float, nullable=True),
                "totalAssets": Column(float, nullable=True),
                "totalLiab": Column(float, nullable=True),
                "totalStockholderEquity": Column(float, nullable=True),
                # ìƒˆë¡œìš´ í˜„ê¸ˆíë¦„ í•­ëª©
                "cashflow_operating": Column(float, nullable=True),
                "cashflow_investing": Column(float, nullable=True),
                "cashflow_financing": Column(float, nullable=True)
            },
            checks=[
                Check(
                    lambda df: (
                            abs(df["totalAssets"].fillna(0)
                                - (df["totalLiab"].fillna(0) + df["totalStockholderEquity"].fillna(0)))
                            <= (df["totalAssets"].abs() * 0.1)
                    ),
                    error="ìì‚°=ë¶€ì±„+ìë³¸ ë¶ˆì¼ì¹˜ (5% ì´ìƒ ì°¨ì´)"
                )
            ],
            strict=False,
        )

    def _get_etf_schema(self):
        return pa.DataFrameSchema(
            columns={
                "etf_total_assets": Column(float, nullable=True),
                "etf_holdings_count": Column(float, nullable=True),
                "etf_expense_ratio": Column(float, nullable=True),
                "etf_yield": Column(float, nullable=True),
            },
            strict=False,
        )

    # ============================================================
    # 3ï¸âƒ£ ì „ì²´ ê²€ì¦ ì‹¤í–‰
    # ============================================================
    def validate(self, **kwargs):
        raw_dir = self._get_lake_path("raw")
        files = [f for f in os.listdir(raw_dir) if f.endswith(".json")]
        if not files:
            raise FileNotFoundError(f"âš ï¸ ê²€ì¦ ëŒ€ìƒ JSON íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {raw_dir}")

        all_records, valid_records = [], []
        for file in files:
            path = os.path.join(raw_dir, file)
            with open(path, "r", encoding="utf-8") as f:
                record = json.load(f)
                all_records.append(record)

        print(f"ğŸ“¦ ì´ {len(all_records)}ê°œ ì¢…ëª© ë°ì´í„° ê²€ì¦ ì‹œì‘ (batch={self.BATCH_SIZE})")

        # âœ… Batch ë‹¨ìœ„ Pandera ê²€ì¦
        for i in range(0, len(all_records), self.BATCH_SIZE):
            batch_records = all_records[i:i + self.BATCH_SIZE]
            df = pd.DataFrame([self._extract_financials(r) for r in batch_records])
            print('df.columns :: ' ,df.columns)
            # ì¢…ëª© íƒ€ì…ë³„ ë¶„ë¦¬
            df_stock = df[df["stock_type"].str.contains("common stock", case=False, na=False)]
            df_etf = df[df["stock_type"].str.contains("etf", case=False, na=False)]

            print('df_stock :: ' ,df_stock)
            print('df_etf :: ' ,df_etf)

            try:
                if not df_stock.empty:
                    self.stock_schema.validate(df_stock)
                if not df_etf.empty:
                    self.etf_schema.validate(df_etf)
                valid_records.extend(df.to_dict("records"))
                print(f"âœ… Pandera ê²€ì¦ í†µê³¼ ({len(df)}/{len(all_records)})")
            except Exception as e:
                print(f"âŒ Pandera ê²€ì¦ ì‹¤íŒ¨ (batch_{i // self.BATCH_SIZE}): {e}")

        if not valid_records:
            raise AssertionError("âŒ ëª¨ë“  ì¢…ëª©ì˜ Pandera ê²€ì¦ ì‹¤íŒ¨")

        merged_df = pd.DataFrame(valid_records)
        temp_path = os.path.join(raw_dir, "_merged_temp.parquet")
        merged_df.to_parquet(temp_path, index=False)
        print(f"ğŸ“„ ë‹¨ì¼ ë³‘í•© íŒŒì¼ ìƒì„± ì™„ë£Œ: {temp_path}")

        # âœ… ETF / STOCK ë¶„ë¦¬ Soda ê²€ì¦
        stock_df = merged_df[merged_df["stock_type"].str.contains("Common Stock", case=False, na=False)]
        etf_df = merged_df[merged_df["stock_type"].str.contains("ETF", case=False, na=False)]

        if not stock_df.empty:
            stock_temp_path = os.path.join(raw_dir, "_stock_temp.parquet")
            stock_df.to_parquet(stock_temp_path, index=False)
            self._run_soda_on_parquet(stock_temp_path, mode="stock")

        if not etf_df.empty:
            etf_temp_path = os.path.join(raw_dir, "_etf_temp.parquet")
            etf_df.to_parquet(etf_temp_path, index=False)
            self._run_soda_on_parquet(etf_temp_path, mode="etf")

        validated_path = os.path.join(
            self._get_lake_path("validated"),
            "fundamentals.parquet"
        )
        merged_df.to_parquet(validated_path, index=False)
        print(f"ğŸ¯ ê²€ì¦ ì™„ë£Œ ë°ì´í„° ì €ì¥: {validated_path}")

    # ============================================================
    # 4ï¸âƒ£ Soda Core ì‹¤í–‰ ë¡œì§
    # ============================================================
    def _run_soda_on_parquet(self, parquet_path: str, mode: str = "stock"):
        """Soda Coreë¡œ parquet íŒŒì¼ ê²€ì¦ (mode: stock / etf)"""
        base_dir = "/opt/airflow/plugins/soda/checks"
        soda_check_file = os.path.join(base_dir, f"fundamentals_{mode}_checks.yml")

        if not os.path.exists(soda_check_file):
            print(f"âš ï¸ {soda_check_file} ì—†ìŒ â€” ê±´ë„ˆëœ€.")
            return

        tmp_config = {
            "data_source my_duckdb": {
                "type": "duckdb",
                "path": ":memory:",
            }
        }

        with tempfile.NamedTemporaryFile(mode="w", suffix=".yml", delete=False) as tmp_file:
            yaml.dump(tmp_config, tmp_file)
            tmp_config_path = tmp_file.name

        scan = Scan()
        scan.set_data_source_name("my_duckdb")
        scan.add_configuration_yaml_file(tmp_config_path)
        scan.add_sodacl_yaml_files(soda_check_file)
        scan._data_source_manager.get_data_source("my_duckdb").connection.execute(
            f"CREATE TABLE fundamentals AS SELECT * FROM read_parquet('{parquet_path}')"
        )

        exit_code = scan.execute()
        print(f"ğŸ§ª Soda Scan ì™„ë£Œ (exit_code={exit_code}, mode={mode})")

        # ğŸš¨ Soda ê²€ì¦ ì‹¤íŒ¨ ì‹œ ì˜ˆì™¸ ë°œìƒì‹œì¼œ Task ì‹¤íŒ¨ ì²˜ë¦¬
        if exit_code != 0:
            raise AssertionError(f"âŒ Soda ê²€ì¦ ì‹¤íŒ¨: exit_code={exit_code}, mode={mode}")

        os.unlink(tmp_config_path)

