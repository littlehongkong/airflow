import os
import json
import tempfile
import pandas as pd
import pandera.pandas as pa
from pandera import Column, Check
from plugins.validators.base_validator import BaseDataValidator
from soda.scan import Scan
import duckdb
import yaml


class FundamentalValidator(BaseDataValidator):
    """ì£¼ì‹/ETF ì¬ë¬´ì œí‘œ í†µí•© ê²€ì¦ Validator"""

    BATCH_SIZE = 1000

    def __init__(self, exchange_code: str, trd_dt: str, data_domain: str = "fundamentals"):
        super().__init__(exchange_code, trd_dt, data_domain)
        self.stock_schema = self._get_stock_schema()
        self.etf_schema = self._get_etf_schema()

    # ============================================================
    # 1ï¸âƒ£ JSON flatten í—¬í¼
    # ============================================================
    @staticmethod
    def _to_float(v):
        """ë¬¸ìì—´, ì •ìˆ˜, ì‹¤ìˆ˜, None ëª¨ë‘ ì•ˆì „í•˜ê²Œ floatìœ¼ë¡œ ë³€í™˜"""
        if v is None or v == "" or v == "NA" or v == "null":
            return None
        try:
            return float(v)
        except (TypeError, ValueError):
            try:
                return float(str(v).replace(",", ""))
            except Exception:
                return None

    def _extract_financials(self, record: dict):
        fin = record.get("Financials", {})
        general = record.get("General", {})
        code = general.get("Code")
        stock_type = general.get("Type", "").lower()

        data = {"code": code, "stock_type": stock_type}

        # ì†ìµê³„ì‚°ì„œ
        income_q = fin.get("Income_Statement", {}).get("quarterly", {})
        if income_q:
            latest_q = max(income_q.keys())
            item = income_q[latest_q]
            data["totalRevenue"] = self._to_float(item.get("totalRevenue"))
            data["operatingIncome"] = self._to_float(item.get("operatingIncome"))
            data["netIncome"] = self._to_float(item.get("netIncome"))

        # ëŒ€ì°¨ëŒ€ì¡°í‘œ
        bs_q = fin.get("Balance_Sheet", {}).get("quarterly", {})
        if bs_q:
            latest_q = max(bs_q.keys())
            item = bs_q[latest_q]
            data["totalAssets"] = self._to_float(item.get("totalAssets"))
            data["totalLiab"] = self._to_float(item.get("totalLiab"))
            data["totalStockholderEquity"] = self._to_float(item.get("totalStockholderEquity"))

        # í˜„ê¸ˆíë¦„í‘œ
        cf_q = fin.get("Cash_Flow", {}).get("quarterly", {})
        if cf_q:
            latest_q = max(cf_q.keys())
            item = cf_q[latest_q]
            data["totalCashflowsFromInvestingActivities"] = self._to_float(
                item.get("totalCashflowsFromInvestingActivities"))
            data["totalCashFromFinancingActivities"] = self._to_float(item.get("totalCashFromFinancingActivities"))
            data["totalCashFromOperatingActivities"] = self._to_float(item.get("totalCashFromOperatingActivities"))

        # ETF
        etf_data = record.get("ETF_Data", {})
        if etf_data:
            data["etf_total_assets"] = self._to_float(etf_data.get("TotalAssets"))
            data["etf_holdings_count"] = self._to_float(etf_data.get("Holdings_Count"))
            data["etf_expense_ratio"] = self._to_float(etf_data.get("NetExpenseRatio"))
            data["etf_yield"] = self._to_float(etf_data.get("Yield"))

        return data

    # ============================================================
    # 2ï¸âƒ£ Pandera Schema ì •ì˜
    # ============================================================
    def _get_stock_schema(self):
        return pa.DataFrameSchema(
            columns={
                "totalRevenue": Column(float, nullable=True),
                "operatingIncome": Column(float, nullable=True),
                "netIncome": Column(float, nullable=True),
                "totalAssets": Column(float, nullable=True),
                "totalLiab": Column(float, nullable=True),
                "totalStockholderEquity": Column(float, nullable=True),
                "totalCashflowsFromInvestingActivities": Column(float, nullable=True),
                "totalCashFromFinancingActivities": Column(float, nullable=True),
                "totalCashFromOperatingActivities": Column(float, nullable=True),
            },
            checks=[
                Check(
                    lambda df: (
                            abs(df["totalAssets"].fillna(0)
                                - (df["totalLiab"].fillna(0) + df["totalStockholderEquity"].fillna(0))) < 1e9
                    ),
                    error="ìì‚°=ë¶€ì±„+ìë³¸ ë¶ˆì¼ì¹˜",
                )
            ],
            strict=False,
        )

    def _get_etf_schema(self):
        return pa.DataFrameSchema(
            columns={
                "etf_total_assets": Column(float, nullable=True),
                "etf_holdings_count": Column(float, nullable=True),
                "etf_expense_ratio": Column(float, nullable=True),
                "etf_yield": Column(float, nullable=True),
            },
            strict=False,
        )

    # ============================================================
    # 3ï¸âƒ£ ì „ì²´ ê²€ì¦ ì‹¤í–‰
    # ============================================================
    def validate(self, **kwargs):
        raw_dir = self._get_lake_path("raw")
        files = [f for f in os.listdir(raw_dir) if f.endswith(".json")]
        if not files:
            raise FileNotFoundError(f"âš ï¸ ê²€ì¦ ëŒ€ìƒ JSON íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {raw_dir}")

        all_records, valid_records = [], []
        for file in files:
            path = os.path.join(raw_dir, file)
            with open(path, "r", encoding="utf-8") as f:
                record = json.load(f)
                all_records.append(record)

        print(f"ğŸ“¦ ì´ {len(all_records)}ê°œ ì¢…ëª© ë°ì´í„° ê²€ì¦ ì‹œì‘ (batch={self.BATCH_SIZE})")

        # âœ… Batch ë‹¨ìœ„ Pandera ê²€ì¦
        for i in range(0, len(all_records), self.BATCH_SIZE):
            batch_records = all_records[i:i + self.BATCH_SIZE]
            df = pd.DataFrame([self._extract_financials(r) for r in batch_records])

            # ì¢…ëª© íƒ€ì…ë³„ ë¶„ë¦¬
            df_stock = df[df["stock_type"].str.contains("stock", case=False, na=False)]
            df_etf = df[df["stock_type"].str.contains("etf", case=False, na=False)]

            try:
                if not df_stock.empty:
                    self.stock_schema.validate(df_stock)
                if not df_etf.empty:
                    self.etf_schema.validate(df_etf)
                valid_records.extend(df.to_dict("records"))
                print(f"âœ… Pandera ê²€ì¦ í†µê³¼ ({len(df)}/{len(all_records)})")
            except Exception as e:
                print(f"âŒ Pandera ê²€ì¦ ì‹¤íŒ¨ (batch_{i // self.BATCH_SIZE}): {e}")

        if not valid_records:
            raise AssertionError("âŒ ëª¨ë“  ì¢…ëª©ì˜ Pandera ê²€ì¦ ì‹¤íŒ¨")

        merged_df = pd.DataFrame(valid_records)
        temp_path = os.path.join(raw_dir, "_merged_temp.parquet")
        merged_df.to_parquet(temp_path, index=False)
        print(f"ğŸ“„ ë‹¨ì¼ ë³‘í•© íŒŒì¼ ìƒì„± ì™„ë£Œ: {temp_path}")

        # âœ… ETF / STOCK ë¶„ë¦¬ Soda ê²€ì¦
        stock_df = merged_df[merged_df["stock_type"].str.contains("stock", case=False, na=False)]
        etf_df = merged_df[merged_df["stock_type"].str.contains("etf", case=False, na=False)]

        if not stock_df.empty:
            stock_temp_path = os.path.join(raw_dir, "_stock_temp.parquet")
            stock_df.to_parquet(stock_temp_path, index=False)
            self._run_soda_on_parquet(stock_temp_path, mode="stock")

        if not etf_df.empty:
            etf_temp_path = os.path.join(raw_dir, "_etf_temp.parquet")
            etf_df.to_parquet(etf_temp_path, index=False)
            self._run_soda_on_parquet(etf_temp_path, mode="etf")

        validated_path = os.path.join(
            self._get_lake_path("validated"),
            "fundamentals.parquet"
        )
        merged_df.to_parquet(validated_path, index=False)
        print(f"ğŸ¯ ê²€ì¦ ì™„ë£Œ ë°ì´í„° ì €ì¥: {validated_path}")

    # ============================================================
    # 4ï¸âƒ£ Soda Core ì‹¤í–‰ ë¡œì§
    # ============================================================
    def _run_soda_on_parquet(self, parquet_path: str, mode: str = "stock"):
        """Soda Coreë¡œ parquet íŒŒì¼ ê²€ì¦ (mode: stock / etf)"""
        base_dir = "/opt/airflow/plugins/soda/checks"
        soda_check_file = os.path.join(base_dir, f"fundamentals_{mode}_checks.yml")

        if not os.path.exists(soda_check_file):
            print(f"âš ï¸ {soda_check_file} ì—†ìŒ â€” ê±´ë„ˆëœ€.")
            return

        tmp_config = {
            "data_source my_duckdb": {
                "type": "duckdb",
                "path": ":memory:",
            }
        }

        with tempfile.NamedTemporaryFile(mode="w", suffix=".yml", delete=False) as tmp_file:
            yaml.dump(tmp_config, tmp_file)
            tmp_config_path = tmp_file.name

        scan = Scan()
        scan.set_data_source_name("my_duckdb")
        scan.add_configuration_yaml_file(tmp_config_path)
        scan.add_sodacl_yaml_files(soda_check_file)
        scan._data_source_manager.get_data_source("my_duckdb").connection.execute(
            f"CREATE TABLE fundamentals AS SELECT * FROM read_parquet('{parquet_path}')"
        )

        exit_code = scan.execute()
        print(f"ğŸ§ª Soda Scan ì™„ë£Œ (exit_code={exit_code}, mode={mode})")

        # ğŸš¨ Soda ê²€ì¦ ì‹¤íŒ¨ ì‹œ ì˜ˆì™¸ ë°œìƒì‹œì¼œ Task ì‹¤íŒ¨ ì²˜ë¦¬
        if exit_code != 0:
            raise AssertionError(f"âŒ Soda ê²€ì¦ ì‹¤íŒ¨: exit_code={exit_code}, mode={mode}")

        os.unlink(tmp_config_path)

