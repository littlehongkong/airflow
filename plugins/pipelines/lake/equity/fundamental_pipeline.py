# plugins/pipelines/fundamental_pipeline.py
from typing import Dict, List
import math
import json
from pathlib import Path
from plugins.hooks.eodhd_hook import EODHDHook
from plugins.pipelines.base_equity_pipeline import BaseEquityPipeline


class FundamentalPipeline(BaseEquityPipeline):
    """
    í€ë”ë©˜í„¸(Fundamentals) ë°ì´í„° ìˆ˜ì§‘ ë° ì ì¬ íŒŒì´í”„ë¼ì¸
    --------------------------------------------------------
    âœ… ë‹¤ë¥¸ íŒŒì´í”„ë¼ì¸(ExchangeInfoPipeline ë“±)ê³¼ ë™ì¼í•œ êµ¬ì¡° ìœ ì§€
    âœ… ë‹¨, ì¢…ëª©ë³„ í˜¸ì¶œ ë° ë°°ì¹˜ ë‹¨ìœ„ ì €ì¥(batch_0001.jsonl ë“±)ë§Œ ì°¨ë³„í™”
    """

    def __init__(self, data_domain: str, exchange_code: str, trd_dt: str):
        super().__init__(data_domain, exchange_code, trd_dt)
        self.hook = EODHDHook()
        self.batch_size = 40  # âœ… ë°°ì¹˜ ë‹¨ìœ„ ì €ì¥ (40ì¢…ëª©ì”©)

    # ------------------------------------------------------------------
    # âœ… 1ï¸âƒ£ Fetch
    # ------------------------------------------------------------------
    def fetch(self, **kwargs):
        tickers = kwargs.get("tickers")

        assert tickers, 'âš  ìˆ˜ì§‘í•  ì¢…ëª© ëª©ë¡ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.'

        all_records = []
        source_meta = None

        for ticker in tickers:
            try:
                data = self.hook.get_fundamentals(
                    symbol=ticker
                )
                records, meta = self._standardize_fetch_output(data)  # âœ… ì–¸íŒ¨í‚¹
                all_records.extend(records)  # âœ… flatten list[dict]
                source_meta = meta  # ë§ˆì§€ë§‰ ë©”íƒ€ë§Œ ê¸°ë¡ (í˜¹ì€ ë³‘í•© ê°€ëŠ¥)

            except Exception as e:
                self.log.error(f"âŒ {ticker} í€ë”ë©˜í„¸ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
                continue

        self.log.info(f"âœ… í€ë”ë©˜í„¸ ìˆ˜ì§‘ ì™„ë£Œ: ì´ {len(all_records)}ê±´")
        return all_records, source_meta

    # ------------------------------------------------------------------
    # âœ… 2ï¸âƒ£ Load
    # ------------------------------------------------------------------
    def load(self, records: List[Dict], **kwargs):
        """
        ìˆ˜ì§‘ëœ í€ë”ë©˜í„¸ ë°ì´í„°ë¥¼ ë°°ì¹˜ ë‹¨ìœ„ë¡œ JSON Linesë¡œ ì ì¬í•©ë‹ˆë‹¤.
        ë‹¤ë¥¸ íŒŒì´í”„ë¼ì¸ê³¼ ë™ì¼í•œ êµ¬ì¡°ë¥¼ ìœ ì§€í•˜ë©°, íŒŒì¼ëª…ë§Œ batch_*.jsonlë¡œ êµ¬ë¶„í•©ë‹ˆë‹¤.
        """

        if not records:
            self.log.warning("âš ï¸ ì ì¬í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        # ğŸ”¹ ê³µí†µ ë©”íƒ€ë°ì´í„° ë° ì €ì¥ ê²½ë¡œ í™•ë³´
        kwargs["partition_key_name"] = kwargs.get("partition_key_name", "trd_dt")
        kwargs["geo_key_name"] = kwargs.get("geo_key_name", "exchange_code")

        target_dir, base_metadata = self._get_lake_path_and_metadata(**kwargs)

        total_records = len(records)
        total_batches = math.ceil(total_records / self.batch_size)

        for i in range(total_batches):
            batch_records = records[i * self.batch_size : (i + 1) * self.batch_size]
            file_name = f"batch_{i + 1:04d}.jsonl"

            self.log.info(f"{file_name} >> {batch_records}")

            self._write_records_to_lake(
                records=batch_records,
                target_dir=target_dir,
                base_metadata=base_metadata,
                file_name=file_name,
            )

        meta = kwargs.get("meta", {}) or {}

        # ğŸ”¹ _source_meta.json ì €ì¥
        self._save_source_meta(
            target_dir=target_dir,
            record_count=total_records,
            source_meta={**meta, "batches": total_batches}
        )

        self.log.info(f"âœ… í€ë”ë©˜í„¸ ë°ì´í„° {total_batches}ê°œ ë°°ì¹˜ ì €ì¥ ì™„ë£Œ ({total_records}ê±´)")

    # ------------------------------------------------------------------
    # âœ… 3ï¸âƒ£ Fetch + Load í†µí•© ì‹¤í–‰
    # ------------------------------------------------------------------
    def fetch_and_load(self, **kwargs):
        """
        í€ë”ë©˜í„¸ ë°ì´í„° ìˆ˜ì§‘ ë° ì ì¬ (ë°°ì¹˜ ì¢…ëª© ê¸°ë°˜)
        - Airflow Dynamic Taskë¡œ ì „ë‹¬ëœ batch_symbolsë§Œ ì²˜ë¦¬
        - ì´ë¯¸ ìˆ˜ì§‘ëœ ì¢…ëª© ì œì™¸ (ì¦ë¶„ ìˆ˜ì§‘)
        - mode='new_listing' ì‹œ í–¥í›„ í™•ì¥ ê°€ëŠ¥
        """

        self.log.info(f"ğŸš€ í€ë”ë©˜í„¸ ìˆ˜ì§‘ íŒŒì´í”„ë¼ì¸ ì‹œì‘ ({self.exchange_code}, {self.trd_dt})")

        # ----------------------------------------------------------------------
        # âœ… 0ï¸âƒ£ í•„ìˆ˜ íŒŒë¼ë¯¸í„° í™•ì¸
        # ----------------------------------------------------------------------
        exchange_code = kwargs.get("exchange_code", self.exchange_code)
        if not exchange_code:
            raise ValueError("exchange_code ê°’ì´ ì—†ìŠµë‹ˆë‹¤ (ì˜ˆ: exchange_code=KO)")
        self.exchange_code = exchange_code

        batch_symbols = kwargs.get("batch_symbols")
        if not batch_symbols:
            raise ValueError("batch_symbols ê°’ì´ ì—†ìŠµë‹ˆë‹¤ â€” Dynamic Taskì—ì„œ ì „ë‹¬ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.")
        self.log.info(f"ğŸ“¦ ì „ë‹¬ë°›ì€ ë°°ì¹˜ ì¢…ëª© ìˆ˜: {len(batch_symbols):,}")

        # âœ… load í˜¸ì¶œ ì‹œì—ë„ ëª…ì‹œì ìœ¼ë¡œ ì „ë‹¬
        kwargs["exchange_code"] = self.exchange_code

        # ----------------------------------------------------------------------
        # âœ… 1ï¸âƒ£ ì¢…ëª© ë¡œë“œ ì œê±° (â†’ ì „ë‹¬ë°›ì€ batch_symbolsë§Œ ì‚¬ìš©)
        # ----------------------------------------------------------------------
        mode = kwargs.get("mode", "incremental")

        # ê¸°ì¡´ ì¢…ëª©(ì´ë¯¸ ìˆ˜ì§‘ëœ ì¢…ëª©) ì¡°íšŒ
        target_dir, base_metadata = self._get_lake_path_and_metadata(**kwargs)
        existing_symbols = self._get_existing_symbols(target_dir)

        # ----------------------------------------------------------------------
        # âœ… 2ï¸âƒ£ ëŒ€ìƒ ì‹¬ë³¼ ê²°ì •
        # ----------------------------------------------------------------------
        if mode == "new_listing":
            self.log.info("âœ¨ ì‹ ê·œìƒì¥ ì „ìš© ëª¨ë“œ í™œì„±í™” â€” ì „ë‹¬ë°›ì€ ì¢…ëª© ì¤‘ ë¯¸ìˆ˜ì§‘ëœ ì¢…ëª©ë§Œ ìˆ˜ì§‘")
            pending_symbols = [s for s in batch_symbols if s not in existing_symbols]
        else:
            pending_symbols = [s for s in batch_symbols if s not in existing_symbols]

        if not pending_symbols:
            self.log.info(f"âœ… ì „ë‹¬ë°›ì€ {len(batch_symbols)}ê°œ ì¢…ëª© ì¤‘ ëª¨ë‘ ì´ë¯¸ ìˆ˜ì§‘ë¨ â€” ìŠ¤í‚µ")
            return {"status": "skipped", "records": 0}

        self.log.info(f"ğŸ“ˆ ìˆ˜ì§‘ ëŒ€ìƒ: {len(pending_symbols)} / {len(batch_symbols)} ì¢…ëª©")

        # ----------------------------------------------------------------------
        # âœ… 3ï¸âƒ£ API í˜¸ì¶œ ë° ë°°ì¹˜ ë‹¨ìœ„ ì ì¬
        # ----------------------------------------------------------------------
        batch_index = kwargs.get("batch_index")
        if batch_index is not None:
            batch_name = f"batch_{int(batch_index):04d}.jsonl"
        else:
            batch_name = "batch_dynamic.jsonl"  # fallback (ë‹¨ì¼ ì‹¤í–‰ ì‹œ)

        total_records = 0
        total_batches = 0

        batch_records = []
        for sym in pending_symbols:
            try:
                data = self.hook.get_fundamentals(symbol=f"{sym}.{exchange_code}")
                if not data:
                    self.log.warning(f"âš ï¸ {sym} ë°ì´í„° ì—†ìŒ â€” ê±´ë„ˆëœ€")
                    continue

                recs, meta = self._standardize_fetch_output(data)
                batch_records.extend(recs)
            except Exception as e:
                self.log.error(f"âŒ {sym} ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")
                continue

        if not batch_records:
            self.log.warning(f"âš ï¸ {len(pending_symbols)}ê°œ ì¢…ëª© ëª¨ë‘ ë°ì´í„° ì—†ìŒ â€” ë°°ì¹˜ ìŠ¤í‚µ")
            return {"status": "skipped", "records": 0}

        # ê¸°ì¡´ íŒŒì¼ì´ ìˆìœ¼ë©´ ì‚­ì œ
        batch_path = target_dir / batch_name
        if batch_path.exists():
            batch_path.unlink()
            self.log.info(f"ğŸ§¹ ê¸°ì¡´ ë°°ì¹˜ íŒŒì¼ ì‚­ì œ í›„ ìƒˆë¡œ ì €ì¥: {batch_name}")

        self._write_records_to_lake(
            records=batch_records,
            target_dir=target_dir,
            base_metadata=base_metadata,
            file_name=batch_name,
            mode="overwrite",
        )

        total_records += len(batch_records)
        total_batches += 1
        self.log.info(f"âœ… í€ë”ë©˜í„¸ ë°ì´í„° {total_batches}ê°œ ë°°ì¹˜ ì €ì¥ ì™„ë£Œ ({len(batch_records)}ê±´)")

        # ----------------------------------------------------------------------
        # âœ… 4ï¸âƒ£ ì›ì²œ ë©”íƒ€ ì €ì¥
        # ----------------------------------------------------------------------
        self._save_source_meta(
            target_dir=target_dir,
            record_count=total_records,
            source_meta={
                "vendor": "EODHD",
                "endpoint": "api/fundamentals",
                "batches": total_batches,
                "mode": mode,
                "batch_symbols": len(batch_symbols),
                "pending_symbols": len(pending_symbols),
            },
        )

        self.log.info(
            f"ğŸ¯ í€ë”ë©˜í„¸ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ ({self.exchange_code}, {self.trd_dt}) - "
            f"{total_records:,}ê±´ / {total_batches}ê°œ ë°°ì¹˜"
        )

        return {"status": "success", "records": total_records, "batches": total_batches}


    # -------------------------------------------------------------------
    def _get_existing_symbols(self, target_dir: Path) -> set[str]:
        """
        ì´ë¯¸ ìˆ˜ì§‘ëœ ì¢…ëª©ì½”ë“œë¥¼ ê¸°ì¡´ JSONL íŒŒì¼ì—ì„œ ì¶”ì¶œ
        """
        symbols = set()
        if not target_dir.exists():
            return symbols

        for f in target_dir.glob("batch_*.jsonl"):
            try:
                with open(f, "r", encoding="utf-8") as fp:
                    for line in fp:
                        rec = json.loads(line)
                        code = rec.get("General", {}).get("Code")
                        if code:
                            symbols.add(code)
            except Exception as e:
                self.log.warning(f"âš ï¸ {f} ì½ê¸° ì˜¤ë¥˜: {e}")
        self.log.info(f"ğŸ§­ ê¸°ì¡´ ìˆ˜ì§‘ ì¢…ëª© {len(symbols):,}ê±´ í™•ì¸ë¨")
        return symbols