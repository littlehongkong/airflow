import logging
import json
from abc import ABC
from pathlib import Path
from typing import List, Dict, Any, Callable


# Data Lake Root Path: Airflow ì»¨í…Œì´ë„ˆ ë‚´ë¶€ ì ˆëŒ€ ê²½ë¡œë¡œ ì„¤ì • (ê¶Œí•œ ì˜¤ë¥˜ ë°©ì§€)
LOCAL_DATA_LAKE_PATH = Path("/opt/airflow/data")

class HookMixin:
    hook: Any

class BaseEquityPipeline(HookMixin, ABC):  # DataPipelineInterface ìƒì† ìœ ì§€ ê°€ì •
    """
    Equity ê´€ë ¨ ë°ì´í„° íŒŒì´í”„ë¼ì¸ì˜ ê³µí†µ ê¸°ë°˜ í´ë˜ìŠ¤
    - Data Lake Raw Zone (File System) ê³µí†µ ì ì¬ ë¡œì§ ì œê³µ
    """

    def __init__(self, data_domain: str, exchange_code: str, trd_dt: str):  # table_name ëŒ€ì‹  data_domain ì‚¬ìš©
        self.data_domain = data_domain
        self.exchange_code = exchange_code
        self.trd_dt = trd_dt
        self.log = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
        # ë£¨íŠ¸ í´ë”ê°€ ì—†ìœ¼ë©´ ìƒì„± (ê¶Œí•œ ì˜¤ë¥˜ëŠ” í™˜ê²½ ì„¤ì •ìœ¼ë¡œ í•´ê²°í•´ì•¼ í•¨)
        LOCAL_DATA_LAKE_PATH.mkdir(parents=True, exist_ok=True)

        # ------------
        # ------------------------
        # 1ï¸âƒ£ ê³µí†µ ìœ í‹¸ë¦¬í‹°: íŒŒì¼ ê²½ë¡œ ìƒì„± ë° ë©”íƒ€ë°ì´í„° ë°˜í™˜
        # ------------------------------------

    def _get_lake_path_and_metadata(
            self,
            layer: str = "raw",  # raw / validated / staging / curated / snapshot
            **kwargs
    ) -> tuple[Path, dict]:
        """
        Hive íŒŒí‹°ì…”ë‹ êµ¬ì¡°ë¥¼ ë”°ë¥´ë˜, S3 êµ¬ì¡°ì™€ ë™ì¼í•˜ê²Œ ê²½ë¡œë¥¼ ìƒì„±.
        ì˜ˆì‹œ:
          raw/equity_prices/exchange_code=US/trd_dt=2025-10-15/
        """

        # 1ï¸âƒ£ ë‚ ì§œ íŒŒí‹°ì…˜ ê²°ì •
        date_value = self.trd_dt or kwargs.get("ds")
        date_partition_key = kwargs.get("partition_key_name", "trd_dt")

        # 2ï¸âƒ£ ì§€ë¦¬ì  íŒŒí‹°ì…˜ ê²°ì •
        geo_partition_key = kwargs.get("geo_partition_key", "exchange_code")
        geo_value = kwargs.get(geo_partition_key)
        if not geo_value:
            raise ValueError(f"{geo_partition_key} ê°’ì´ ì—†ìŠµë‹ˆë‹¤ (ì˜ˆ: exchange_code=KO)")

        # 3ï¸âƒ£ ê³„ì¸µ êµ¬ì¡°: /data_lake/raw/equity_prices/exchange_code=KO/trd_dt=2025-10-15
        target_dir = (
                LOCAL_DATA_LAKE_PATH
                / "data_lake"
                / layer
                / self.data_domain
                / f"{geo_partition_key}={geo_value}"
                / f"{date_partition_key}={date_value}"
        )
        target_dir.mkdir(parents=True, exist_ok=True)

        # 4ï¸âƒ£ ë©”íƒ€ë°ì´í„° êµ¬ì„±
        metadata = {
            "layer": layer,
            geo_partition_key: geo_value,
            date_partition_key: date_value,
            "source": self.data_domain,
        }

        return target_dir, metadata



    # ------------------------------------
    # 2ï¸âƒ£ ê³µí†µ: íŒŒì¼ ì ì¬ ë¡œì§ (í†µí•©)
    # ------------------------------------
    def _write_records_to_lake(
            self,
            records: List[Dict],
            target_dir: Path,
            base_metadata: Dict[str, str],
            file_name: str | Callable[[List[Dict]], str],
            mode: str = "overwrite",  # âœ… ê¸°ë³¸: ë®ì–´ì“°ê¸°
    ) -> Dict[str, Any]:
        """
        Data Lakeì— ë°ì´í„°ë¥¼ ì €ì¥í•˜ëŠ” ê³µí†µ í•¨ìˆ˜ (ë‹¨ì¼ JSONL ë²„ì „)
        - ëª¨ë“  ë°ì´í„°ìœ í˜• ê³µí†µ: 1ê°œ JSONL íŒŒì¼ì— ì €ì¥
        - mode:
            - "overwrite": ê¸°ì¡´ íŒŒì¼ êµì²´ (ê¸°ë³¸)
            - "append": ê¸°ì¡´ íŒŒì¼ ë’¤ì— ì¶”ê°€
        """
        if not records:
            self.log.info("ì €ì¥í•  ë ˆì½”ë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return {"count": 0, "target_path": str(target_dir)}

        target_dir.mkdir(parents=True, exist_ok=True)
        saved_count = 0

        # íŒŒì¼ ê²½ë¡œ ê²°ì •
        file_path = target_dir / (
            file_name if isinstance(file_name, str) else file_name(records)
        )

        # ëª¨ë“œ ì„¤ì •
        open_mode = "a" if mode == "append" else "w"

        try:
            with open(file_path, open_mode, encoding="utf-8") as f:
                for rec in records:
                    rec["metadata"] = base_metadata.copy()
                    f.write(json.dumps(rec, ensure_ascii=False) + "\n")
                    saved_count += 1

            self.log.info(
                f"âœ… JSONL íŒŒì¼ ì €ì¥ ì™„ë£Œ: {file_path} "
                f"({saved_count:,}ê±´, mode={mode})"
            )

        except Exception as e:
            self.log.error(f"âŒ JSONL ì €ì¥ ì‹¤íŒ¨: {str(e)}")
            raise

        return {"count": saved_count, "target_path": str(target_dir)}


    # --------------------------------------------------
    # âœ… 0ï¸âƒ£ record_count ê°•ì œ ì‚½ì… í—¬í¼
    # --------------------------------------------------
    @staticmethod
    def _enforce_record_count(result: Any, records: list | None = None) -> dict:
        """
        ëª¨ë“  íŒŒì´í”„ë¼ì¸ì—ì„œ ë°˜í™˜ ì‹œ record_count ëˆ„ë½ ë°©ì§€
        - result: íŒŒì´í”„ë¼ì¸ ë°˜í™˜ ê²°ê³¼
        - records: (ì„ íƒ) ì‹¤ì œ ë°ì´í„° ëª©ë¡ (len() ê³„ì‚°ìš©)
        """
        # ê²°ê³¼ê°€ None â†’ ê¸°ë³¸ê°’ ë°˜í™˜
        if result is None:
            count = len(records) if records is not None else 0
            return {"record_count": count}

        # dictê°€ ì•„ë‹Œ ê²½ìš° â†’ dictë¡œ ë³€í™˜
        if not isinstance(result, dict):
            result = {"result": result}

        # record_countê°€ ëˆ„ë½ëœ ê²½ìš° ìë™ ì¶”ê°€
        if "record_count" not in result:
            count = len(records) if records is not None else 0
            result["record_count"] = count

        return result

    def fetch_and_load(self, **kwargs):
        """
        ê³µí†µ: ë°ì´í„° ìˆ˜ì§‘ â†’ ì •ê·œí™” â†’ ì €ì¥ â†’ ì›ì²œ ë©”íƒ€ ê¸°ë¡
        - fetch()ê°€ (records, meta) íŠœí”Œì„ ë°˜í™˜í•´ë„ OK
        - fetch()ê°€ ì›ì‹œ ì‘ë‹µ(list|dict|None)ì„ ë°˜í™˜í•´ë„ OK
        """
        self.log.info(f"ğŸ“¡ Fetching data for {self.exchange_code} ({self.data_domain})")

        raw = self.fetch(**kwargs)

        # âœ… fetchê°€ (records, meta) íŠœí”Œì„ ë°˜í™˜í•˜ëŠ” ê²½ìš° ì§€ì›
        if isinstance(raw, tuple) and len(raw) == 2:
            records, source_meta = raw
            # recordsê°€ dictë©´ listë¡œ, Noneì´ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ
            if records is None:
                records = []
            elif isinstance(records, dict):
                records = [records]
        else:
            # âœ… ê¸°ì¡´ ì„¤ê³„: ì›ì‹œ ì‘ë‹µì„ í‘œì¤€í™”
            records, source_meta = self._standardize_fetch_output(raw)

        record_count = len(records)

        # âœ… ê²½ë¡œ/ë©”íƒ€ êµ¬ì„±
        target_dir, base_meta = self._get_lake_path_and_metadata(layer="raw", **kwargs)
        file_name = f"{self.data_domain}.jsonl"

        # âœ… ì €ì¥
        write_result = self._write_records_to_lake(
            records=records,
            target_dir=target_dir,
            base_metadata=base_meta,
            file_name=file_name,
        )

        # âœ… record_count ì¼ê´€ ë³´ì • (write_resultë¥¼ ì‹¤ì œë¡œ í™œìš©)
        if isinstance(write_result, dict):
            record_count = write_result.get("count", record_count)

        # âœ… ì›ì²œ ë©”íƒ€ ì €ì¥ (ì‹œê·¸ë‹ˆì²˜ í†µì¼)
        self._save_source_meta(target_dir, record_count, source_meta)

        # âœ… ë°˜í™˜ ê°ì²´ í‘œì¤€í™” (í•­ìƒ record_count í¬í•¨)
        result = {
            "record_count": record_count,
            "target_path": str(target_dir),
        }
        self.log.info(f"âœ… Completed fetch_and_load: {result}")
        return result

    def _save_source_meta(self, target_dir: Path, record_count: int, source_meta: dict | None = None):
        """ìˆ˜ì§‘ ì›ì²œ ì •ë³´ë¥¼ `_source_meta.json` í˜•íƒœë¡œ ì €ì¥"""
        meta_path = target_dir / "_source_meta.json"

        if "api_token" in source_meta.get("params", {}):
            source_meta["params"]["api_token"] = "***"

        with open(meta_path, "w", encoding="utf-8") as f:
            json.dump(source_meta, f, indent=2, ensure_ascii=False)
        self.log.info(f"ğŸ“˜ Source metadata saved: {meta_path}")

    def load_to_datalake(self, records: List[Dict], **kwargs) -> Dict:
        """
        ê³µí†µ Data Lake ì ì¬ ë¡œì§
        - Hive íŒŒí‹°ì…˜ êµ¬ì¡°
        - JSONL or ê°œë³„ JSON íŒŒì¼ ì €ì¥
        - _source_meta.json ìë™ ìƒì„±
        """
        kwargs["partition_key_name"] = "trd_dt"
        kwargs["geo_partition_key"] = kwargs.get("geo_partition_key", "exchange_code")

        target_dir, base_metadata = self._get_lake_path_and_metadata(**kwargs)
        file_name = f"{self.data_domain}.jsonl"

        # 1ï¸âƒ£ ì‹¤ì œ ë°ì´í„° ì €ì¥
        save_info = self._write_records_to_lake(
            records=records,
            target_dir=target_dir,
            base_metadata=base_metadata,
            file_name=file_name,
        )

        # 2ï¸âƒ£ ì›ì²œ ë©”íƒ€ì •ë³´ (_source_meta.json) ì¶”ê°€ ìƒì„±
        if hasattr(self, "source_meta") and self.source_meta:
            meta_path = target_dir / "_source_meta.json"
            with open(meta_path, "w", encoding="utf-8") as f:
                json.dump(self.source_meta, f, indent=2, ensure_ascii=False)

        return save_info

    def _standardize_fetch_output(self, data: Any) -> tuple[list[dict], dict]:
        """
        Fetch ê²°ê³¼ë¥¼ í•­ìƒ (records, meta) í˜•íƒœë¡œ ì •ê·œí™”í•©ë‹ˆë‹¤.
        - data: API ì‘ë‹µ (list or dict)
        - endpoint, params, vendorëŠ” Hook ê°ì²´ì—ì„œ ì§ì ‘ ê°€ì ¸ì˜µë‹ˆë‹¤.
        """
        # âœ… data í˜•ì‹ ì •ê·œí™”

        if data is None:
            records = []
        elif isinstance(data, list):
            records = data
        elif isinstance(data, dict):
            records = [data]
        else:
            self.log.warning(f"âš ï¸ Unexpected fetch type: {type(data)}")
            records = []

        # âœ… hookìœ¼ë¡œë¶€í„° ì›ì²œ ë©”íƒ€ì •ë³´ ì¶”ì¶œ
        meta = {
            "vendor": getattr(self.hook, "vendor", "EODHD"),
            "endpoint": getattr(self.hook, "endpoint", None),
            "params": getattr(self.hook, "params", None),
        }

        return records, meta

    # ------------------------------------
    # 3ï¸âƒ£ ê³µí†µ: ê¸°ë³¸ fetch/load (í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ êµ¬í˜„)
    # ------------------------------------
    def fetch(self, **kwargs):
        raise NotImplementedError("í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ fetch()ë¥¼ êµ¬í˜„í•´ì•¼ í•©ë‹ˆë‹¤.")

    def load(self, records: List[Dict], **kwargs):
        """ê¸°ë³¸ load ë¡œì§ â€” í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ override í•„ìš” ì—†ìŒ"""
        return self.load_to_datalake(records, **kwargs)
