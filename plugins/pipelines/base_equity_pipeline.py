import logging
import json
from abc import ABC
from pathlib import Path
from typing import List, Dict, Any, Callable, Tuple
from datetime import datetime
from plugins.config.constants import (
    DATA_LAKE_ROOT,
    LAYERS,
    VENDORS,
    WAREHOUSE_SOURCE_MAP,
)

REDACT_KEYS = {"api_token", "apikey", "api_key", "authorization", "auth", "token", "access_token", "secret"}

class HookMixin:
    hook: Any  # ê° Hook(e.g. EODHDHook, KRXHook)ëŠ” ìµœì†Œ vendor/endpoint/params ì†ì„±ì„ ì œê³µí•˜ë„ë¡

class BaseEquityPipeline(HookMixin, ABC):
    """
    Equity ê´€ë ¨ ê³µí†µ íŒŒì´í”„ë¼ì¸
    - ì›ë³¸(raw) ì ì¬ ê³µí†µ
    - vendor íŒŒí‹°ì…”ë‹ ì§€ì›
    - _source_meta.json ì €ì¥ + ë¹„ë°€ê°’ ë§ˆìŠ¤í‚¹
    """

    def __init__(self, data_domain: str, exchange_code: str, trd_dt: str):
        self.data_domain = data_domain  # e.g. ", "symbol_list", "fundamentals"
        self.exchange_code = exchange_code
        self.trd_dt = trd_dt
        self.log = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
        DATA_LAKE_ROOT.mkdir(parents=True, exist_ok=True)


    # =============================
    # Domain ì •ê·œí™” ìœ í‹¸
    # =============================
    def _normalize_domain(self, data_domain: str) -> str:
        """
        exchange_list, symbol_list ë“± lake ì „ìš© ë„ë©”ì¸ì„ warehouse í˜¸í™˜í˜•ìœ¼ë¡œ ì •ê·œí™”
        ì˜ˆ:
          exchange_list â†’ exchange
          symbol_list â†’ asset
        """
        for wh_domain, sources in WAREHOUSE_SOURCE_MAP.items():
            if data_domain in sources:
                return wh_domain
        return data_domain

    # =============================
    # ë¹„ë°€ê°’ ë§ˆìŠ¤í‚¹
    # =============================
    @staticmethod
    def _redact(obj: Any) -> Any:
        if isinstance(obj, dict):
            redacted = {}
            for k, v in obj.items():
                if k.lower() in REDACT_KEYS:
                    redacted[k] = "***"
                else:
                    redacted[k] = BaseEquityPipeline._redact(v)
            return redacted
        if isinstance(obj, list):
            return [BaseEquityPipeline._redact(v) for v in obj]
        return obj


    # ---------------------------
    # ìœ í‹¸: ë¹„ë°€ê°’ ë§ˆìŠ¤í‚¹
    # ---------------------------
    @staticmethod
    def _redact(obj: Any) -> Any:
        if isinstance(obj, dict):
            redacted = {}
            for k, v in obj.items():
                if k.lower() in REDACT_KEYS:
                    redacted[k] = "***"
                else:
                    redacted[k] = BaseEquityPipeline._redact(v)
            return redacted
        if isinstance(obj, list):
            return [BaseEquityPipeline._redact(v) for v in obj]
        return obj

    # ---------------------------
    # 1) ê²½ë¡œ & ë©”íƒ€ ìƒì„±
    # ---------------------------
    def _get_lake_path_and_metadata(
        self,
        layer: str = "raw",
        vendor: str = None,
        **kwargs
    ) -> Tuple[Path, dict]:
        """
        Hive-style íŒŒí‹°ì…”ë‹ êµ¬ì¡° ìƒì„±
        ì˜ˆ: /data_lake/validated/exchange/vendor=eodhd/exchange_code=US/trd_dt=2025-10-15/
        """
        vendor_value = vendor or getattr(self.hook, "vendor", None)
        if not vendor_value:
            raise ValueError("âŒ vendor ê°’ì´ ì—†ìŠµë‹ˆë‹¤. DAG op_kwargs ë˜ëŠ” Hook.vendor í™•ì¸ í•„ìš”.")

        date_partition_key = kwargs.get("partition_key_name", "trd_dt")
        date_value = kwargs.get(date_partition_key, self.trd_dt)
        geo_partition_key = kwargs.get("geo_partition_key", "exchange_code")
        geo_value = kwargs.get(geo_partition_key, self.exchange_code)

        target_dir = (
            DATA_LAKE_ROOT
            / layer
            / self.data_domain
            / f"vendor={vendor_value.lower()}"
            / f"{geo_partition_key}={geo_value}"
            / f"{date_partition_key}={date_value}"
        )
        target_dir.mkdir(parents=True, exist_ok=True)

        metadata = {
            "layer": layer,
            "vendor": vendor_value.lower(),
            geo_partition_key: geo_value,
            date_partition_key: date_value,
            "source": self.data_domain,
            "saved_at": datetime.utcnow().isoformat(timespec="seconds") + "Z",
        }
        return target_dir, metadata
    # ---------------------------
    # 2) íŒŒì¼ ì €ì¥ (JSONL)
    # ---------------------------
    def _write_records_to_lake(
        self,
        records: List[Dict],
        target_dir: Path,
        base_metadata: Dict[str, str],
        file_name: str | Callable[[List[Dict]], str],
        mode: str = "overwrite",
    ) -> Dict[str, Any]:
        if not records:
            self.log.info("ì €ì¥í•  ë ˆì½”ë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return {"count": 0, "target_path": str(target_dir), "file_path": None}

        file_path = target_dir / (file_name if isinstance(file_name, str) else file_name(records))
        open_mode = "a" if mode == "append" else "w"

        with open(file_path, open_mode, encoding="utf-8") as f:
            for rec in records:
                f.write(json.dumps(rec, ensure_ascii=False) + "\n")

        meta_path = target_dir / "_source_meta.json"
        meta_info = base_metadata.copy()
        meta_info["record_count"] = len(records)
        meta_info["saved_at"] = datetime.utcnow().isoformat() + "Z"
        with open(meta_path, "w", encoding="utf-8") as mf:
            json.dump(meta_info, mf, indent=2, ensure_ascii=False)

        self.log.info(f"âœ… JSONL ì €ì¥: {file_path} ({len(records):,}ê±´)")
        return {"count": len(records), "target_path": str(target_dir), "file_path": str(file_path)}

    # ---------------------------
    # 3) fetch â†’ í‘œì¤€í™”
    # ---------------------------
    def _standardize_fetch_output(self, data: Any) -> tuple[list[dict], dict]:
        """
        Fetch ê²°ê³¼ë¥¼ í•­ìƒ (records, meta)ë¡œ í‘œì¤€í™”
        metaëŠ” hook(vendor/endpoint/params) ê¸°ì¤€ìœ¼ë¡œ êµ¬ì„±í•˜ë©° ë¹„ë°€ê°’ ë§ˆìŠ¤í‚¹
        """

        print("data type:::::", type(data))
        print("data:::::", data)

        if data is None:
            records = []
        elif isinstance(data, list):
            records = data
        elif isinstance(data, dict):
            records = [data]
        else:
            self.log.warning(f"âš ï¸ Unexpected fetch type: {type(data)} â€” ë¦¬ìŠ¤íŠ¸ë¡œ ìºìŠ¤íŒ…")
            records = []

        # hook ë©”íƒ€
        hook_vendor = getattr(self.hook, "vendor", None)
        hook_endpoint = getattr(self.hook, "endpoint", None)
        hook_params = self._redact(getattr(self.hook, "params", None) or {})
        meta = {
            "vendor": (hook_vendor or "unknown").lower(),
            "endpoint": hook_endpoint,
            "params": hook_params,
        }
        return records, meta

    # ---------------------------
    # 4) ë©”íƒ€ íŒŒì¼ ì €ì¥
    # ---------------------------
    def _save_source_meta(self, target_dir: Path, record_count: int, source_meta: dict | None = None):
        """
        _source_meta.json ì €ì¥ (ë¹„ë°€ê°’ ë§ˆìŠ¤í‚¹ + record_count í¬í•¨)
        """
        meta_path = target_dir / "_source_meta.json"
        safe_meta = self._redact(source_meta or {})
        envelope = {
            "record_count": record_count,
            "saved_at": datetime.utcnow().isoformat(timespec="seconds") + "Z",
            **safe_meta,
        }
        with open(meta_path, "w", encoding="utf-8") as f:
            json.dump(envelope, f, indent=2, ensure_ascii=False)
        self.log.info(f"ğŸ“˜ Source metadata saved: {meta_path}")

    # ---------------------------
    # 5) í‘œì¤€ ì‹¤í–‰ (fetch â†’ save â†’ meta)
    # ---------------------------
    def fetch_and_load(self, **kwargs):
        self.log.info(f"ğŸ“¡ Fetching {self.data_domain} for exchange={self.exchange_code}")

        raw = self.fetch(**kwargs)
        records, source_meta = self._standardize_fetch_output(raw)
        vendor_override = kwargs.get("vendor")

        target_dir, base_meta = self._get_lake_path_and_metadata(layer=LAYERS["raw"], vendor=vendor_override)
        file_name = f"{self.data_domain}.jsonl"
        write_result = self._write_records_to_lake(records, target_dir, base_meta, file_name)
        record_count = write_result.get("count", len(records))

        self._save_source_meta(target_dir, record_count, source_meta)
        self.log.info(f"âœ… [FETCH COMPLETE] {self.data_domain} | {record_count:,}ê±´ ì €ì¥ ì™„ë£Œ â†’ {target_dir}")

        return {"record_count": record_count, "target_path": str(target_dir)}

    # ---------------------------
    # 6) ì™¸ë¶€ì—ì„œ ì§ì ‘ ì ì¬í•  ë•Œ ì‚¬ìš©
    # ---------------------------
    def load_to_datalake(self, records: List[Dict], **kwargs) -> Dict:
        """
        ì™¸ë¶€ì—ì„œ recordsë¥¼ ì§ì ‘ ë„˜ê²¨ ì ì¬í•  ë•Œ ì‚¬ìš©
        - vendor/partition í‚¤ ìë™ ì²˜ë¦¬
        - _source_meta.json ìë™ ìƒì„±
        """
        kwargs["partition_key_name"] = kwargs.get("partition_key_name", "trd_dt")
        kwargs["geo_partition_key"] = kwargs.get("geo_partition_key", "exchange_code")

        target_dir, base_meta = self._get_lake_path_and_metadata(**kwargs)
        file_name = f"{self.data_domain}.jsonl"

        save_info = self._write_records_to_lake(
            records=records,
            target_dir=target_dir,
            base_metadata=base_meta,
            file_name=file_name,
        )

        # ë³„ë„ source_meta envelopeê°€ ìˆë‹¤ë©´ ì—¬ê¸°ì„œë„ ì €ì¥ ê°€ëŠ¥
        # if hasattr(self, "source_meta") and self.source_meta:
        #     self._save_source_meta(Path(save_info["target_path"]), save_info.get("count", 0), self.source_meta)

        return save_info

    # ---------------------------
    # 7) í•˜ìœ„ í´ë˜ìŠ¤ í•„ìˆ˜ êµ¬í˜„
    # ---------------------------
    def fetch(self, **kwargs):
        raise NotImplementedError("í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ fetch()ë¥¼ êµ¬í˜„í•´ì•¼ í•©ë‹ˆë‹¤.")

    def load(self, records: List[Dict], **kwargs):
        return self.load_to_datalake(records, **kwargs)
