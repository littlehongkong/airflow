import logging
import json
from abc import ABC
from pathlib import Path
from typing import List, Dict, Any, Callable, Tuple
from datetime import datetime
from plugins.config.constants import DATA_LAKE_ROOT

REDACT_KEYS = {"api_token", "apikey", "api_key", "authorization", "auth", "token", "access_token", "secret"}

class HookMixin:
    hook: Any  # ê° Hook(e.g. EODHDHook, KRXHook)ëŠ” ìµœì†Œ vendor/endpoint/params ì†ì„±ì„ ì œê³µí•˜ë„ë¡

class BaseEquityPipeline(HookMixin, ABC):
    """
    Equity ê´€ë ¨ ê³µí†µ íŒŒì´í”„ë¼ì¸
    - ì›ë³¸(raw) ì ì¬ ê³µí†µ
    - vendor íŒŒí‹°ì…”ë‹ ì§€ì›
    - _source_meta.json ì €ì¥ + ë¹„ë°€ê°’ ë§ˆìŠ¤í‚¹
    """

    def __init__(self, data_domain: str, exchange_code: str, trd_dt: str):
        self.data_domain = data_domain  # e.g. ", "symbol_list", "fundamentals"
        self.exchange_code = exchange_code
        self.trd_dt = trd_dt
        self.log = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
        DATA_LAKE_ROOT.mkdir(parents=True, exist_ok=True)

    # ---------------------------
    # ìœ í‹¸: ë¹„ë°€ê°’ ë§ˆìŠ¤í‚¹
    # ---------------------------
    @staticmethod
    def _redact(obj: Any) -> Any:
        if isinstance(obj, dict):
            redacted = {}
            for k, v in obj.items():
                if k.lower() in REDACT_KEYS:
                    redacted[k] = "***"
                else:
                    redacted[k] = BaseEquityPipeline._redact(v)
            return redacted
        if isinstance(obj, list):
            return [BaseEquityPipeline._redact(v) for v in obj]
        return obj

    # ---------------------------
    # 1) ê²½ë¡œ & ë©”íƒ€ ìƒì„±
    # ---------------------------
    def _get_lake_path_and_metadata(
        self,
        layer: str = "raw",  # raw / validated / staging / curated / snapshot
        vendor: str = None,
        **kwargs
    ) -> Tuple[Path, dict]:
        """
        Hive íŒŒí‹°ì…”ë‹ + vendor êµ¬ë¶„
          ì˜ˆ: /opt/airflow/data/data_lake/raw/fundamentals/vendor=eodhd/exchange_code=US/trd_dt=2025-10-15/
        """

        self.log.info(f"[DEBUG] hook type: {type(self.hook)}, vendor: {getattr(self.hook, 'vendor', None)}")

        # ë‚ ì§œ íŒŒí‹°ì…˜
        date_partition_key = kwargs.get("partition_key_name", "trd_dt")
        date_value = kwargs.get(date_partition_key) or self.trd_dt
        if not date_value:
            raise ValueError(f"{date_partition_key} ê°’ì´ ì—†ìŠµë‹ˆë‹¤ (ì˜ˆ: trd_dt=YYYY-MM-DD)")

        # ì§€ë¦¬ íŒŒí‹°ì…˜
        geo_partition_key = kwargs.get("geo_partition_key", "exchange_code")
        geo_value = kwargs.get(geo_partition_key) or self.exchange_code
        if not geo_value:
            raise ValueError(f"{geo_partition_key} ê°’ì´ ì—†ìŠµë‹ˆë‹¤ (ì˜ˆ: exchange_code=US)")

        # ë²¤ë” íŒŒí‹°ì…˜
        vendor_value = vendor or getattr(self.hook, "vendor", None)
        if not vendor_value:
            raise ValueError(
                f"âŒ vendor ê°’ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. "
                f"pipeline_cls={self.__class__.__name__}, hook={type(self.hook).__name__}. "
                f"DAGì˜ op_kwargs ë˜ëŠ” Hook.vendor ì†ì„±ì„ í™•ì¸í•˜ì„¸ìš”."
            )
        vendor_value = vendor_value.lower()

        # ê²½ë¡œ ìƒì„±
        target_dir = (
            DATA_LAKE_ROOT
            / layer
            / self.data_domain
            / f"vendor={vendor_value}"
            / f"{geo_partition_key}={geo_value}"
            / f"{date_partition_key}={date_value}"
        )
        target_dir.mkdir(parents=True, exist_ok=True)

        # ë©”íƒ€ë°ì´í„°
        metadata = {
            "layer": layer,
            "vendor": vendor_value,
            geo_partition_key: geo_value,
            date_partition_key: date_value,
            "source": self.data_domain,
            "saved_at": datetime.utcnow().isoformat(timespec="seconds") + "Z",
        }
        return target_dir, metadata

    # ---------------------------
    # 2) íŒŒì¼ ì €ì¥ (JSONL)
    # ---------------------------
    def _write_records_to_lake(
        self,
        records: List[Dict],
        target_dir: Path,
        base_metadata: Dict[str, str],
        file_name: str | Callable[[List[Dict]], str],
        mode: str = "overwrite",
    ) -> Dict[str, Any]:
        """
        JSONLë¡œ ì €ì¥. ê° ë ˆì½”ë“œì— metadata í•„ë“œ ë³‘í•©(ì–•ì€ ë³µì‚¬)
        """
        if not records:
            self.log.info("ì €ì¥í•  ë ˆì½”ë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return {"count": 0, "target_path": str(target_dir), "file_path": None}

        file_path = target_dir / (file_name if isinstance(file_name, str) else file_name(records))
        open_mode = "a" if mode == "append" else "w"

        saved_count = 0
        try:
            with open(file_path, open_mode, encoding="utf-8") as f:
                for rec in records:
                    f.write(json.dumps(rec, ensure_ascii=False) + "\n")
                    saved_count += 1

            # âœ… ë³„ë„ì˜ ë©”íƒ€íŒŒì¼ ìƒì„±
            meta_path = target_dir / "_source_meta.json"
            with open(meta_path, "w", encoding="utf-8") as mf:
                meta_info = base_metadata.copy()
                meta_info["record_count"] = len(records)
                meta_info["saved_at"] = datetime.utcnow().isoformat() + "Z"
                json.dump(meta_info, mf, indent=2, ensure_ascii=False)
            self.log.info(f"âœ… JSONL ì €ì¥: {file_path} ({saved_count:,}ê±´, mode={mode})")
        except Exception as e:
            self.log.exception(f"âŒ JSONL ì €ì¥ ì‹¤íŒ¨: {file_path} - {e}")
            raise

        return {"count": saved_count, "target_path": str(target_dir), "file_path": str(file_path)}

    # ---------------------------
    # 3) fetch â†’ í‘œì¤€í™”
    # ---------------------------
    def _standardize_fetch_output(self, data: Any) -> tuple[list[dict], dict]:
        """
        Fetch ê²°ê³¼ë¥¼ í•­ìƒ (records, meta)ë¡œ í‘œì¤€í™”
        metaëŠ” hook(vendor/endpoint/params) ê¸°ì¤€ìœ¼ë¡œ êµ¬ì„±í•˜ë©° ë¹„ë°€ê°’ ë§ˆìŠ¤í‚¹
        """
        if data is None:
            records = []
        elif isinstance(data, list):
            records = data
        elif isinstance(data, dict):
            records = [data]
        else:
            self.log.warning(f"âš ï¸ Unexpected fetch type: {type(data)} â€” ë¦¬ìŠ¤íŠ¸ë¡œ ìºìŠ¤íŒ…")
            records = []

        # hook ë©”íƒ€
        hook_vendor = getattr(self.hook, "vendor", None)
        hook_endpoint = getattr(self.hook, "endpoint", None)
        hook_params = self._redact(getattr(self.hook, "params", None) or {})
        meta = {
            "vendor": (hook_vendor or "unknown").lower(),
            "endpoint": hook_endpoint,
            "params": hook_params,
        }
        return records, meta

    # ---------------------------
    # 4) ë©”íƒ€ íŒŒì¼ ì €ì¥
    # ---------------------------
    def _save_source_meta(self, target_dir: Path, record_count: int, source_meta: dict | None = None):
        """
        _source_meta.json ì €ì¥ (ë¹„ë°€ê°’ ë§ˆìŠ¤í‚¹ + record_count í¬í•¨)
        """
        meta_path = target_dir / "_source_meta.json"
        safe_meta = self._redact(source_meta or {})
        envelope = {
            "record_count": record_count,
            "saved_at": datetime.utcnow().isoformat(timespec="seconds") + "Z",
            **safe_meta,
        }
        with open(meta_path, "w", encoding="utf-8") as f:
            json.dump(envelope, f, indent=2, ensure_ascii=False)
        self.log.info(f"ğŸ“˜ Source metadata saved: {meta_path}")

    # ---------------------------
    # 5) í‘œì¤€ ì‹¤í–‰ (fetch â†’ save â†’ meta)
    # ---------------------------
    def fetch_and_load(self, **kwargs):
        """
        ê³µí†µ: ë°ì´í„° ìˆ˜ì§‘ â†’ ì •ê·œí™” â†’ ì €ì¥ â†’ ì›ì²œ ë©”íƒ€ ê¸°ë¡
        - fetch()ê°€ (records, meta) íŠœí”Œì„ ë°˜í™˜í•´ë„ OK
        - fetch()ê°€ ì›ì‹œ ì‘ë‹µ(list|dict|None)ì„ ë°˜í™˜í•´ë„ OK
        """
        self.log.info(f"ğŸ“¡ Fetching {self.data_domain} for exchange={self.exchange_code}, trd_dt={self.trd_dt}")

        raw = self.fetch(**kwargs)

        if isinstance(raw, tuple) and len(raw) == 2:
            records, source_meta = raw
            if records is None:
                records = []
            elif isinstance(records, dict):
                records = [records]
        else:
            records, source_meta = self._standardize_fetch_output(raw)

        # ê²½ë¡œ/ë©”íƒ€
        vendor_override = kwargs.get("vendor")  # í•„ìš” ì‹œ DAGì—ì„œ ê°•ì œ ì§€ì • ê°€ëŠ¥
        target_dir, base_meta = self._get_lake_path_and_metadata(layer="raw", vendor=vendor_override, **kwargs)

        # ì €ì¥
        file_name = f"{self.data_domain}.jsonl"
        write_result = self._write_records_to_lake(
            records=records,
            target_dir=target_dir,
            base_metadata=base_meta,
            file_name=file_name,
        )
        record_count = write_result.get("count", len(records))

        # ì›ì²œ ë©”íƒ€ ì €ì¥
        self._save_source_meta(target_dir, record_count, source_meta)

        result = {
            "record_count": record_count,
            "target_path": write_result.get("target_path"),
            "file_path": write_result.get("file_path"),
            "vendor": base_meta.get("vendor"),
        }
        self.log.info(
            f"âœ… [FETCH COMPLETE] {self.data_domain} | "
            f"exchange={self.exchange_code} | vendor={vendor_override} | "
            f"{record_count:,}ê±´ ì €ì¥ ì™„ë£Œ â†’ {target_dir}"
        )
        return result

    # ---------------------------
    # 6) ì™¸ë¶€ì—ì„œ ì§ì ‘ ì ì¬í•  ë•Œ ì‚¬ìš©
    # ---------------------------
    def load_to_datalake(self, records: List[Dict], **kwargs) -> Dict:
        """
        ì™¸ë¶€ì—ì„œ recordsë¥¼ ì§ì ‘ ë„˜ê²¨ ì ì¬í•  ë•Œ ì‚¬ìš©
        - vendor/partition í‚¤ ìë™ ì²˜ë¦¬
        - _source_meta.json ìë™ ìƒì„±
        """
        kwargs["partition_key_name"] = kwargs.get("partition_key_name", "trd_dt")
        kwargs["geo_partition_key"] = kwargs.get("geo_partition_key", "exchange_code")

        target_dir, base_meta = self._get_lake_path_and_metadata(**kwargs)
        file_name = f"{self.data_domain}.jsonl"

        save_info = self._write_records_to_lake(
            records=records,
            target_dir=target_dir,
            base_metadata=base_meta,
            file_name=file_name,
        )

        # ë³„ë„ source_meta envelopeê°€ ìˆë‹¤ë©´ ì—¬ê¸°ì„œë„ ì €ì¥ ê°€ëŠ¥
        # if hasattr(self, "source_meta") and self.source_meta:
        #     self._save_source_meta(Path(save_info["target_path"]), save_info.get("count", 0), self.source_meta)

        return save_info

    # ---------------------------
    # 7) í•˜ìœ„ í´ë˜ìŠ¤ í•„ìˆ˜ êµ¬í˜„
    # ---------------------------
    def fetch(self, **kwargs):
        raise NotImplementedError("í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ fetch()ë¥¼ êµ¬í˜„í•´ì•¼ í•©ë‹ˆë‹¤.")

    def load(self, records: List[Dict], **kwargs):
        return self.load_to_datalake(records, **kwargs)
