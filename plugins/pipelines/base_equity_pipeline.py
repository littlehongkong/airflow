"""
plugins/pipelines/equity/base_equity_pipeline.py

ğŸ’¾ BaseEquityPipeline
- ì£¼ì‹/ETF/í€ë”ë©˜í„¸/ë°°ë‹¹ ë“± ê³µí†µ ìˆ˜ì§‘ íŒŒì´í”„ë¼ì¸ì˜ ìƒìœ„ í´ë˜ìŠ¤
- Data Lake (raw/validated) ì ì¬ ê³µí†µ ë¡œì§
- vendor íŒŒí‹°ì…”ë‹ ë° _source_meta.json ìë™ ê´€ë¦¬
"""

import logging
import json
from abc import ABC, abstractmethod
from pathlib import Path
from typing import List, Dict, Any, Callable, Tuple, Union
from datetime import datetime

from plugins.config import constants as C

# ë¯¼ê°ì •ë³´ í‚¤ ëª©ë¡
REDACT_KEYS = {"api_token", "apikey", "api_key", "authorization", "auth", "token", "access_token", "secret"}


class HookMixin:
    """Hook ê°ì²´ í˜¼í•©ìš© í´ë˜ìŠ¤"""
    hook: Any  # ì˜ˆ: EODHDHook, KRXHook ë“±


class BaseEquityPipeline(HookMixin, ABC):
    """
    âœ… Equity ê³„ì—´ ê³µí†µ íŒŒì´í”„ë¼ì¸ ë² ì´ìŠ¤ í´ë˜ìŠ¤

    [ê³µí†µ ê¸°ëŠ¥]
    1ï¸âƒ£ fetch â†’ JSONL íŒŒì¼ ì €ì¥ (Data Lake êµ¬ì¡°)
    2ï¸âƒ£ vendor/exchange_code/trd_dt íŒŒí‹°ì…”ë‹
    3ï¸âƒ£ _source_meta.json ìë™ ìƒì„± ë° ë¹„ë°€ê°’ ë§ˆìŠ¤í‚¹
    """

    def __init__(self, domain: str, exchange_code: str, trd_dt: str, domain_group: str | None = None, allow_empty: bool = False):
        self.domain = domain                  # ì˜ˆ: "symbol_list", "fundamentals"
        self.exchange_code = exchange_code    # ì˜ˆ: "US"
        self.trd_dt = trd_dt                  # ì˜ˆ: "2025-11-03"
        self.layer = "data_lake"
        self.log = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
        self.allow_empty = allow_empty
        C.DATA_LAKE_ROOT.mkdir(parents=True, exist_ok=True)

        # âœ… 1ï¸âƒ£ domain_group ìë™ ì¸ì‹ (ì—†ìœ¼ë©´ constants ê¸°ë°˜ ì¶”ë¡ )
        if domain_group:
            self.domain_group = domain_group.lower()
        else:
            self.domain_group = C.DOMAIN_GROUPS.get(domain.lower(), "equity")

    # ============================================================
    # 1ï¸âƒ£ Domain ì •ê·œí™”
    # ============================================================
    def _normalize_domain(self, domain: str) -> str:
        """
        exchange_list, symbol_list ë“± lake ì „ìš© ë„ë©”ì¸ì„ warehouse í˜¸í™˜í˜•ìœ¼ë¡œ ì •ê·œí™”
        ì˜ˆ:
          exchange_list â†’ exchange
          symbol_list â†’ asset_master
        """
        for wh_domain, sources in C.WAREHOUSE_SOURCE_MAP.items():
            if domain in sources:
                return wh_domain
        return domain

    # ============================================================
    # 2ï¸âƒ£ ë¹„ë°€ê°’ ë§ˆìŠ¤í‚¹
    # ============================================================
    @staticmethod
    def _redact(obj: Any) -> Any:
        """dict/list ë‚´ API í‚¤ ë“±ì˜ ë¯¼ê°ê°’ ë§ˆìŠ¤í‚¹"""
        if isinstance(obj, dict):
            return {
                k: ("***" if k.lower() in REDACT_KEYS else BaseEquityPipeline._redact(v))
                for k, v in obj.items()
            }
        elif isinstance(obj, list):
            return [BaseEquityPipeline._redact(v) for v in obj]
        else:
            return obj

    # ============================================================
    # 3ï¸âƒ£ ê²½ë¡œ ë° ë©”íƒ€ë°ì´í„° ìƒì„±
    # ============================================================
    def _get_lake_path_and_metadata(
        self,
        stage: str = "raw",
        vendor: str = None,
        **kwargs,
    ) -> Tuple[Path, dict]:
        """
        Hive-style íŒŒí‹°ì…”ë‹ êµ¬ì¡° ìƒì„±
        ì˜ˆ:
          /data_lake/raw/symbol_list/vendor=eodhd/exchange_code=US/trd_dt=2025-11-05/
        """
        vendor_value = vendor or getattr(self.hook, "vendor", None)
        if not vendor_value:
            raise ValueError("âŒ vendor ê°’ì´ ì—†ìŠµë‹ˆë‹¤. DAG op_kwargs ë˜ëŠ” Hook.vendor í™•ì¸ í•„ìš”.")

        date_partition_key = kwargs.get("partition_key_name", "trd_dt")
        geo_partition_key = kwargs.get("geo_partition_key", "exchange_code")

        date_value = kwargs.get(date_partition_key, self.trd_dt)
        geo_value = kwargs.get(geo_partition_key, self.exchange_code)

        target_dir = (
            C.DATA_LAKE_ROOT
            / stage
            / self.domain_group
            / self.domain
            / f"vendor={vendor_value.lower()}"
            / f"{geo_partition_key}={geo_value}"
            / f"{date_partition_key}={date_value}"
        )
        target_dir.mkdir(parents=True, exist_ok=True)

        metadata = {
            "stage": stage,
            "vendor": vendor_value.lower(),
            geo_partition_key: geo_value,
            date_partition_key: date_value,
            "source": self.domain,
            "saved_at": datetime.utcnow().isoformat(timespec="seconds") + "Z",
        }
        return target_dir, metadata

    # ============================================================
    # 4ï¸âƒ£ íŒŒì¼ ì €ì¥ (JSONL)
    # ============================================================
    def _write_records_to_lake(
            self,
            records: List[Dict],
            target_dir: Path,
            base_metadata: Dict[str, Any],
            file_name: Union[str, Callable[[List[Dict]], str]],
            mode: str = "overwrite",
    ) -> Dict[str, Any]:
        target_dir.mkdir(parents=True, exist_ok=True)
        file_path = target_dir / (file_name if isinstance(file_name, str) else file_name(records))
        meta_path = target_dir / "_source_meta.json"

        # ============================================================
        # 1ï¸âƒ£ Empty Handling
        # ============================================================
        if not records:
            if getattr(self, "allow_empty", False):
                self.log.warning("âš ï¸ ì €ì¥í•  ë ˆì½”ë“œê°€ ì—†ìŠµë‹ˆë‹¤. (allow_empty=True)")
                # âœ… ë¹ˆ JSONL íŒŒì¼ ìƒì„±
                with open(file_path, "w", encoding="utf-8") as f:
                    pass  # ë¹ˆ íŒŒì¼
                record_count = 0
            else:
                self.log.error("âŒ ë¹ˆ ë°ì´í„°ê°€ í—ˆìš©ë˜ì§€ ì•ŠëŠ” ë„ë©”ì¸ì…ë‹ˆë‹¤.")
                raise ValueError("ë¹ˆ ë°ì´í„°ê°€ í—ˆìš©ë˜ì§€ ì•ŠëŠ” ë„ë©”ì¸ì…ë‹ˆë‹¤.")
        else:
            open_mode = "a" if mode == "append" else "w"
            with open(file_path, open_mode, encoding="utf-8") as f:
                for rec in records:
                    f.write(json.dumps(rec, ensure_ascii=False) + "\n")
            record_count = len(records)

        # ============================================================
        # 2ï¸âƒ£ ë©”íƒ€ì •ë³´ ì‘ì„± (_source_meta.json)
        # ============================================================
        meta_info = base_metadata.copy()
        meta_info["record_count"] = record_count
        meta_info["saved_at"] = datetime.utcnow().isoformat(timespec="seconds") + "Z"

        with open(meta_path, "w", encoding="utf-8") as mf:
            json.dump(meta_info, mf, indent=2, ensure_ascii=False)

        # ============================================================
        # 3ï¸âƒ£ ë¡œê·¸ ë° ë°˜í™˜
        # ============================================================
        self.log.info(f"âœ… JSONL ì €ì¥ ì™„ë£Œ: {file_path.name} ({record_count:,}ê±´)")
        return {
            "count": record_count,
            "target_path": str(target_dir),
            "file_path": str(file_path),
        }

    # ============================================================
    # 5ï¸âƒ£ Fetch ê²°ê³¼ í‘œì¤€í™”
    # ============================================================
    def _standardize_fetch_output(self, data: Any) -> Tuple[List[Dict], Dict[str, Any]]:
        """
        Fetch ê²°ê³¼ë¥¼ í•­ìƒ (records, meta) í˜•íƒœë¡œ ë³€í™˜
        """
        if data is None:
            records = []
        elif isinstance(data, list):
            records = [r for r in data if isinstance(r, dict)]
        elif isinstance(data, dict):
            records = [data]
        else:
            self.log.warning(f"âš ï¸ Unexpected fetch type: {type(data)} â€” ê°•ì œ ë¦¬ìŠ¤íŠ¸ ë³€í™˜")
            records = [{"value": str(data)}]

        hook_vendor = getattr(self.hook, "vendor", None)
        hook_endpoint = getattr(self.hook, "endpoint", None)
        hook_params = self._redact(getattr(self.hook, "params", None) or {})

        meta = {
            "vendor": (hook_vendor or "unknown").lower(),
            "endpoint": hook_endpoint,
            "params": hook_params,
        }
        return records, meta

    # ============================================================
    # 6ï¸âƒ£ _source_meta.json ì €ì¥
    # ============================================================
    def _save_source_meta(self, target_dir: Path, record_count: int, source_meta: dict | None = None):
        meta_path = target_dir / "_source_meta.json"
        safe_meta = self._redact(source_meta or {})
        envelope = {
            "record_count": record_count,
            "saved_at": datetime.utcnow().isoformat(timespec="seconds") + "Z",
            **safe_meta,
        }
        with open(meta_path, "w", encoding="utf-8") as f:
            json.dump(envelope, f, indent=2, ensure_ascii=False)
        self.log.info(f"ğŸ“˜ Source metadata saved: {meta_path}")

    # ============================================================
    # 7ï¸âƒ£ í‘œì¤€ ì‹¤í–‰(fetch â†’ save â†’ meta)
    # ============================================================
    def fetch_and_load(self, **kwargs) -> Dict[str, Any]:
        self.log.info(f"ğŸ“¡ Fetching {self.domain} for exchange={self.exchange_code}")

        raw = self.fetch(**kwargs)
        records, source_meta = self._standardize_fetch_output(raw)
        vendor_override = kwargs.get("vendor")

        target_dir, base_meta = self._get_lake_path_and_metadata(
            stage=C.Stages.get("raw", "raw"),  # ì•ˆì „ ì ‘ê·¼
            vendor=vendor_override,
        )

        file_name = f"{self.domain}.jsonl"
        write_result = self._write_records_to_lake(records, target_dir, base_meta, file_name)
        record_count = write_result.get("count", len(records))

        self._save_source_meta(target_dir, record_count, source_meta)
        self.log.info(f"âœ… [FETCH COMPLETE] {self.domain} | {record_count:,}ê±´ ì €ì¥ ì™„ë£Œ â†’ {target_dir}")

        return {"record_count": record_count, "target_path": str(target_dir)}

    # ============================================================
    # 8ï¸âƒ£ ì™¸ë¶€ì—ì„œ ì§ì ‘ ì ì¬í•  ë•Œ ì‚¬ìš©
    # ============================================================
    def load_to_datalake(self, records: List[Dict], **kwargs) -> Dict[str, Any]:
        kwargs["partition_key_name"] = kwargs.get("partition_key_name", "trd_dt")
        kwargs["geo_partition_key"] = kwargs.get("geo_partition_key", "exchange_code")

        target_dir, base_meta = self._get_lake_path_and_metadata(**kwargs)
        file_name = f"{self.domain}.jsonl"

        save_info = self._write_records_to_lake(
            records=records,
            target_dir=target_dir,
            base_metadata=base_meta,
            file_name=file_name,
        )
        return save_info

    # ============================================================
    # 9ï¸âƒ£ ì¶”ìƒ ë©”ì„œë“œ ì •ì˜
    # ============================================================
    @abstractmethod
    def fetch(self, **kwargs) -> Any:
        """í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ fetch() êµ¬í˜„ í•„ìˆ˜"""
        pass

    def load(self, records: List[Dict], **kwargs):
        return self.load_to_datalake(records, **kwargs)
