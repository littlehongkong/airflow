# plugins/pipelines/warehouse/asset_master_pipeline.py
import json
import duckdb
import pandas as pd
from pathlib import Path
from datetime import datetime, timezone
from typing import List, Dict, Optional

from plugins.config.constants import (
    DATA_DOMAINS, LAYERS, VENDORS,
    MASTER_DOMAIN, ENTITY_PREFIX,
    DATA_WAREHOUSE_ROOT
)
from plugins.utils.partition_finder import latest_trd_dt_path, parquet_file_in
from plugins.utils.id_generator import generate_entity_id
from plugins.utils.snapshot_registry import get_latest_snapshot_meta, update_global_snapshot_registry


class AssetMasterWarehousePipeline:
    """
    âœ… êµ­ê°€ ë‹¨ìœ„ ìì‚° ë§ˆìŠ¤í„° ìŠ¤ëƒ…ìƒ· ìƒì„± íŒŒì´í”„ë¼ì¸
      - ì…ë ¥: Data Lake 'validated' (symbol_list, fundamentals), Warehouse 'exchange_master'
      - ì¶œë ¥: data_warehouse/asset_master/country_code=XX/snapshot_dt=YYYY-MM-DD/asset_master.parquet

    ğŸ“Œ ê±°ë˜ì†Œ ë§¤í•‘ì€ exchange_masterì˜ ìµœì‹  ìŠ¤ëƒ…ìƒ· ë©”íƒ€(ë˜ëŠ” parquet)ì—ì„œ ê°€ì ¸ì˜¨ë‹¤.
       (ë” ì´ìƒ exchange_mapper.py ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
    """

    def __init__(self, snapshot_dt: str, country_code: str):
        self.snapshot_dt = snapshot_dt  # 'YYYY-MM-DD'
        self.country_code = country_code.upper()

        # âœ… ìµœì‹  exchange_masterì—ì„œ êµ­ê°€-ê±°ë˜ì†Œ ë§¤í•‘ ë¡œë“œ & exchanges ì„¤ì •
        country_exchange_map = self._load_latest_exchange_master()
        if self.country_code not in country_exchange_map:
            raise ValueError(f"âŒ ìµœì‹  exchange_masterì—ì„œ {self.country_code} êµ­ê°€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        self.exchanges: List[str] = country_exchange_map[self.country_code]

        # âœ… ë²¤ë” ìš°ì„ ìˆœìœ„ (í•„ìš” ì‹œ í–¥í›„ êµ­ê°€ë³„ ì„¸ë¶„í™” ê°€ëŠ¥)
        self.vendor_priority: List[str] = [VENDORS["EODHD"]]

        # âœ… ì¶œë ¥ ê²½ë¡œ
        self.output_dir = (
            DATA_WAREHOUSE_ROOT / MASTER_DOMAIN / f"country_code={self.country_code}" / f"snapshot_dt={self.snapshot_dt}"
        )
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.output_file = self.output_dir / f"{MASTER_DOMAIN}.parquet"
        self.meta_file = self.output_dir / "_build_meta.json"

    # -------------------------
    # ë‚´ë¶€ í—¬í¼
    # -------------------------
    def _choose_best_partition(self, domain: str, exchange_code: str) -> Optional[Path]:
        """ë²¤ë” ìš°ì„ ìˆœìœ„ì— ë”°ë¼ ìµœì‹  validated íŒŒí‹°ì…˜ ì„ íƒ"""
        for vendor in self.vendor_priority:
            pdir = latest_trd_dt_path(domain, exchange_code, vendor, layer=LAYERS["VALIDATED"])
            if pdir:
                return pdir
        return None

    def _load_latest_exchange_master(self) -> Dict[str, List[str]]:
        """
        âœ… ì „ì—­ ë ˆì§€ìŠ¤íŠ¸ë¦¬ì—ì„œ ìµœì‹  exchange_master ë©”íƒ€íŒŒì¼ì„ ì°¾ì•„
           êµ­ê°€ â†’ [ê±°ë˜ì†Œì½”ë“œ] ë§¤í•‘ì„ ë°˜í™˜í•œë‹¤.
        1) metaì˜ country_exchange_map ì‚¬ìš©
        2) ì—†ìœ¼ë©´ exchange_master.parquetì„ ì½ì–´ groupbyë¡œ ìƒì„± (fallback)
        """
        meta_info = get_latest_snapshot_meta("exchange_master")
        if not meta_info:
            raise FileNotFoundError("âŒ latest_snapshot_meta.jsonì—ì„œ exchange_master í•­ëª©ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

        meta_path = Path(meta_info.get("meta_file", ""))
        if not meta_path.exists():
            raise FileNotFoundError(f"âŒ exchange_master ë©”íƒ€íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {meta_path}")

        with open(meta_path, "r", encoding="utf-8") as f:
            meta = json.load(f)

        # 1) ë©”íƒ€ì— ì´ë¯¸ ë§¤í•‘ì´ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        mapping = meta.get("country_exchange_map")
        if isinstance(mapping, dict) and mapping:
            # ê°’ì´ listê°€ ì•„ë‹ ê²½ìš° ë³´ì •
            cleaned = {k.upper(): sorted({str(x).upper() for x in v}) for k, v in mapping.items() if v}
            return cleaned

        # 2) fallback: parquetì—ì„œ ì¬êµ¬ì„±
        pq_path = meta_path.parent / "exchange_master.parquet"
        if not pq_path.exists():
            raise FileNotFoundError(
                f"âŒ exchange_master parquetì´ ì—†ì–´ ë§¤í•‘ì„ êµ¬ì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pq_path}"
            )

        conn = duckdb.connect(database=":memory:")
        df = conn.execute(f"SELECT country_code, exchange_code FROM read_parquet('{pq_path.as_posix()}')").df()
        conn.close()

        if df.empty:
            raise ValueError("âŒ exchange_master parquetì´ ë¹„ì–´ ìˆì–´ ë§¤í•‘ì„ êµ¬ì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        df.columns = df.columns.str.lower()
        if not {"country_code", "exchange_code"}.issubset(df.columns):
            raise ValueError("âŒ exchange_master parquetì— í•„ìš”í•œ ì»¬ëŸ¼(country_code, exchange_code)ì´ ì—†ìŠµë‹ˆë‹¤.")

        df["country_code"] = df["country_code"].astype(str).str.strip().str.upper()
        df["exchange_code"] = df["exchange_code"].astype(str).str.strip().str.upper()

        mapping = (
            df.groupby("country_code")["exchange_code"]
              .apply(lambda s: sorted(set(s.dropna().astype(str))))
              .to_dict()
        )
        return mapping

    @staticmethod
    def _normalize_symbol_df(df: pd.DataFrame) -> pd.DataFrame:
        df = df.copy()
        df.columns = df.columns.str.lower()

        ticker_col = "ticker" if "ticker" in df.columns else ("code" if "code" in df.columns else ("symbol" if "symbol" in df.columns else None))
        if not ticker_col:
            raise ValueError("ì‹¬ë³¼ ë°ì´í„°ì— ticker/code/symbol ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")

        name_col = next((c for c in ("name", "companyname", "securityname", "issuer_name", "ê¸°ì—…ëª…") if c in df.columns), None)
        ex_col = next((c for c in ("exchange", "exchange_code", "market", "ê±°ë˜ì†Œ", "ì‹œì¥êµ¬ë¶„") if c in df.columns), None)

        out = pd.DataFrame({
            "ticker": df[ticker_col].astype(str).str.strip(),
            "name": df[name_col].astype(str).str.strip() if name_col else None,
            "exchange_code": df[ex_col].astype(str).str.strip() if ex_col else None,
        })
        return out.drop_duplicates(subset=["ticker"], keep="first").reset_index(drop=True)

    @staticmethod
    def _normalize_fund_df(df: pd.DataFrame) -> pd.DataFrame:
        df = df.copy()
        df.columns = df.columns.str.lower()
        ticker_col = "code" if "code" in df.columns else ("ticker" if "ticker" in df.columns else None)
        if not ticker_col:
            raise ValueError("í€ë”ë©˜í„¸ ë°ì´í„°ì— code/ticker ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")

        pick = {
            "ticker": df[ticker_col].astype(str).str.strip(),
            "type": df.get("type"),
            "sector": df.get("sector"),
            "industry": df.get("industry"),
            "currency_code": df.get("currencycode", df.get("currency")),
            "currency_name": df.get("currencyname"),
            "currency_symbol": df.get("currencysymbol"),
            "country_name": df.get("countryname"),
            "country_iso": df.get("countryiso"),
            "isin": df.get("isin"),
            "cik": df.get("cik"),
            "cusip": df.get("cusip"),
            "lei": df.get("lei"),
            "openfigi": df.get("openfigi"),
            "ipo_date": df.get("ipodate"),
            "gic_sector": df.get("gicsector"),
            "gic_group": df.get("gicgroup"),
            "gic_industry": df.get("gicindustry"),
            "gic_subindustry": df.get("gicsubindustry"),
            "is_delisted": df.get("isdelisted"),
            "primary_ticker": df.get("primaryticker"),
            "home_category": df.get("homecategory"),
            "fiscal_year_end": df.get("fiscalyearend"),
        }
        out = pd.DataFrame(pick)
        return out.drop_duplicates(subset=["ticker"], keep="first").reset_index(drop=True)

    @staticmethod
    def _normalize_exchange_df(df: pd.DataFrame) -> pd.DataFrame:
        df = df.copy()
        df.columns = df.columns.str.lower()
        code_col = next((c for c in ("code", "exchange_code", "mic", "operatingmic") if c in df.columns), None)
        if not code_col:
            raise ValueError("ê±°ë˜ì†Œ ë°ì´í„°ì— code/exchange_code ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")

        out = pd.DataFrame({
            "exchange_code": df[code_col].astype(str).str.strip(),
            "exchange_name": df.get("name"),
            "country": df.get("country"),
            "currency": df.get("currency"),
            "country_iso2": df.get("countryiso2"),
            "country_iso3": df.get("countryiso3"),
        })
        return out.drop_duplicates(subset=["exchange_code"], keep="first").reset_index(drop=True)

    # -------------------------
    # ë©”ì¸ ë¡œì§
    # -------------------------
    def build(self, **kwargs) -> Dict:
        """
        1) (êµ­ê°€ ë‚´ ì—¬ëŸ¬ ê±°ë˜ì†Œì— ëŒ€í•´) vendor ìš°ì„ ìˆœìœ„ë¡œ ìµœì‹  symbol/fund íŒŒí‹°ì…˜ ì„ íƒ
        2) ì½ê¸° â†’ ì •ê·œí™” â†’ ë³‘í•©
        3) entity_id ë°œê¸‰
        4) ì €ì¥ + ë©”íƒ€ + ì „ì—­ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ê°±ì‹ 
        """
        sym_dfs, fund_dfs, ex_dfs = [], [], []
        conn = duckdb.connect(database=":memory:")

        # --- ê±°ë˜ì†Œë³„ íŒŒí‹°ì…˜ ì„ íƒ/ë¡œë”©
        for ex in self.exchanges:
            # symbol_list
            sp = self._choose_best_partition(DATA_DOMAINS["SYMBOL"], ex)
            if sp:
                sym_pq = parquet_file_in(sp, DATA_DOMAINS["SYMBOL"])
                if sym_pq:
                    df = conn.execute(f"SELECT * FROM read_parquet('{sym_pq.as_posix()}')").df()
                    df["__exchange_chosen__"] = ex
                    sym_dfs.append(df)

            # fundamentals
            fp = self._choose_best_partition(DATA_DOMAINS["FUNDAMENTALS"], ex)
            if fp:
                fund_pq = parquet_file_in(fp, DATA_DOMAINS["FUNDAMENTALS"])
                if fund_pq:
                    df = conn.execute(f"SELECT * FROM read_parquet('{fund_pq.as_posix()}')").df()
                    df["__exchange_chosen__"] = ex
                    fund_dfs.append(df)

            # exchange_list (ì˜µì…˜)
            ep = self._choose_best_partition(DATA_DOMAINS["EXCHANGE_LIST"], ex)
            if ep:
                ex_pq = parquet_file_in(ep, DATA_DOMAINS["EXCHANGE_LIST"])
                if ex_pq:
                    df = conn.execute(f"SELECT * FROM read_parquet('{ex_pq.as_posix()}')").df()
                    df["__exchange_chosen__"] = ex
                    ex_dfs.append(df)

        conn.close()

        if not sym_dfs:
            raise RuntimeError(f"[{self.country_code}] ì‹¬ë³¼ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë²¤ë”/ê²€ì¦ ì—¬ë¶€ í™•ì¸ í•„ìš”.")
        symbols_raw = pd.concat(sym_dfs, ignore_index=True)
        symbols = self._normalize_symbol_df(symbols_raw)

        funds = None
        if fund_dfs:
            funds_raw = pd.concat(fund_dfs, ignore_index=True)
            funds = self._normalize_fund_df(funds_raw)

        exchanges_df = None
        if ex_dfs:
            exchanges_raw = pd.concat(ex_dfs, ignore_index=True)
            exchanges_df = self._normalize_exchange_df(exchanges_raw)

        # --- ë³‘í•© (ì‹¬ë³¼ ê¸°ì¤€ LEFT)
        master = symbols
        if funds is not None:
            master = master.merge(funds, on="ticker", how="left", suffixes=("", "_fund"))

        if exchanges_df is not None:
            master = master.merge(
                exchanges_df[["exchange_code", "exchange_name", "country", "currency", "country_iso2", "country_iso3"]],
                on="exchange_code",
                how="left"
            )

        # --- êµ­ê°€ ì½”ë“œ & entity_id
        master["country_code"] = self.country_code
        master["entity_id"] = master.apply(
            lambda r: generate_entity_id(
                ENTITY_PREFIX["EQUITY"],
                country=self.country_code,
                exchange=r.get("exchange_code") or "",
                ticker=r.get("ticker") or "",
            ),
            axis=1
        )

        # --- ì¹¼ëŸ¼ ìˆœì„œ ì •ë¦¬
        preferred_cols = [
            "entity_id", "ticker", "name", "exchange_code", "country_code",
            "type", "sector", "industry",
            "currency_code", "currency_name", "currency_symbol",
            "country_name", "country_iso",
            "isin", "cusip", "cik", "lei", "openfigi",
            "ipo_date",
            "gic_sector", "gic_group", "gic_industry", "gic_subindustry",
            "is_delisted", "primary_ticker", "home_category", "fiscal_year_end",
            "exchange_name", "country", "currency", "country_iso2", "country_iso3",
        ]
        for c in preferred_cols:
            if c not in master.columns:
                master[c] = None
        master = master[preferred_cols]

        # --- ì €ì¥
        import pyarrow as pa
        import pyarrow.parquet as pq
        table = pa.Table.from_pandas(master)
        pq.write_table(table, self.output_file.as_posix())

        # --- ë©”íƒ€ ì €ì¥
        meta = {
            "dataset": MASTER_DOMAIN,
            "country_code": self.country_code,
            "snapshot_dt": self.snapshot_dt,
            "exchanges": self.exchanges,
            "vendor_priority": self.vendor_priority,
            "row_count": len(master),
            "created_at": datetime.now(timezone.utc).isoformat(),
            "sources": {
                "symbol_list": True,
                "fundamentals": bool(fund_dfs),
                "exchange_list": bool(ex_dfs),
            },
            "output_file": self.output_file.as_posix(),
        }
        with open(self.meta_file, "w", encoding="utf-8") as f:
            json.dump(meta, f, indent=2, ensure_ascii=False)

        # --- ì „ì—­ ìŠ¤ëƒ…ìƒ· ë ˆì§€ìŠ¤íŠ¸ë¦¬ ê°±ì‹  (asset_master[KOR] / [USA] ...)
        update_global_snapshot_registry(
            domain="asset_master",
            snapshot_dt=self.snapshot_dt,
            meta_file=self.meta_file,
            country_code=self.country_code,
        )

        return meta
