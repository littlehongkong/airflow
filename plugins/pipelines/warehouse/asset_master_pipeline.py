import pandas as pd
from pathlib import Path
from typing import Dict, Any, Optional, List
from datetime import datetime, timezone
import psutil, gc
from plugins.pipelines.warehouse.base_warehouse_pipeline import BaseWarehousePipeline
from plugins.config.constants import (
    WAREHOUSE_DOMAINS,
    EXCLUDED_EXCHANGES_BY_COUNTRY,
    VENDORS, DATA_WAREHOUSE_ROOT
)
from plugins.utils.loaders.exchange_holiday_loader import load_exchange_holiday_list
from plugins.utils.transform_utils import normalize_columns, safe_merge
from plugins.utils.id_generator import generate_or_reuse_entity_id

from plugins.utils.loaders.symbol_loader import load_symbol_list
from plugins.utils.loaders.fundamentals_loader import load_fundamentals_latest
from plugins.utils.loaders.exchange_loader import load_exchange_list


class AssetMasterPipeline(BaseWarehousePipeline):
    """
    âœ… ìì‚° ë§ˆìŠ¤í„° íŒŒì´í”„ë¼ì¸ (Warehouse Domain: asset_master)
    --------------------------------------------------------
    - Symbol / Fundamentals / Exchange ë°ì´í„°ë¥¼ ë³‘í•©í•˜ì—¬ êµ­ê°€ ë‹¨ìœ„ ë§ˆìŠ¤í„° ìƒì„±
    - ê³ ìœ  ID ìƒì„±ì€ id_generator ìœ í‹¸ ì‚¬ìš©
    - Exchange ë§¤í•‘ì€ latest_snapshot_meta.json ê¸°ë°˜ ë™ì  ë¡œë“œ
    """

    def __init__(
        self,
        trd_dt: str,
        domain_group: str,
        country_code: Optional[str] = None,
        vendor: str = None,
    ):
        super().__init__(
            domain=WAREHOUSE_DOMAINS["asset"],
            domain_group=domain_group,
            trd_dt=trd_dt,
            vendor=vendor,
        )
        self.country_code = country_code
        self.exchanges = []

    # ============================================================
    # ğŸ“˜ 1ï¸âƒ£ ê±°ë˜ì†Œ ë§¤í•‘ ë¡œë“œ (latest_snapshot_meta ê¸°ë°˜)
    # ============================================================
    def _load_exchange_codes(self) -> List[str]:
        """latest_snapshot_meta.jsonì—ì„œ í•´ë‹¹ êµ­ê°€ì˜ ê±°ë˜ì†Œ ëª©ë¡ ë¡œë“œ"""
        if not self.country_code:
            raise ValueError("âŒ country_code ê°’ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤. (ì˜ˆ: 'KOR', 'USA')")
        try:
            df = load_exchange_list(domain_group=self.domain_group, vendor=self.vendor, trd_dt=self.trd_dt)

            exchanges = df.loc[
                df["CountryISO3"].astype(str).str.upper() == self.country_code.upper(),
                "Code"
            ].dropna().unique().tolist()

            if not exchanges:
                raise ValueError(f"âŒ {self.country_code} êµ­ê°€ì— ëŒ€í•œ ê±°ë˜ì†Œ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
            self.log.info(f"ğŸŒ {self.country_code} ê±°ë˜ì†Œ ì½”ë“œ: {exchanges}")
            return exchanges
        except Exception as e:
            raise RuntimeError(f"âš ï¸ ê±°ë˜ì†Œ ë§¤í•‘ ë¡œë“œ ì‹¤íŒ¨: {e}")

    # ============================================================
    # ğŸ“˜ 2ï¸âƒ£ Symbol ë¦¬ìŠ¤íŠ¸ ì •ê·œí™”
    # ============================================================
    def _normalize_symbol_list(self, df: pd.DataFrame) -> pd.DataFrame:
        df = normalize_columns(df)

        df_norm = pd.DataFrame({
            "ticker": df.get("code", df.get("symbol", "")).astype(str).str.upper(),
            "security_name": df.get("name", ""),
            "security_type": df.get("type", ""),
            "exchange_code": df.get("exchange", df.get("exchange_code", "")),
            "currency_code": df.get("currency", ""),
            "country_code": df.get("countryiso2", self.country_code)
        }).drop_duplicates(subset=["ticker", "exchange_code"])

        # âœ… ê±°ë˜ì†Œ í•„í„°ë§ (êµ­ê°€ë³„ ì œì™¸ ë¦¬ìŠ¤íŠ¸ ë°˜ì˜)
        exclude_exchanges = EXCLUDED_EXCHANGES_BY_COUNTRY.get(self.country_code, [])
        if exclude_exchanges and "exchange_code" in df_norm.columns:
            before_rows = len(df_norm)
            df_norm = df_norm[~df_norm["exchange_code"].isin(exclude_exchanges)]
            df_norm["exchange_code"] = (
                df_norm["exchange_code"]
                .astype(str)
                .str.upper()
                .str.strip()  # ì•ë’¤ ê³µë°± ì œê±°
                .str.replace(r"[^A-Z0-9]", "", regex=True)  # ê³µë°±, ì½œë¡ , í•˜ì´í”ˆ ì œê±°
            )
            after_rows = len(df_norm)
            self.log.info(
                f"ğŸš« Excluded {before_rows - after_rows:,} symbols "
                f"where exchange_code in {exclude_exchanges} "
                f"({before_rows:,} â†’ {after_rows:,})"
            )

        return df_norm

    # ============================================================
    # ğŸ“˜ 3ï¸âƒ£ Fundamentals ì •ê·œí™”
    # ============================================================
    # ============================================================
    # ğŸ“˜ 3ï¸âƒ£ Fundamentals ì •ê·œí™” (General-only parquet ê¸°ë°˜)
    # ============================================================
    def _normalize_fundamentals(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        fundamentals_general_latest.parquet íŒŒì¼ì„ ê¸°ë°˜ìœ¼ë¡œ ê°„ì†Œí™”ëœ ì •ê·œí™”
        """
        df = normalize_columns(df)
        return df

    # ============================================================
    # ğŸ“˜ 4ï¸âƒ£ Exchange Detail(Holiday) ì •ê·œí™”
    # ============================================================
    def _normalize_exchange_detail(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        âœ… ê±°ë˜ì†Œ ìƒì„¸ì •ë³´ ì •ê·œí™” (Lake â†’ Warehouseìš©)
        - ì—¬ëŸ¬ ê±°ë˜ì†Œ(KO, KQ, etc.) ë°ì´í„° í¬í•¨ ê°€ëŠ¥
        - nested í•„ë“œëŠ” ì œì™¸í•˜ê³  ì£¼ìš” ë©”íƒ€ ì»¬ëŸ¼ë§Œ ìœ ì§€
        """

        assert df.empty is False, "âš ï¸ Empty exchange_list received for normalization."

        # 1ï¸âƒ£ ì»¬ëŸ¼ ì´ë¦„ í†µì¼ (ëŒ€ì†Œë¬¸ì, ê³µë°± ì œê±° ë“±)
        df = normalize_columns(df)  # ì˜ˆ: 'Code' â†’ 'code', 'OperatingMIC' â†’ 'operatingmic'

        # 2ï¸âƒ£ ë§¤í•‘ ì •ì˜
        rename_map = {
            "code": "exchange_code",
            "name": "exchange_name",
            "country": "country_code",
            "currency": "currency_code",
            "timezone": "time_zone",
            "operatingmic": "operating_mic",
        }

        # 3ï¸âƒ£ ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì¶”ì¶œ
        valid_cols = [col for col in rename_map.keys() if col in df.columns]
        if not valid_cols:
            self.log.warning("âš ï¸ No valid exchange columns found to normalize.")
            return pd.DataFrame()

        # 4ï¸âƒ£ ì„ íƒëœ ì»¬ëŸ¼ rename
        df_out = df[valid_cols].rename(columns={k: rename_map[k] for k in valid_cols})

        # 5ï¸âƒ£ ì¤‘ë³µ ì œê±° ë° ì •ë ¬
        df_out = df_out.drop_duplicates(subset=["exchange_code"]).reset_index(drop=True)

        return df_out

    # ============================================================
    # ğŸ“˜ 5ï¸âƒ£ ë³‘í•© ë° ë³€í™˜ ë¡œì§
    # ============================================================
    def _transform_business_logic(
        self,
        symbol_list: pd.DataFrame,
        fundamentals: Optional[pd.DataFrame] = None,
        exchange_list: Optional[pd.DataFrame] = None,
    ) -> pd.DataFrame:
        merged = symbol_list.copy()

        # âœ… fundamentals ë³‘í•©
        if fundamentals is not None and not fundamentals.empty:
            merged = safe_merge(merged, fundamentals, on=["ticker", "exchange_code"], how="left")

        # âœ… exchange ì •ë³´ ë³‘í•©
        if exchange_list is not None and not exchange_list.empty:
            merged = safe_merge(merged, exchange_list, on=["exchange_code"], how="left")

        merged = merged.drop_duplicates(subset=["ticker", "exchange_code"])


        # âœ… ë‚ ì§œ ë©”íƒ€í•„ë“œ
        merged["trd_dt"] = self.trd_dt
        merged["last_symbol_update"] = self.trd_dt
        merged["ingested_at"] = datetime.now(timezone.utc).isoformat()
        merged["source_vendor"] = VENDORS["EODHD"]

        # âœ… ì»¬ëŸ¼ ìˆœì„œ ì •ë ¬
        preferred_cols = [
            "security_id", "ticker", "security_name", "exchange_code", "country_code",
            "security_type", "isin", "cusip", "lei",
            "sector", "industry", "gic_sector", "gic_group",
            "gic_industry", "gic_sub_industry", "ipo_date", "is_delisted",
            "currency_code", "last_fundamental_update", "last_symbol_update",
            "ingested_at", "source_vendor", "snapshot_date"
        ]
        for col in preferred_cols:
            if col not in merged.columns:
                merged[col] = None
        return merged[preferred_cols]

    import pandas as pd

    def _load_source_datasets(self, exchanges: list) -> dict[str, pd.DataFrame]:
        """
        âœ… Exchange-Holiday ê¸°ë°˜ í†µí•© ë°ì´í„° ë¡œë” (ë¶ˆí•„ìš”í•œ nested í•„ë“œ ì œê±°)
        - validated parquet ê¸°ì¤€: ì´ë¯¸ í‰íƒ„í™”ë˜ì–´ ìˆìŒ
        - ì‹¤ì œ warehouseì—ì„œ ì‚¬ìš©í•˜ëŠ” ì£¼ìš” ì»¬ëŸ¼ë§Œ ì¶”ì¶œ
        """
        # 1ï¸âƒ£ Symbol List
        symbol_df = load_symbol_list(
            domain_group=self.domain_group,
            vendor=self.vendor,
            exchange_codes=exchanges,
            trd_dt=self.trd_dt
        )

        # 2ï¸âƒ£ Fundamentals (ìµœì‹  ìŠ¤ëƒ…ìƒ·)
        fundamentals_df = load_fundamentals_latest(
            domain_group=self.domain_group,
            vendor=self.vendor,
            exchange_codes=exchanges
        )

        # 3ï¸âƒ£ Exchange ìƒì„¸ì •ë³´ (Holiday API ê¸°ë°˜)
        exchange_frames = []
        for exchange_code in exchanges:
            df = load_exchange_holiday_list(
                domain_group=self.domain_group,
                vendor=self.vendor,
                trd_dt=self.trd_dt,
                exchange_code=exchange_code
            )

            if df is not None and not df.empty:
                # âœ… í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ë‚¨ê¸°ê¸° (nested í•„ë“œëŠ” ì œì™¸)
                used_cols = [
                    "Name", "Code", "OperatingMIC", "Country", "Currency",
                    "Timezone", "isOpen", "ActiveTickers", "UpdatedTickers"
                ]
                available_cols = [c for c in used_cols if c in df.columns]
                df = df[available_cols].copy()

                # âœ… exchange_code ëª…ì‹œ ì¶”ê°€ (í†µí•© ì‹œ ì¤‘ë³µ ë°©ì§€)
                df["exchange_code"] = exchange_code

                exchange_frames.append(df)

        # 4ï¸âƒ£ í†µí•© Exchange DF ìƒì„±
        exchange_list_df = (
            pd.concat(exchange_frames, ignore_index=True)
            if exchange_frames else pd.DataFrame()
        )

        # âœ… ìµœì¢… ë°˜í™˜
        return {
            "symbol_list": symbol_df,
            "fundamentals": fundamentals_df,
            "exchange_list": exchange_list_df,
        }

    # ============================================================
    # ğŸ“˜ 6ï¸âƒ£ ì „ì²´ ë¹Œë“œ ì‹¤í–‰
    # ============================================================
    def build(self, **kwargs) -> Dict[str, Any]:
        self.log.info(f"ğŸ—ï¸ Building AssetMasterPipeline | trd_dt={self.trd_dt}, country={self.country_code}")

        # âœ… ê±°ë˜ì†Œ ë§¤í•‘
        self.exchanges = self._load_exchange_codes()
        conn = self._get_duckdb_connection()

        # âœ… Symbol ë° Exchange ë¡œë“œ
        sources = self._load_source_datasets(exchanges=self.exchanges)
        symbol_df = self._normalize_symbol_list(sources.get("symbol_list"))
        exchange_detail_df = self._normalize_exchange_detail(sources.get("exchange_list"))
        fundamental_df = self._normalize_fundamentals(sources.get("fundamentals"))

        if symbol_df.empty:
            raise FileNotFoundError("âŒ symbol_list ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")


        # âœ… DuckDB ë³‘í•©
        conn.register("symbol_df", symbol_df)
        conn.register("fundamental_df", fundamental_df)
        conn.register("exchange_detail_df", exchange_detail_df)

        # âœ… DuckDB SQLì—ì„œ ë³‘í•© ë° ID ìƒì„±
        country_fallback = self.country_code or "XX"
        query = f"""
        WITH merged AS (
        SELECT 
            -- âœ… symbol_df ì›ë³¸
            upper(s.ticker) AS ticker,
            s.security_name,                         -- symbol_list ì›ë³¸ í•„ë“œ
            s.exchange_code,
            s.security_type,
            COALESCE(s.country_code, '{country_fallback}') AS country_code,
            s.currency_code,
            f.isin,                                          -- symbol_dfì—ë„ ì¡´ì¬í•˜ëŠ” ê²½ìš°
            f.cusip,
            f.lei,
            f.open_figi,
            f.cik,
            f.fiscal_year_end,
            f.primary_ticker,
            f.sector,
            f.industry,
            f.gic_sector,
            f.gic_group,
            f.gic_industry,
            f.gic_sub_industry,
            f.ipo_date,
            f.is_delisted,
            f.logo_url,
    
            -- âœ… fundamentals (ìµœì‹  ìŠ¤ëƒ…ìƒ· ê´€ë ¨)
            f.last_fundamental_update,
    
            -- âœ… exchange_list (ê±°ë˜ì†Œ ì •ë³´)
            e.exchange_name AS exchange_name,
    
            -- âœ… ë©”íƒ€ì •ë³´
            '{self.trd_dt}'::DATE AS last_symbol_update,
            now() AT TIME ZONE 'UTC' AS ingested_at,
            '{VENDORS["eodhd"]}' AS source_vendor,
            '{self.trd_dt}'::DATE AS snapshot_date
    
        FROM symbol_df s
        LEFT JOIN fundamental_df f
            ON s.ticker = f.ticker AND s.exchange_code = f.exchange_code
        LEFT JOIN exchange_detail_df e
            ON s.exchange_code = e.exchange_code
    )
    SELECT
        ticker,
        security_name AS name,
        exchange_code,
        country_code,
        COALESCE(security_type, NULL) AS security_type,   -- ì¼ë¶€ vendorë§Œ ì œê³µ
        isin,
        cusip,
        lei,
        open_figi,
        cik,
        fiscal_year_end,
        primary_ticker,
        sector,
        industry,
        gic_sector,
        gic_group,
        gic_industry,
        gic_sub_industry,
        ipo_date,
        is_delisted,
        currency_code,
        logo_url,
        last_fundamental_update,
        last_symbol_update,
        ingested_at,
        source_vendor,
        snapshot_date
    FROM merged;

        """

        # âœ… ì¶œë ¥ ê²½ë¡œ(êµ­ê°€ íŒŒí‹°ì…˜) ë™ì¼ ê·œì¹™ ì ìš©
        output_dir = self.output_file.parent
        if self.country_code:
            output_dir = output_dir / f"country_code={self.country_code}"
            output_dir.mkdir(parents=True, exist_ok=True)
            output_file = (output_dir / self.output_file.name).as_posix()
        else:
            output_dir.mkdir(parents=True, exist_ok=True)
            output_file = self.output_file.as_posix()

        # âœ… DuckDBë¡œ ë³‘í•© ê²°ê³¼ DataFrameë§Œ ê°€ì ¸ì˜¤ê¸°
        self.log.info("ğŸ§© Executing DuckDB join query ...")
        merged_df = conn.execute(query).df()

        # âœ… Pythonì—ì„œ ê¸°ì¡´ ID ìœ ì§€ + ì‹ ê·œ ID ìƒì„±
        merged_df = self._assign_persistent_security_id(merged_df)

        # âœ… ì €ì¥
        self.save_parquet(merged_df)

        row_count = len(merged_df)
        file_size = Path(output_file).stat().st_size
        self.log.info(f"âœ… Parquet saved: {output_file} ({row_count:,} rows, {file_size:,} bytes)")

        meta = self.save_metadata(
            row_count=row_count,
            source_datasets=["symbol_list", "fundamentals_general_latest", "exchange_list"],
            metrics={
                "symbol_count": row_count,
                "vendor": self.vendor,
                "exchanges": self.exchanges,
            },
            context=kwargs.get("context"),
        )

        self.log.info(f"âœ… [BUILD COMPLETE] asset_master | {row_count:,} symbols ({self.country_code})")
        gc.collect()
        return meta


    # ============================================================
    # â™»ï¸ ê¸°ì¡´ security_id ìœ ì§€ + ì‹ ê·œ ë°œê¸‰ í•¨ìˆ˜
    # ============================================================
    def _assign_persistent_security_id(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        ê¸°ì¡´ ìŠ¤ëƒ…ìƒ·ì—ì„œ security_idë¥¼ ë¶ˆëŸ¬ì™€ ë™ì¼ ì¢…ëª©ì— ë™ì¼ IDë¥¼ ì¬ì‚¬ìš©í•˜ê³ ,
        ì‹ ê·œ ì¢…ëª©ë§Œ ìƒˆ IDë¥¼ ìƒì„±í•œë‹¤.
        """

        if df.empty:
            self.log.warning("âš ï¸ No records to assign security_id (empty DataFrame).")
            return df

        existing_map = {}
        latest_snapshot_dir = (
            Path(DATA_WAREHOUSE_ROOT)
            / "snapshot"
            / self.domain_group
            / self.domain
        )

        if latest_snapshot_dir.exists():
            snapshots = sorted(latest_snapshot_dir.glob("trd_dt=*"), reverse=True)
            if snapshots:
                latest_snapshot_path = snapshots[0] / f"{self.domain}.parquet"
                if latest_snapshot_path.exists():
                    try:
                        old_df = pd.read_parquet(latest_snapshot_path)
                        existing_map = (
                            old_df[["ticker", "exchange_code", "security_id"]]
                            .drop_duplicates()
                            .set_index(["ticker", "exchange_code"])["security_id"]
                            .to_dict()
                        )
                        self.log.info(
                            f"â™»ï¸ Loaded {len(existing_map):,} existing security_id mappings "
                            f"from {latest_snapshot_path}"
                        )
                    except Exception as e:
                        self.log.warning(f"âš ï¸ Failed to load previous snapshot for ID mapping: {e}")

        def assign_security_id(row):
            key = (str(row.get("ticker", "")).upper(), str(row.get("exchange_code", "")).upper())
            if key in existing_map:
                return existing_map[key]
            return generate_or_reuse_entity_id(
                prefix="AST",
                country=row.get("country_code", self.country_code),
                exchange=row.get("exchange_code", ""),
                ticker=row.get("ticker", "")
            )

        df["security_id"] = df.apply(assign_security_id, axis=1)
        self.log.info(f"ğŸ”‘ Assigned deterministic security_id for {len(df):,} records")

        return df
