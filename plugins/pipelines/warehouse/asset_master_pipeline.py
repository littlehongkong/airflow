# plugins/pipelines/warehouse/asset_master_pipeline.py
import pandas as pd
from typing import Dict, Any, List
from datetime import datetime, timezone

from plugins.pipelines.warehouse.base_warehouse_pipeline import BaseWarehousePipeline
from plugins.config.constants import WAREHOUSE_DOMAINS, WAREHOUSE_SOURCE_MAP, VENDORS


class AssetMasterPipeline(BaseWarehousePipeline):
    """
    âœ… ì¢…ëª© ë§ˆìŠ¤í„° íŒŒì´í”„ë¼ì¸ (Asset Master)
    ì…ë ¥: validated/symbol_list, symbol_changes
    ì¶œë ¥: warehouse/asset/snapshot_dt=YYYY-MM-DD/asset.parquet
    """

    def __init__(self, snapshot_dt: str, vendor_priority: List[str] = None):
        super().__init__(
            domain=WAREHOUSE_DOMAINS["ASSET"],
            snapshot_dt=snapshot_dt,
            vendor_priority=vendor_priority,
        )

    # =============================
    # ë°ì´í„° ì •ê·œí™”
    # =============================
    def _normalize_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:
        df = df.copy()
        df.columns = df.columns.str.lower()

        # í•„ìˆ˜ ì»¬ëŸ¼ ë§¤í•‘
        symbol_col = next((c for c in ["code", "symbol", "ticker"] if c in df.columns), None)
        if not symbol_col:
            raise ValueError("âŒ ì¢…ëª© ë°ì´í„°ì— symbol/code ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")

        normalized = pd.DataFrame({
            "symbol": df[symbol_col].astype(str).str.strip().str.upper(),
            "name": df.get("name", df.get("symbol_name", "")),
            "exchange_code": df.get("exchange", df.get("exchange_code", "")),
            "currency": df.get("currency", ""),
            "isin": df.get("isin", ""),
            "figi": df.get("figi", ""),
            "sector": df.get("sector", ""),
            "industry": df.get("industry", ""),
            "country": df.get("country", ""),
        })

        # í•„ìˆ˜ê°’ í•„í„°ë§
        normalized = normalized[normalized["symbol"] != ""].reset_index(drop=True)
        return normalized

    def _get_preferred_columns(self) -> List[str]:
        return [
            "symbol",
            "name",
            "exchange_code",
            "currency",
            "isin",
            "figi",
            "sector",
            "industry",
            "country",
        ]

    # =============================
    # ë©”ì¸ ë¹Œë“œ ë¡œì§
    # =============================
    def build(self, **kwargs) -> Dict[str, Any]:
        """
        ì¢…ëª© ë§ˆìŠ¤í„° ë¹Œë“œ
        1ï¸âƒ£ symbol_list + symbol_changes validated ë°ì´í„° ë¡œë“œ
        2ï¸âƒ£ ë³‘í•© ë° ì •ê·œí™”
        3ï¸âƒ£ ì¤‘ë³µ ì œê±°
        4ï¸âƒ£ Parquet ì €ì¥ + ë©”íƒ€ë°ì´í„° ê¸°ë¡
        """
        self.log.info(f"ğŸ—ï¸ Building {self.domain} for snapshot_dt={self.snapshot_dt}")

        # 1ï¸âƒ£ ì†ŒìŠ¤ íŒŒì¼ ë¡œë“œ
        source_domains = WAREHOUSE_SOURCE_MAP[self.domain]  # ["symbol_list", "symbol_changes"]
        parquet_files = self._get_validated_files(source_domains)

        if not parquet_files:
            raise FileNotFoundError(f"âŒ No validated parquet files found for {source_domains}")
        self.log.info(f"ğŸ“‚ Found {len(parquet_files)} parquet files from {source_domains}")

        # 2ï¸âƒ£ íŒŒì¼ ë³‘í•©
        conn = self._get_duckdb_connection()
        files_expr = ", ".join([f"'{str(p)}'" for p in parquet_files])
        raw_df = conn.execute(f"SELECT * FROM read_parquet([{files_expr}])").df()
        self.log.info(f"ğŸ“Š Loaded {len(raw_df):,} raw records")

        # 3ï¸âƒ£ ì •ê·œí™” ë° ì¤‘ë³µ ì œê±°
        normalized = self._normalize_dataframe(raw_df)
        deduped = normalized.drop_duplicates(subset=["symbol"], keep="first").reset_index(drop=True)
        self.log.info(f"ğŸ”„ Deduplicated: {len(normalized)} â†’ {len(deduped)}")

        final_df = self._reorder_columns(deduped)

        # 4ï¸âƒ£ ì €ì¥
        save_result = self.save_parquet(final_df)

        # 5ï¸âƒ£ ë©”íƒ€ë°ì´í„°
        meta = self.save_metadata(
            row_count=len(final_df),
            source_info={
                "datasets": source_domains,
                "validated_files": [str(p) for p in parquet_files],
                "record_counts": {self.domain: len(final_df)},
                "last_validated_timestamps": {
                    d: datetime.now(timezone.utc).isoformat() for d in source_domains
                },
            },
            metrics={
                "symbol_count": len(final_df),
                "partial_records": int(final_df[["sector", "industry"]].isna().any(axis=1).sum()),
                "vendor_priority": [VENDORS["EODHD"]]
            },
            context=kwargs.get("context"),
        )

        self.update_registry()
        self.log.info(f"âœ… [BUILD COMPLETE] {self.domain} | {len(final_df)} symbols")

        return meta
