import json
from pathlib import Path
from typing import Dict, Any
import pandas as pd

from plugins.pipelines.warehouse.base_warehouse_pipeline import BaseWarehousePipeline
from plugins.utils.loaders.warehouse.asset_master_loader import load_asset_master_latest
from plugins.config.constants import DATA_LAKE_ROOT, DATA_WAREHOUSE_ROOT


class FundamentalsTickerSplitPipeline(BaseWarehousePipeline):
    """
    âœ… Fundamentals Warehouse Builder (Ticker + Section Split)
    Data Lake â†’ Warehouse (per security_id, per section JSON)

    Example output:
    data_warehouse/snapshot/equity/fundamentals/
        trd_dt=2025-11-05/
        exchange_code=KQ/
        security_id=AST_KOR_KQ_AAPL/
        General.json
    """

    def __init__(self, domain_group: str, vendor: str, exchange_code: str, trd_dt: str, country_code: str, **kwargs):
        super().__init__(
            domain="fundamentals",
            domain_group=domain_group,
            trd_dt=trd_dt,
            country_code=country_code,
        )
        self.vendor = vendor
        self.exchange_code = exchange_code
        self.trigger_source = kwargs.get("trigger_source", None)

    # ------------------------------------------------------------------
    def _load_source_datasets(self) -> Dict[str, pd.DataFrame]:
        """
        âœ… BaseWarehousePipelineì˜ abstractmethod êµ¬í˜„
        - asset_master ë¡œë“œ
        - fundamentals JSON íŒŒì¼ ëª©ë¡ ë¡œë“œ (Pathë§Œ ë°˜í™˜)
        """
        lake_dir = (
            Path(DATA_LAKE_ROOT)
            / "validated"
            / self.domain_group
            / "fundamentals"
            / f"vendor={self.vendor}"
            / f"exchange_code={self.exchange_code}"
            / f"trd_dt={self.trd_dt}"
        )

        if not lake_dir.exists():
            raise FileNotFoundError(f"âŒ Source directory not found: {lake_dir}")

        json_files = list(lake_dir.glob("*.json"))
        if not json_files:
            raise FileNotFoundError(f"âŒ No JSON files found in {lake_dir}")
        else:
            self.log.info(f"ì²˜ë¦¬í•  ëŒ€ìƒ íŒŒì¼ ëª©ë¡ : {len(json_files)}")

        # ğŸ§© ë§ˆìŠ¤í„° ë¡œë“œ
        master_df = load_asset_master_latest(self.domain_group)

        # ê¸°ë³¸ í‚¤ ì •ê·œí™”
        master_df["ticker"] = master_df["ticker"].astype(str).str.upper().str.strip()
        master_df["exchange_code"] = master_df["exchange_code"].astype(str).str.upper().str.strip()

        return {"json_files": json_files, "master_df": master_df}

    def _setup_output_paths(self):
        """
        âœ… Fundamentals ì „ìš© ì¶œë ¥ ë””ë ‰í† ë¦¬
        - country_code / trd_dt íŒŒí‹°ì…˜ êµ¬ì¡° ìœ ì§€
        - BaseWarehousePipeline ê¸°ë³¸ ì„¤ì •ì€ ë¹„í™œì„±í™”
        """
        snapshot_root = (
                DATA_WAREHOUSE_ROOT
                / "snapshot"
                / self.domain_group
                / "fundamentals"
                / f"country_code={self.country_code}"
                / f"trd_dt={self.trd_dt}"
        )
        snapshot_root.mkdir(parents=True, exist_ok=True)

        self.output_dir = snapshot_root
        self.output_file = snapshot_root / f"{self.domain}.parquet"
        self.meta_file = snapshot_root / "_build_meta.json"

        # (ì˜µì…˜) ê³µìš© ë©”íƒ€íŒŒì¼ í•„ìš” ì‹œë§Œ ìœ ì§€
        self.domain_meta_file = (
                DATA_WAREHOUSE_ROOT
                / self.domain_group
                / self.domain
                / "_warehouse_meta.json"
        )

        self.log.info(f"ğŸ“¦ Output path configured: {snapshot_root}")


    # ------------------------------------------------------------------
    def _transform_business_logic(self, **kwargs) -> Dict[str, Any]:
        """
        ğŸ’¡ asset_masterë¥¼ ê¸°ì¤€ìœ¼ë¡œ fundamentals JSONì„ security_idë³„ / sectionë³„ë¡œ ì €ì¥
        - master_dfì—ëŠ” ì´ë¯¸ ë¹„ì£¼ë¥˜ ê±°ë˜ì†Œ ì¢…ëª©ì´ ì œì™¸ëœ ìƒíƒœ
        - masterì— ì¡´ì¬í•˜ëŠ” tickerë§Œ JSON ë§¤í•‘ ëŒ€ìƒ
        """
        json_files = kwargs["json_files"]
        master_df = kwargs["master_df"]

        self.log.info(
            f"ğŸ—ï¸ Building FundamentalsTickerSplitPipeline | exchange_code={self.exchange_code}, trd_dt={self.trd_dt}"
        )

        # JSON íŒŒì¼ ì¸ë±ìŠ¤ (ticker â†’ Path)
        json_index = {f.stem.upper(): f for f in json_files}

        # Warehouse ê²½ë¡œ ì„¤ì • (êµ­ê°€ ë‹¨ìœ„)
        base_out = (
                Path(DATA_WAREHOUSE_ROOT)
                / "snapshot"
                / self.domain_group
                / "fundamentals"
                / f"country_code={self.country_code}"
                / f"trd_dt={self.trd_dt}"
        )
        base_out.mkdir(parents=True, exist_ok=True)

        ticker_count, section_count, skipped = 0, 0, 0

        # ----------------------------------------------------------
        # ë§ˆìŠ¤í„° ê¸°ì¤€ìœ¼ë¡œ ticker ë§¤í•‘
        # ----------------------------------------------------------
        for _, row in master_df.iterrows():
            ticker = str(row["ticker"]).upper().strip()
            security_id = row["security_id"]

            # í•´ë‹¹ tickerì˜ fundamentals JSON ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            jf = json_index.get(ticker)
            if jf is None:
                skipped += 1
                continue  # JSONì´ ì—†ìœ¼ë©´ skip

            try:
                with open(jf, "r", encoding="utf-8") as f:
                    data = json.load(f)
                if not isinstance(data, dict):
                    continue

                # ë³´ê´€ ê²½ë¡œ: security_id ê¸°ì¤€ìœ¼ë¡œ ìƒì„±
                security_dir = base_out / f"security_id={security_id}"
                security_dir.mkdir(parents=True, exist_ok=True)

                # ì„¹ì…˜ë³„ JSON ì €ì¥
                for section_name, section_data in data.items():
                    out_path = security_dir / f"{section_name}.json"
                    with open(out_path, "w", encoding="utf-8") as out_f:
                        json.dump(section_data, out_f, ensure_ascii=False, indent=2)
                    section_count += 1

                ticker_count += 1
                self.log.info(f"ğŸ“„ Saved {len(data.keys())} sections for {security_id} ({ticker})")

            except Exception as e:
                skipped += 1
                self.log.warning(f"âš ï¸ Failed to process {ticker}: {e}")

        self.log.info(
            f"âœ… Fundamentals ticker-split build complete | {ticker_count} saved | {skipped} skipped | {section_count} total sections | path={base_out}"
        )

        # ë©”íƒ€ë°ì´í„° ì €ì¥
        meta = self.save_metadata(
            row_count=ticker_count,
            country_code=self.country_code,
            vendor=self.vendor,
            section_count=section_count,
            skipped=skipped,
            context=kwargs.get("context"),
        )

        return meta

    # ------------------------------------------------------------------
    def build(self, **kwargs) -> Dict[str, Any]:
        """ë©”ì¸ ë¹Œë“œ"""
        self.log.info("ğŸš€ [START] FundamentalsTickerSplitPipeline")
        datasets = self._load_source_datasets()
        result = self._transform_business_logic(**datasets)
        self.log.info(
            f"âœ… [BUILD COMPLETE] fundamentals_ticker_split | {self.exchange_code} | trd_dt={self.trd_dt}"
        )
        return result
