import json
import logging
import duckdb
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
from abc import ABC, abstractmethod
from typing import List, Dict, Optional, Any
from datetime import datetime, timezone
from pathlib import Path

from plugins.config.constants import (
    DATA_LAKE_ROOT, DATA_WAREHOUSE_ROOT,
    VENDORS, WAREHOUSE_SOURCE_MAP,
    VALIDATOR_SCHEMA_WAREHOUSE
)

class BaseWarehousePipeline(ABC):
    """
    âœ… Warehouse ê³µí†µ íŒŒì´í”„ë¼ì¸ ë² ì´ìŠ¤ í´ë˜ìŠ¤

    ì£¼ìš” ê¸°ëŠ¥:
    - Data Lake validated â†’ Warehouse í†µí•© ë³€í™˜ íŒŒì´í”„ë¼ì¸ì˜ í‘œì¤€ ì¸í„°í˜ì´ìŠ¤
    - ë°ì´í„° ë¡œë”©, íŒŒì¼€ ì €ì¥, ë©”íƒ€ë°ì´í„° ê¸°ë¡, ìŠ¤ëƒ…ìƒ· ë ˆì§€ìŠ¤íŠ¸ë¦¬ ê°±ì‹  ë‹´ë‹¹
    - ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ì€ í•˜ìœ„ í´ë˜ìŠ¤ì˜ `_transform_business_logic()`ì—ì„œ ìˆ˜í–‰
    """

    # -------------------------------------------------------------------------
    # 1ï¸âƒ£ ì´ˆê¸°í™” ë° ê²½ë¡œ ì„¤ì •
    # -------------------------------------------------------------------------
    def __init__(
            self,
            domain: str,
            domain_group: str,
            trd_dt: str,
            vendor_priority: Optional[List[str]] = None,
            country_code: Optional[str] = None,
    ):
        self.layer:str ='warehouse'
        self.domain = domain
        self.domain_group = domain_group
        self.trd_dt = trd_dt
        self.vendor_priority = vendor_priority or [VENDORS.get("EODHD", "eodhd")]
        self.log = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
        self.country_code = country_code # êµ­ê°€ë‹¨ìœ„ íŒŒí‹°ì…”ë‹ì— ì‚¬ìš©

        # âœ… ê²½ë¡œ ì„¤ì • í†µí•©
        self._setup_output_paths()

        self.conn = None  # duckdb ì—°ê²° ê°ì²´
        self.exchanges: list = []

    # -------------------------------------------------------------------------
    # 2ï¸âƒ£ ê³µí†µ ë°ì´í„° ë¡œë”©
    # -------------------------------------------------------------------------
    def _get_duckdb_connection(self) -> duckdb.DuckDBPyConnection:
        """DuckDB ì—°ê²° ë°˜í™˜"""
        if self.conn is None:
            self.conn = duckdb.connect(database=":memory:")
        return self.conn

    def _load_source_datasets(self, warehouse_domain: str) -> Dict[str, pd.DataFrame]:
        """
        âœ… êµ­ê°€ë³„ ë‹¤ì¤‘ ê±°ë˜ì†Œ ì§€ì› Data Lake â†’ Warehouse ë°ì´í„° ë¡œë”
        - self.exchangesë¥¼ ê¸°ë°˜ìœ¼ë¡œ exchange_code ë¦¬ìŠ¤íŠ¸ ìë™ ì¡°íšŒ
        - exchange_listëŠ” í•­ìƒ exchange_code=ALL
        - fundamentalsëŠ” JSON, ë‚˜ë¨¸ì§€ëŠ” Parquet
        - union_by_name=True (ìŠ¤í‚¤ë§ˆ ë¶ˆì¼ì¹˜ í—ˆìš©)
        """
        source_map = WAREHOUSE_SOURCE_MAP.get(warehouse_domain)
        if not source_map:
            raise ValueError(f"âŒ No source mapping defined for {warehouse_domain}")

        # ê³µí†µ ì†ì„±
        trd_dt = getattr(self, "trd_dt", None)
        domain_group = getattr(self, "domain_group", None)
        vendor = getattr(self, "vendor", "eodhd")
        country_code = getattr(self, "country_code", None)
        conn = self._get_duckdb_connection()

        if not all([trd_dt, domain_group, country_code]):
            raise ValueError(
                f"âŒ Missing required parameters: trd_dt={trd_dt}, domain_group={domain_group}, country_code={country_code}")

        # âœ… êµ­ê°€ë³„ ê±°ë˜ì†Œì½”ë“œ ë¦¬ìŠ¤íŠ¸ ë¡œë“œ
        if not self.exchanges:
            raise ValueError(f"âŒ No exchange codes found for country={country_code} in self.exchanges")

        self.log.info(f"ğŸŒ {country_code} ê±°ë˜ì†Œ ëª©ë¡: {self.exchanges}")

        results: Dict[str, pd.DataFrame] = {}

        for lake_domain in source_map:
            dfs = []
            # âœ… exchange_listëŠ” ALL ê³ ì •
            target_codes = ["ALL"] if lake_domain == "exchange_list" else self.exchanges

            for ex_code in target_codes:
                base_path = (
                        Path(DATA_LAKE_ROOT)
                        / "validated"
                        / domain_group
                        / lake_domain
                        / f"vendor={vendor}"
                        / f"exchange_code={ex_code}"
                        / f"trd_dt={trd_dt}"
                )

                is_fundamentals = lake_domain == "fundamentals"
                file_pattern = "*.json" if is_fundamentals else "*.parquet"

                if not base_path.exists():
                    self.log.warning(f"âš ï¸ Directory not found for {lake_domain}: {base_path}")
                    continue

                files = list(base_path.rglob(file_pattern))
                if not files:
                    self.log.warning(f"âš ï¸ No {file_pattern} files for {lake_domain} ({ex_code})")
                    continue

                try:
                    if is_fundamentals:
                        query = f"SELECT * FROM read_json_auto('{base_path / file_pattern}')"
                    else:
                        query = f"SELECT * FROM read_parquet('{base_path / file_pattern}', union_by_name=true)"

                    df = conn.execute(query).df()

                    if df.empty:
                        self.log.warning(f"âš ï¸ {lake_domain} empty for exchange_code={ex_code}")
                        continue

                    df["exchange_code"] = ex_code
                    dfs.append(df)

                    self.log.info(
                        f"ğŸ“Š Loaded {len(df):,} rows | {lake_domain} | exchange_code={ex_code} | trd_dt={trd_dt}"
                    )

                except Exception as e:
                    self.log.warning(f"âš ï¸ Failed to load {lake_domain} ({ex_code}): {e}")

            # âœ… ì—¬ëŸ¬ ê±°ë˜ì†Œ ë°ì´í„°ë¥¼ í•˜ë‚˜ë¡œ ë³‘í•©
            if dfs:
                results[lake_domain] = pd.concat(dfs, ignore_index=True)
            else:
                raise FileNotFoundError(f"âŒ No valid data found for {lake_domain}")

        return results

    # -------------------------------------------------------------------------
    # 3ï¸âƒ£ ìŠ¤í‚¤ë§ˆ ë¡œë“œ ë° ì»¬ëŸ¼ ìˆœì„œ ì •ë ¬
    # -------------------------------------------------------------------------
    def _load_schema_definition(self) -> dict:
        """ğŸ“˜ warehouse_schemas í´ë”ì—ì„œ domainë³„ ìŠ¤í‚¤ë§ˆ ì •ì˜ JSONì„ ë¡œë“œ"""
        schema_path = VALIDATOR_SCHEMA_WAREHOUSE / f"{self.domain}.json"
        if not schema_path.exists():
            self.log.warning(f"âš ï¸ Schema file not found for {self.domain}")
            return {}
        else:
            self.log.info(f"ğŸ†—ï¸ Schema file exists! {schema_path}")
        with open(schema_path, "r", encoding="utf-8") as f:
            return json.load(f)

    def _get_preferred_columns(self) -> list[str]:
        schema = self._load_schema_definition()
        return [c["name"] for c in schema.get("columns", [])]

    def _reorder_columns(self, df: pd.DataFrame) -> pd.DataFrame:
        """ì»¬ëŸ¼ ìˆœì„œ ì¬ì •ë ¬"""
        preferred = self._get_preferred_columns()
        if not preferred:
            return df
        for col in preferred:
            if col not in df.columns:
                df[col] = None

        return df[[c for c in preferred if c in df.columns]]

    # -------------------------------------------------------------------------
    # 4ï¸âƒ£ ì €ì¥ ë° ë©”íƒ€ë°ì´í„° ê´€ë¦¬
    # -------------------------------------------------------------------------
    def save_parquet(self, df: pd.DataFrame) -> Dict[str, Any]:
        """
        Parquet íŒŒì¼ ì €ì¥
        - object ì»¬ëŸ¼ì€ ë¬¸ìì—´ë¡œ ë³€í™˜ (PyArrow ArrowTypeError ë°©ì§€)
        """
        try:
            # ğŸ”¹ ëª¨ë“  object íƒ€ì… ì»¬ëŸ¼ì„ ë¬¸ìì—´ë¡œ ë³€í™˜
            for col in df.select_dtypes(include=["object"]).columns:
                df[col] = df[col].astype(str)

            table = pa.Table.from_pandas(df)

            # âœ… êµ­ê°€ íŒŒí‹°ì…˜ ë””ë ‰í† ë¦¬ ì¶”ê°€
            if self.country_code:
                output_dir = self.output_file.parent / f"country_code={self.country_code}"
                output_dir.mkdir(parents=True, exist_ok=True)
                self.output_file = output_dir / self.output_file.name  # ì˜ˆ: /asset/snapshot_dt=2025-11-05/country_code=US/asset.parquet

            pq.write_table(table, self.output_file.as_posix())

            file_size = self.output_file.stat().st_size
            self.log.info(
                f"âœ… Parquet saved: {self.output_file} "
                f"({len(df):,} rows, {file_size:,} bytes)")

            return {
                "file_path": self.output_file.as_posix(),
                "row_count": len(df),
                "file_size_bytes": file_size,
            }

        except Exception as e:
            self.log.error(f"âŒ Failed to save parquet: {e}")
            raise

    def _setup_output_paths(self):
        """
        âœ… ì¶œë ¥ ë””ë ‰í† ë¦¬ ë° íŒŒì¼ ê²½ë¡œ ì„¤ì •
        - snapshot ë ˆì´ì–´ì— ìš°ì„  ì €ì¥
        - validatedëŠ” validatorê°€ promote ì‹œ ìƒì„±
        """
        snapshot_root = (
                DATA_WAREHOUSE_ROOT
                / "snapshot"
                / self.domain_group
                / self.domain
                / f"trd_dt={self.trd_dt}"
        )
        snapshot_root.mkdir(parents=True, exist_ok=True)

        self.output_dir = snapshot_root
        self.output_file = snapshot_root / f"{self.domain}.parquet"
        self.meta_file = snapshot_root / "_build_meta.json"

        # ğŸ“˜ ë„ë©”ì¸ ì „ì—­ ë©”íƒ€íŒŒì¼ ê²½ë¡œ (ì˜ˆ: /data_warehouse/exchange/_warehouse_meta.json)
        self.domain_meta_file = DATA_WAREHOUSE_ROOT / self.domain / "_warehouse_meta.json"

    def save_metadata(self, row_count: int, **kwargs) -> Dict[str, Any]:
        """
        ë©”íƒ€ë°ì´í„° íŒŒì¼(_metadata.json) ê¸°ë¡
        """
        import json
        meta = {
            "domain": self.domain,
            "snapshot_dt": self.trd_dt,
            "row_count": row_count,
            "timestamp": datetime.utcnow().isoformat(),
            **kwargs,  # âœ… ì¶”ê°€ì ì¸ ëª¨ë“  ì¸ì í¬í•¨
        }

        meta_path = (
                DATA_WAREHOUSE_ROOT
                / self.domain
                / "snapshot"
                / f"trd_dt={self.trd_dt}"
                / "_metadata.json"
        )

        meta_path.parent.mkdir(parents=True, exist_ok=True)
        with open(meta_path, "w") as f:
            json.dump(meta, f, indent=2)

        self.log.info(f"ğŸ§¾ Metadata saved â†’ {meta_path.as_posix()}")
        return meta


    def _reorder_columns(self, df, schema_name: str = None):
        """
        âœ… Pandera JSON ìŠ¤í‚¤ë§ˆ ê¸°ì¤€ìœ¼ë¡œ ì»¬ëŸ¼ ìˆœì„œ ì •ë ¬
        ëª¨ë“  Warehouse íŒŒì´í”„ë¼ì¸ì—ì„œ ê³µí†µ ì‚¬ìš©
        """
        if not schema_name:
            schema_name = f"{self.domain}_schema.json"

        schema_path = VALIDATOR_SCHEMA_WAREHOUSE / schema_name
        if not schema_path.exists():
            self.log.warning(f"âš ï¸ ìŠ¤í‚¤ë§ˆ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {schema_path}")
            return df

        try:
            schema = json.loads(schema_path.read_text())
            preferred_order = [col["name"] for col in schema.get("columns", [])]
            existing = [c for c in preferred_order if c in df.columns]
            others = [c for c in df.columns if c not in existing]
            ordered_df = df[existing + others]
            return ordered_df
        except Exception as e:
            self.log.warning(f"âš ï¸ ì»¬ëŸ¼ ì •ë ¬ ì‹¤íŒ¨, ì›ë³¸ ìˆœì„œ ìœ ì§€: {e}")
            return df

    # -------------------------------------------------------------------------
    # 5ï¸âƒ£ ì¶”ìƒ ë©”ì„œë“œ (í•˜ìœ„ í´ë˜ìŠ¤ êµ¬í˜„ë¶€)
    # -------------------------------------------------------------------------    @abstractmethod
    def _normalize_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:
        """ë„ë©”ì¸ë³„ ì •ê·œí™” ë¡œì§ (í•˜ìœ„ í´ë˜ìŠ¤ êµ¬í˜„)"""
        pass

    @abstractmethod
    def _transform_business_logic(self, **kwargs) -> pd.DataFrame:
        """
        ë„ë©”ì¸ë³„ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ êµ¬í˜„ë¶€
        ì˜ˆì‹œ:
          - exchange: ê±°ë˜ì†Œ + íœ´ì¥ì¼ ë³‘í•©
        """
        pass

    def _update_domain_metadata(self, record_count: int):
        """
        âœ… ë„ë©”ì¸ ë£¨íŠ¸ì— `_warehouse_meta.json` ìƒì„±/ê°±ì‹ 
        - ìµœì‹  ìŠ¤ëƒ…ìƒ· ì¼ì, ìµœì´ˆ ì ì¬ì¼ì, ì´ ì ì¬ íšŸìˆ˜ ë“±ì„ ê´€ë¦¬
        """
        meta_path = DATA_WAREHOUSE_ROOT / self.domain / "_warehouse_meta.json"
        now = datetime.now(timezone.utc).isoformat()

        new_meta = {
            "domain": self.domain,
            "latest_snapshot": self.trd_dt,
            "record_count": record_count,
            "last_build_meta": str(self.meta_file),
            "updated_at": now,
        }

        # ì´ì „ ë©”íƒ€ ìœ ì§€ (ìµœì´ˆ ì ì¬ì¼, ëˆ„ì  ì¹´ìš´íŠ¸)
        if meta_path.exists():
            try:
                with open(meta_path, "r", encoding="utf-8") as f:
                    old = json.load(f)
                new_meta["first_ingested"] = old.get("first_ingested", self.trd_dt)
                new_meta["total_snapshots"] = old.get("total_snapshots", 0) + 1
            except Exception:
                new_meta["first_ingested"] = self.trd_dt
                new_meta["total_snapshots"] = 1
        else:
            new_meta["first_ingested"] = self.trd_dt
            new_meta["total_snapshots"] = 1

        with open(meta_path, "w", encoding="utf-8") as f:
            json.dump(new_meta, f, indent=2, ensure_ascii=False)

        self.log.info(f"ğŸ“˜ Domain meta updated: {meta_path}")



    @abstractmethod
    def build(self, **kwargs) -> Dict[str, Any]:
        """ë©”ì¸ ë¹Œë“œ í”„ë¡œì„¸ìŠ¤ (í•˜ìœ„ í´ë˜ìŠ¤ êµ¬í˜„)"""
        pass

    # -------------------------------------------------------------------------
    # 6ï¸âƒ£ ë¦¬ì†ŒìŠ¤ ì •ë¦¬
    # -------------------------------------------------------------------------
    def cleanup(self):
        """DuckDB ì—°ê²° ì¢…ë£Œ"""
        if self.conn:
            self.conn.close()
            self.conn = None

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.cleanup()