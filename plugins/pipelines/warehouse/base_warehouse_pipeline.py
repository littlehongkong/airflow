import json
import logging
import duckdb
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
from abc import ABC, abstractmethod
from typing import Dict, Optional, Any
from datetime import datetime, timezone

from plugins.config.constants import (
    DATA_WAREHOUSE_ROOT, VALIDATOR_SCHEMA_LAKE, VALIDATOR_CHECKS_LAKE, DATA_LAKE_ROOT,
    VALIDATOR_SCHEMA_WAREHOUSE, VALIDATOR_CHECKS_WAREHOUSE
)

class BaseWarehousePipeline(ABC):
    """
    âœ… Warehouse ê³µí†µ íŒŒì´í”„ë¼ì¸ ë² ì´ìŠ¤ í´ë˜ìŠ¤

    ì£¼ìš” ê¸°ëŠ¥:
    - Data Lake validated â†’ Warehouse í†µí•© ë³€í™˜ íŒŒì´í”„ë¼ì¸ì˜ í‘œì¤€ ì¸í„°í˜ì´ìŠ¤
    - ë°ì´í„° ë¡œë”©, íŒŒì¼€ ì €ì¥, ë©”íƒ€ë°ì´í„° ê¸°ë¡, ìŠ¤ëƒ…ìƒ· ë ˆì§€ìŠ¤íŠ¸ë¦¬ ê°±ì‹  ë‹´ë‹¹
    - ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ì€ í•˜ìœ„ í´ë˜ìŠ¤ì˜ `_transform_business_logic()`ì—ì„œ ìˆ˜í–‰
    """

    # -------------------------------------------------------------------------
    # 1ï¸âƒ£ ì´ˆê¸°í™” ë° ê²½ë¡œ ì„¤ì •
    # -------------------------------------------------------------------------
    def __init__(
            self,
            domain: str,
            domain_group: str,
            trd_dt: str,
            vendor: str = None,
            country_code: Optional[str] = None,
    ):
        self.layer:str ='warehouse'
        self.domain = domain
        self.domain_group = domain_group
        self.trd_dt = trd_dt
        self.vendor = vendor
        self.log = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
        self.country_code = country_code # êµ­ê°€ë‹¨ìœ„ íŒŒí‹°ì…”ë‹ì— ì‚¬ìš©

        # âœ… ê²½ë¡œ ì„¤ì • í†µí•©
        self._setup_output_paths()

        self.conn = None  # duckdb ì—°ê²° ê°ì²´
        self.exchanges: list = []

        # âœ… constants ê¸°ë°˜ ê²½ë¡œ ì„¤ì •
        if self.layer == "lake":
            self.schema_root = VALIDATOR_SCHEMA_LAKE / domain_group / vendor.lower()
            self.check_root = VALIDATOR_CHECKS_LAKE / domain_group / vendor.lower()
            self.data_root = DATA_LAKE_ROOT

        elif self.layer == "warehouse":
            # âœ… equity ë„ë©”ì¸ í´ë”ë¥¼ í¬í•¨í•˜ë„ë¡ ìˆ˜ì •
            self.schema_root = VALIDATOR_SCHEMA_WAREHOUSE / domain_group
            self.check_root = VALIDATOR_CHECKS_WAREHOUSE / domain_group
            self.data_root = DATA_WAREHOUSE_ROOT

    # -------------------------------------------------------------------------
    # 2ï¸âƒ£ ê³µí†µ ë°ì´í„° ë¡œë”©
    # -------------------------------------------------------------------------
    def _get_duckdb_connection(self) -> duckdb.DuckDBPyConnection:
        """DuckDB ì—°ê²° ë°˜í™˜"""
        if self.conn is None:
            self.conn = duckdb.connect(database=":memory:")
        return self.conn


    @abstractmethod
    def _load_source_datasets(self) -> dict[str, pd.DataFrame]:
        """âœ… Domainë³„ Loaderë¥¼ ì§ì ‘ í˜¸ì¶œí•˜ëŠ” ëª…ì‹œì  ë²„ì „"""
        pass

    # -------------------------------------------------------------------------
    # 3ï¸âƒ£ ìŠ¤í‚¤ë§ˆ ë¡œë“œ ë° ì»¬ëŸ¼ ìˆœì„œ ì •ë ¬
    # -------------------------------------------------------------------------
    def _load_schema_definition(self) -> dict:
        """ğŸ“˜ warehouse_schemas í´ë”ì—ì„œ domainë³„ ìŠ¤í‚¤ë§ˆ ì •ì˜ JSONì„ ë¡œë“œ"""
        schema_path = self.schema_root / f"{self.domain}.json"
        if not schema_path.exists():
            self.log.warning(f"âš ï¸ Schema file not found for {self.domain}")
            return {}
        else:
            self.log.info(f"ğŸ†—ï¸ Schema file exists! {schema_path}")
        with open(schema_path, "r", encoding="utf-8") as f:
            return json.load(f)

    def _get_preferred_columns(self) -> list[str]:
        schema = self._load_schema_definition()
        return [c["name"] for c in schema.get("columns", [])]

    def _reorder_columns(self, df: pd.DataFrame) -> pd.DataFrame:
        """ì»¬ëŸ¼ ìˆœì„œ ì¬ì •ë ¬"""
        preferred = self._get_preferred_columns()
        if not preferred:
            return df
        for col in preferred:
            if col not in df.columns:
                df[col] = None

        return df[[c for c in preferred if c in df.columns]]

    # -------------------------------------------------------------------------
    # 4ï¸âƒ£ ì €ì¥ ë° ë©”íƒ€ë°ì´í„° ê´€ë¦¬
    # -------------------------------------------------------------------------
    def save_parquet(self, df: pd.DataFrame) -> Dict[str, Any]:
        """
        Parquet íŒŒì¼ ì €ì¥
        - object ì»¬ëŸ¼ì€ ë¬¸ìì—´ë¡œ ë³€í™˜ (PyArrow ArrowTypeError ë°©ì§€)
        """
        try:
            # ğŸ”¹ ëª¨ë“  object íƒ€ì… ì»¬ëŸ¼ì„ ë¬¸ìì—´ë¡œ ë³€í™˜
            for col in df.select_dtypes(include=["object"]).columns:
                df[col] = df[col].astype(str)

            string_cols = df.select_dtypes(include=["object", "string"]).columns
            if len(string_cols) > 0:
                df[string_cols] = df[string_cols].replace(
                    ["None", "none", "NULL", "null", "NaN", "nan"], pd.NA
                )

            table = pa.Table.from_pandas(df)

            # âœ… êµ­ê°€ íŒŒí‹°ì…˜ ë””ë ‰í† ë¦¬ ì¶”ê°€
            if self.country_code:
                output_dir = self.output_file.parent / f"country_code={self.country_code}"
                output_dir.mkdir(parents=True, exist_ok=True)
                self.output_file = output_dir / self.output_file.name  # ì˜ˆ: /asset_master/snapshot_dt=2025-11-05/country_code=US/asset_master.parquet

            pq.write_table(table, self.output_file.as_posix())

            file_size = self.output_file.stat().st_size
            self.log.info(
                f"âœ… Parquet saved: {self.output_file} "
                f"({len(df):,} rows, {file_size:,} bytes)")

            return {
                "file_path": self.output_file.as_posix(),
                "row_count": len(df),
                "file_size_bytes": file_size,
            }

        except Exception as e:
            self.log.error(f"âŒ Failed to save parquet: {e}")
            raise

    def _setup_output_paths(self):
        """
        âœ… ì¶œë ¥ ë””ë ‰í† ë¦¬ ë° íŒŒì¼ ê²½ë¡œ ì„¤ì •
        - snapshot ë ˆì´ì–´ì— ìš°ì„  ì €ì¥
        - validatedëŠ” validatorê°€ promote ì‹œ ìƒì„±
        """
        if self.domain == "fundamentals":
            # í•˜ìœ„ pipelineì—ì„œ ì§ì ‘ ì •ì˜í•˜ë¯€ë¡œ skip
            return

        snapshot_root = (
                DATA_WAREHOUSE_ROOT
                / "snapshot"
                / self.domain_group
                / self.domain
                / f"trd_dt={self.trd_dt}"
        )
        snapshot_root.mkdir(parents=True, exist_ok=True)

        self.output_dir = snapshot_root
        self.output_file = snapshot_root / f"{self.domain}.parquet"
        self.meta_file = snapshot_root / "_build_meta.json"

        # ğŸ“˜ ë„ë©”ì¸ ì „ì—­ ë©”íƒ€íŒŒì¼ ê²½ë¡œ (ì˜ˆ: /data_warehouse/exchange/_warehouse_meta.json)
        self.domain_meta_file = DATA_WAREHOUSE_ROOT / self.domain / "_warehouse_meta.json"

    def save_metadata(self, row_count: int, **kwargs) -> Dict[str, Any]:
        """
        ë©”íƒ€ë°ì´í„° íŒŒì¼(_metadata.json) ê¸°ë¡
        """
        import json
        meta = {
            "domain": self.domain,
            "snapshot_dt": self.trd_dt,
            "row_count": row_count,
            "timestamp": datetime.utcnow().isoformat(),
            **kwargs,  # âœ… ì¶”ê°€ì ì¸ ëª¨ë“  ì¸ì í¬í•¨
        }

        meta_path = (
                DATA_WAREHOUSE_ROOT
                / self.domain
                / "snapshot"
                / f"trd_dt={self.trd_dt}"
                / "_metadata.json"
        )

        meta_path.parent.mkdir(parents=True, exist_ok=True)
        with open(meta_path, "w") as f:
            json.dump(meta, f, indent=2)

        self.log.info(f"ğŸ§¾ Metadata saved â†’ {meta_path.as_posix()}")
        return meta

    # -------------------------------------------------------------------------
    # 5ï¸âƒ£ ì¶”ìƒ ë©”ì„œë“œ (í•˜ìœ„ í´ë˜ìŠ¤ êµ¬í˜„ë¶€)
    # -------------------------------------------------------------------------    @abstractmethod
    def _normalize_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:
        """ë„ë©”ì¸ë³„ ì •ê·œí™” ë¡œì§ (í•˜ìœ„ í´ë˜ìŠ¤ êµ¬í˜„)"""
        pass

    @abstractmethod
    def _transform_business_logic(self, **kwargs) -> pd.DataFrame:
        """
        ë„ë©”ì¸ë³„ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ êµ¬í˜„ë¶€
        ì˜ˆì‹œ:
          - exchange: ê±°ë˜ì†Œ + íœ´ì¥ì¼ ë³‘í•©
        """
        pass

    def _update_domain_metadata(self, record_count: int):
        """
        âœ… ë„ë©”ì¸ ë£¨íŠ¸ì— `_warehouse_meta.json` ìƒì„±/ê°±ì‹ 
        - ìµœì‹  ìŠ¤ëƒ…ìƒ· ì¼ì, ìµœì´ˆ ì ì¬ì¼ì, ì´ ì ì¬ íšŸìˆ˜ ë“±ì„ ê´€ë¦¬
        """
        meta_path = DATA_WAREHOUSE_ROOT / self.domain / "_warehouse_meta.json"
        now = datetime.now(timezone.utc).isoformat()

        new_meta = {
            "domain": self.domain,
            "latest_snapshot": self.trd_dt,
            "record_count": record_count,
            "last_build_meta": str(self.meta_file),
            "updated_at": now,
        }

        # ì´ì „ ë©”íƒ€ ìœ ì§€ (ìµœì´ˆ ì ì¬ì¼, ëˆ„ì  ì¹´ìš´íŠ¸)
        if meta_path.exists():
            try:
                with open(meta_path, "r", encoding="utf-8") as f:
                    old = json.load(f)
                new_meta["first_ingested"] = old.get("first_ingested", self.trd_dt)
                new_meta["total_snapshots"] = old.get("total_snapshots", 0) + 1
            except Exception:
                new_meta["first_ingested"] = self.trd_dt
                new_meta["total_snapshots"] = 1
        else:
            new_meta["first_ingested"] = self.trd_dt
            new_meta["total_snapshots"] = 1

        with open(meta_path, "w", encoding="utf-8") as f:
            json.dump(new_meta, f, indent=2, ensure_ascii=False)

        self.log.info(f"ğŸ“˜ Domain meta updated: {meta_path}")



    @abstractmethod
    def build(self, **kwargs) -> Dict[str, Any]:
        """ë©”ì¸ ë¹Œë“œ í”„ë¡œì„¸ìŠ¤ (í•˜ìœ„ í´ë˜ìŠ¤ êµ¬í˜„)"""
        pass

    # -------------------------------------------------------------------------
    # 6ï¸âƒ£ ë¦¬ì†ŒìŠ¤ ì •ë¦¬
    # -------------------------------------------------------------------------
    def cleanup(self):
        """DuckDB ì—°ê²° ì¢…ë£Œ"""
        if self.conn:
            self.conn.close()
            self.conn = None

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.cleanup()