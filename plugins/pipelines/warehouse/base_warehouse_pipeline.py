# plugins/pipelines/warehouse/base_warehouse_pipeline.py
import json
import logging
import duckdb
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
from abc import ABC, abstractmethod
from pathlib import Path
from datetime import datetime, timezone
from typing import List, Dict, Optional, Any, Tuple

from plugins.config.constants import (
    DATA_LAKE_ROOT, DATA_WAREHOUSE_ROOT,
    LAYERS, DATA_DOMAINS, VENDORS
)
from plugins.utils.partition_finder import latest_trd_dt_path, parquet_file_in
from plugins.utils.snapshot_registry import update_global_snapshot_registry


class BaseWarehousePipeline(ABC):
    """
    âœ… Warehouse ê³µí†µ íŒŒì´í”„ë¼ì¸ ë² ì´ìŠ¤ í´ë˜ìŠ¤

    ì£¼ìš” ê¸°ëŠ¥:
    1. Data Lake validated ë ˆì´ì–´ì—ì„œ ë°ì´í„° ë¡œë”© (vendor ìš°ì„ ìˆœìœ„ ì§€ì›)
    2. ë°ì´í„° ì •ê·œí™” ë° ë³€í™˜
    3. Entity ID ìƒì„± (optional)
    4. Parquet ì €ì¥ (snapshot ê¸°ë°˜)
    5. ë©”íƒ€ë°ì´í„° ê´€ë¦¬ ë° ì „ì—­ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ê°±ì‹ 

    í•˜ìœ„ í´ë˜ìŠ¤ê°€ êµ¬í˜„í•´ì•¼ í•  ë©”ì„œë“œ:
    - _normalize_dataframe(): ë„ë©”ì¸ë³„ ì •ê·œí™” ë¡œì§
    - _generate_entity_ids(): Entity ID ìƒì„± ë¡œì§ (optional)
    - _get_preferred_columns(): ì¶œë ¥ ì»¬ëŸ¼ ìˆœì„œ ì •ì˜
    """

    def __init__(
            self,
            domain: str,
            snapshot_dt: str,
            partition_key: str = "country_code",
            partition_value: Optional[str] = None,
            vendor_priority: Optional[List[str]] = None,
            layer: str = "warehouse"
    ):
        """
        Args:
            domain: Warehouse ë„ë©”ì¸ëª… (e.g. 'asset_master', 'exchange_master')
            snapshot_dt: ìŠ¤ëƒ…ìƒ· ë‚ ì§œ (YYYY-MM-DD)
            partition_key: íŒŒí‹°ì…”ë‹ í‚¤ (default: country_code)
            partition_value: íŒŒí‹°ì…”ë‹ ê°’ (e.g. 'KOR', 'USA')
            vendor_priority: ë²¤ë” ìš°ì„ ìˆœìœ„ ë¦¬ìŠ¤íŠ¸
        """
        self.domain = domain
        self.snapshot_dt = snapshot_dt
        self.partition_key = partition_key
        self.partition_value = partition_value.upper() if partition_value else None
        self.vendor_priority = vendor_priority or [VENDORS.get("EODHD", "eodhd")]
        self.layer = layer

        self.log = logging.getLogger(f"{__name__}.{self.__class__.__name__}")

        # ì¶œë ¥ ê²½ë¡œ ì„¤ì •
        self._setup_output_paths()

        # DuckDB ì—°ê²°
        self.conn = None

    def _setup_output_paths(self):
        """ì¶œë ¥ ë””ë ‰í† ë¦¬ ë° íŒŒì¼ ê²½ë¡œ ì„¤ì •"""
        if self.partition_value:
            self.output_dir = (
                    DATA_WAREHOUSE_ROOT
                    / self.domain
                    / f"{self.partition_key}={self.partition_value}"
                    / f"snapshot_dt={self.snapshot_dt}"
            )
        else:
            self.output_dir = (
                    DATA_WAREHOUSE_ROOT
                    / self.domain
                    / f"snapshot_dt={self.snapshot_dt}"
            )

        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.output_file = self.output_dir / f"{self.domain}.parquet"
        self.meta_file = self.output_dir / "_build_meta.json"



    def _get_validated_files(self, lake_domain: str, filename: str = None) -> List[Path]:
        """
        Data Lake validated/<lake_domain> ê²½ë¡œì—ì„œ parquet íŒŒì¼ ëª©ë¡ì„ ì•ˆì „í•˜ê²Œ ì¡°íšŒ

        Args:
            lake_domain: validated ë ˆì´ì–´ ë‚´ì˜ ë„ë©”ì¸ëª… (ì˜ˆ: 'exchange_list')
            filename: íŠ¹ì • íŒŒì¼ëª… (ê¸°ë³¸ê°’: <lake_domain>.parquet)

        Returns:
            parquet íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
        """
        layer_root = DATA_LAKE_ROOT / "validated" / lake_domain
        layer_root = layer_root.resolve()

        if not layer_root.exists():
            raise FileNotFoundError(f"âŒ validated path not found: {layer_root}")

        parquet_name = filename or f"{lake_domain}.parquet"
        parquet_files = list(layer_root.glob(f"**/{parquet_name}"))

        if not parquet_files:
            raise FileNotFoundError(f"âŒ No parquet files found under {layer_root}")

        self.log.info(f"ğŸ“‚ Found {len(parquet_files)} {lake_domain} files in {layer_root}")
        return parquet_files

    # ==============================
    # Data Lake ë°ì´í„° ë¡œë”©
    # ==============================

    def _get_duckdb_connection(self) -> duckdb.DuckDBPyConnection:
        """DuckDB ì—°ê²° ë°˜í™˜ (ì‹±ê¸€í†¤ íŒ¨í„´)"""
        if self.conn is None:
            self.conn = duckdb.connect(database=":memory:")
        return self.conn

    def _choose_best_partition(
            self,
            lake_domain: str,
            exchange_code: str,
            layer: str = "validated"
    ) -> Optional[Path]:
        """
        ë²¤ë” ìš°ì„ ìˆœìœ„ì— ë”°ë¼ ìµœì‹  validated íŒŒí‹°ì…˜ ì„ íƒ

        Args:
            lake_domain: Data Lake ë„ë©”ì¸ (e.g. 'symbol_list', 'fundamentals')
            exchange_code: ê±°ë˜ì†Œ ì½”ë“œ
            layer: ë ˆì´ì–´ëª… (default: 'validated')

        Returns:
            ì„ íƒëœ íŒŒí‹°ì…˜ ë””ë ‰í† ë¦¬ ê²½ë¡œ ë˜ëŠ” None
        """
        for vendor in self.vendor_priority:
            partition_dir = latest_trd_dt_path(
                domain=lake_domain,
                exchange_code=exchange_code,
                vendor=vendor,
                layer=LAYERS.get(layer.upper(), layer)
            )
            if partition_dir:
                self.log.info(f"âœ… Selected partition: {partition_dir} (vendor={vendor})")
                return partition_dir

        self.log.warning(f"âš ï¸ No partition found for {lake_domain} / {exchange_code}")
        return None

    def _load_parquet_from_partition(
            self,
            partition_dir: Path,
            domain: str,
            additional_columns: Optional[Dict[str, Any]] = None
    ) -> Optional[pd.DataFrame]:
        """
        íŒŒí‹°ì…˜ ë””ë ‰í† ë¦¬ì—ì„œ parquet íŒŒì¼ ë¡œë“œ

        Args:
            partition_dir: íŒŒí‹°ì…˜ ë””ë ‰í† ë¦¬ ê²½ë¡œ
            domain: ë„ë©”ì¸ëª… (íŒŒì¼ëª… ê²°ì •ìš©)
            additional_columns: ì¶”ê°€í•  ì»¬ëŸ¼ ë”•ì…”ë„ˆë¦¬

        Returns:
            DataFrame ë˜ëŠ” None
        """
        pq_file = parquet_file_in(partition_dir, domain)

        assert pq_file, f"âš ï¸ Parquet file not found in {partition_dir}"

        try:
            conn = self._get_duckdb_connection()
            df = conn.execute(
                f"SELECT * FROM read_parquet('{pq_file.as_posix()}')"
            ).df()

            # ì¶”ê°€ ì»¬ëŸ¼ ì„¤ì •
            if additional_columns:
                for col_name, col_value in additional_columns.items():
                    df[col_name] = col_value

            self.log.info(f"âœ… Loaded {len(df):,} rows from {pq_file.name}")
            return df

        except Exception as e:
            self.log.error(f"âŒ Failed to load parquet: {pq_file} - {e}")
            return None

    def load_lake_data(
            self,
            lake_domain: str,
            exchange_codes: List[str],
            layer: str = "validated",
            additional_columns: Optional[Dict[str, Any]] = None
    ) -> List[pd.DataFrame]:
        """
        ì—¬ëŸ¬ ê±°ë˜ì†Œì˜ Data Lake ë°ì´í„° ë¡œë“œ

        Args:
            lake_domain: Data Lake ë„ë©”ì¸
            exchange_codes: ê±°ë˜ì†Œ ì½”ë“œ ë¦¬ìŠ¤íŠ¸
            layer: ë ˆì´ì–´ëª…
            additional_columns: ê° ê±°ë˜ì†Œë³„ ì¶”ê°€ ì»¬ëŸ¼

        Returns:
            DataFrame ë¦¬ìŠ¤íŠ¸
        """
        dataframes = []

        for ex_code in exchange_codes:
            partition_dir = self._choose_best_partition(lake_domain, ex_code, layer)
            if not partition_dir:
                continue

            # ê±°ë˜ì†Œë³„ ì¶”ê°€ ì»¬ëŸ¼ ì„¤ì •
            extra_cols = {"exchange_code": ex_code}
            if additional_columns:
                extra_cols.update(additional_columns)

            df = self._load_parquet_from_partition(
                partition_dir,
                lake_domain,
                extra_cols
            )

            if df is not None:
                dataframes.append(df)

        return dataframes

    # ==============================
    # ë°ì´í„° ë³€í™˜ ë° ì •ê·œí™”
    # ==============================

    @abstractmethod
    def _normalize_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        ë„ë©”ì¸ë³„ DataFrame ì •ê·œí™” ë¡œì§

        í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ ë°˜ë“œì‹œ êµ¬í˜„:
        - ì»¬ëŸ¼ëª… í‘œì¤€í™”
        - ë°ì´í„° íƒ€ì… ë³€í™˜
        - í•„ìˆ˜ í•„ë“œ ê²€ì¦
        - ê¸°ë³¸ê°’ ì„¤ì •

        Args:
            df: ì›ë³¸ DataFrame

        Returns:
            ì •ê·œí™”ëœ DataFrame
        """
        pass

    def _generate_entity_ids(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Entity ID ìƒì„± (Optional)

        í•„ìš”í•œ ê²½ìš° í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ ì˜¤ë²„ë¼ì´ë“œ

        Args:
            df: DataFrame

        Returns:
            entity_id ì»¬ëŸ¼ì´ ì¶”ê°€ëœ DataFrame
        """
        return df

    def _get_preferred_columns(self) -> List[str]:
        """
        ì¶œë ¥ ì»¬ëŸ¼ ìˆœì„œ ì •ì˜ (Optional)

        í•„ìš”í•œ ê²½ìš° í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ ì˜¤ë²„ë¼ì´ë“œ

        Returns:
            ì»¬ëŸ¼ëª… ë¦¬ìŠ¤íŠ¸
        """
        return []

    def _reorder_columns(self, df: pd.DataFrame) -> pd.DataFrame:
        """ì»¬ëŸ¼ ìˆœì„œ ì¬ì •ë ¬"""
        preferred = self._get_preferred_columns()
        if not preferred:
            return df

        # ëˆ„ë½ëœ ì»¬ëŸ¼ì€ Noneìœ¼ë¡œ ì¶”ê°€
        for col in preferred:
            if col not in df.columns:
                df[col] = None

        # ê¸°ì¡´ ì»¬ëŸ¼ ì¤‘ preferredì— ì—†ëŠ” ê²ƒ ì¶”ê°€
        remaining = [c for c in df.columns if c not in preferred]
        final_order = preferred + remaining

        return df[final_order]

    # ==============================
    # ë³‘í•© ë° ì €ì¥
    # ==============================

    def merge_dataframes(
            self,
            dataframes: List[pd.DataFrame],
            deduplicate_on: Optional[List[str]] = None
    ) -> pd.DataFrame:
        """
        ì—¬ëŸ¬ DataFrame ë³‘í•© ë° ì¤‘ë³µ ì œê±°

        Args:
            dataframes: DataFrame ë¦¬ìŠ¤íŠ¸
            deduplicate_on: ì¤‘ë³µ ì œê±° ê¸°ì¤€ ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸

        Returns:
            ë³‘í•©ëœ DataFrame
        """
        if not dataframes:
            raise ValueError("âŒ No dataframes to merge")

        merged = pd.concat(dataframes, ignore_index=True)
        self.log.info(f"ğŸ“Š Merged {len(dataframes)} dataframes: {len(merged):,} total rows")

        if deduplicate_on:
            before = len(merged)
            merged = merged.drop_duplicates(subset=deduplicate_on, keep="first")
            after = len(merged)
            if before != after:
                self.log.info(f"ğŸ”„ Deduplicated: {before:,} â†’ {after:,} rows ({before - after:,} removed)")

        return merged.reset_index(drop=True)

    def save_parquet(self, df: pd.DataFrame) -> Dict[str, Any]:
        """
        Parquet íŒŒì¼ ì €ì¥

        Args:
            df: ì €ì¥í•  DataFrame

        Returns:
            ì €ì¥ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        try:
            table = pa.Table.from_pandas(df)
            pq.write_table(table, self.output_file.as_posix())

            file_size = self.output_file.stat().st_size
            self.log.info(
                f"âœ… Parquet saved: {self.output_file} "
                f"({len(df):,} rows, {file_size:,} bytes)"
            )

            return {
                "file_path": self.output_file.as_posix(),
                "row_count": len(df),
                "file_size_bytes": file_size
            }

        except Exception as e:
            self.log.error(f"âŒ Failed to save parquet: {e}")
            raise

    # ==============================
    # ë©”íƒ€ë°ì´í„° ê´€ë¦¬
    # ==============================

    def save_metadata(
            self,
            row_count: int,
            source_info: Optional[Dict[str, Any]] = None,
            metrics: Optional[Dict[str, Any]] = None,
            context: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Warehouse ë‹¨ê³„ ë©”íƒ€ë°ì´í„° ì €ì¥

        Args:
            row_count: ìµœì¢… Parquet ë ˆì½”ë“œ ìˆ˜
            source_info: validated ë°ì´í„°ì…‹ë³„ lineage ì •ë³´ (dict)
            metrics: ë„ë©”ì¸ë³„ í†µê³„ ì •ë³´
            context: Airflow context (dag_run_id, task_id ë“±)
        Returns:
            meta dict (Warehouse í‘œì¤€ í¬ë§·)
        """
        from datetime import datetime, timezone

        meta = {
            "dataset": self.domain,
            "snapshot_dt": self.snapshot_dt,
            "status": "success",
            "record_count": row_count,
            "output_file": self.output_file.as_posix(),
            "sources": source_info or {},
            "metrics": metrics or {},
            "created_at": datetime.now(timezone.utc).isoformat(),
        }

        # DAG/Task context ì¶”ê°€
        if context:
            meta["dag_run_id"] = context.get("run_id")
            ti = context.get("task_instance")
            if ti:
                meta["task_id"] = ti.task_id

        # íŒŒí‹°ì…˜ ì •ë³´
        if self.partition_value:
            meta[self.partition_key] = self.partition_value

        # ë©”íƒ€ ì €ì¥
        with open(self.meta_file, "w", encoding="utf-8") as f:
            json.dump(meta, f, indent=2, ensure_ascii=False)

        self.log.info(f"ğŸ“˜ Metadata saved: {self.meta_file}")
        return meta

    def update_registry(self, additional_key: Optional[str] = None):
        """
        ì „ì—­ ìŠ¤ëƒ…ìƒ· ë ˆì§€ìŠ¤íŠ¸ë¦¬ ê°±ì‹ 

        Args:
            additional_key: ì¶”ê°€ í‚¤ (e.g. country_code)
        """
        kwargs = {
            "domain": self.domain,
            "snapshot_dt": self.snapshot_dt,
            "meta_file": self.meta_file,
        }

        if additional_key and self.partition_value:
            kwargs[additional_key] = self.partition_value

        update_global_snapshot_registry(**kwargs)
        self.log.info(f"ğŸ“ Registry updated: {self.domain} / {self.snapshot_dt}")

    # ==============================
    # ë©”ì¸ ë¹Œë“œ í”„ë¡œì„¸ìŠ¤
    # ==============================

    @abstractmethod
    def build(self) -> Dict[str, Any]:
        """
        Warehouse ë¹Œë“œ ë©”ì¸ ë¡œì§

        í‘œì¤€ í”„ë¡œì„¸ìŠ¤:
        1. Data Lakeì—ì„œ ë°ì´í„° ë¡œë“œ
        2. ì •ê·œí™” ë° ë³€í™˜
        3. Entity ID ìƒì„± (optional)
        4. ë³‘í•© ë° ì¤‘ë³µ ì œê±°
        5. ì»¬ëŸ¼ ìˆœì„œ ì •ë ¬
        6. Parquet ì €ì¥
        7. ë©”íƒ€ë°ì´í„° ì €ì¥
        8. ë ˆì§€ìŠ¤íŠ¸ë¦¬ ê°±ì‹ 

        Returns:
            ë¹Œë“œ ê²°ê³¼ ë©”íƒ€ë°ì´í„°
        """
        pass

    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if self.conn:
            self.conn.close()
            self.conn = None

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.cleanup()