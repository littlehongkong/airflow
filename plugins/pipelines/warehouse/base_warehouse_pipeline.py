import json
import logging
import duckdb
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
from abc import ABC, abstractmethod
from pathlib import Path
from typing import List, Dict, Optional, Any
from datetime import datetime, timezone

from plugins.config.constants import (
    DATA_LAKE_ROOT, DATA_WAREHOUSE_ROOT,
    VENDORS, WAREHOUSE_SOURCE_MAP, SCHEMA_DIR
)
from plugins.utils.snapshot_registry import update_global_snapshot_registry


class BaseWarehousePipeline(ABC):
    """
    âœ… Warehouse ê³µí†µ íŒŒì´í”„ë¼ì¸ ë² ì´ìŠ¤ í´ë˜ìŠ¤

    ì£¼ìš” ê¸°ëŠ¥:
    - Data Lake validated â†’ Warehouse í†µí•© ë³€í™˜ íŒŒì´í”„ë¼ì¸ì˜ í‘œì¤€ ì¸í„°í˜ì´ìŠ¤
    - ë°ì´í„° ë¡œë”©, íŒŒì¼€ ì €ì¥, ë©”íƒ€ë°ì´í„° ê¸°ë¡, ìŠ¤ëƒ…ìƒ· ë ˆì§€ìŠ¤íŠ¸ë¦¬ ê°±ì‹  ë‹´ë‹¹
    - ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ì€ í•˜ìœ„ í´ë˜ìŠ¤ì˜ `_transform_business_logic()`ì—ì„œ ìˆ˜í–‰
    """

    # -------------------------------------------------------------------------
    # 1ï¸âƒ£ ì´ˆê¸°í™” ë° ê²½ë¡œ ì„¤ì •
    # -------------------------------------------------------------------------
    def __init__(
        self,
        domain: str,
        snapshot_dt: str,
        vendor_priority: Optional[List[str]] = None,
    ):
        self.domain = domain
        self.snapshot_dt = snapshot_dt
        self.vendor_priority = vendor_priority or [VENDORS.get("EODHD", "eodhd")]
        self.log = logging.getLogger(f"{__name__}.{self.__class__.__name__}")

        self.output_dir = DATA_WAREHOUSE_ROOT / self.domain / f"snapshot_dt={self.snapshot_dt}"
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.output_file = self.output_dir / f"{self.domain}.parquet"
        self.meta_file = self.output_dir / "_build_meta.json"

        self.conn = None  # duckdb ì—°ê²° ê°ì²´

    # -------------------------------------------------------------------------
    # 2ï¸âƒ£ ê³µí†µ ë°ì´í„° ë¡œë”©
    # -------------------------------------------------------------------------
    def _get_duckdb_connection(self) -> duckdb.DuckDBPyConnection:
        """DuckDB ì—°ê²° ë°˜í™˜"""
        if self.conn is None:
            self.conn = duckdb.connect(database=":memory:")
        return self.conn

    def _load_source_datasets(self, warehouse_domain: str) -> Dict[str, pd.DataFrame]:
        """
        âœ… ê³µí†µ Data Lake â†’ Warehouse ë°ì´í„° ë¡œë”

        constants.WAREHOUSE_SOURCE_MAP ê¸°ë°˜ìœ¼ë¡œ validated í´ë”ì—ì„œ parquet ìë™ ë¡œë“œ
        """
        source_map = WAREHOUSE_SOURCE_MAP.get(warehouse_domain)
        if not source_map:
            raise ValueError(f"âŒ No source mapping defined for {warehouse_domain}")

        conn = self._get_duckdb_connection()
        results = {}

        for lake_domain in source_map:
            domain_dir = DATA_LAKE_ROOT / "validated" / lake_domain
            parquet_files = list(domain_dir.rglob("*.parquet"))

            if not parquet_files:
                self.log.warning(f"âš ï¸ No parquet files found for {lake_domain}")
                continue

            files_expr = ", ".join([f"'{p.as_posix()}'" for p in parquet_files])
            df = conn.execute(f"SELECT * FROM read_parquet([{files_expr}])").df()

            self.log.info(f"ğŸ“Š Loaded {len(df):,} records from {lake_domain}")
            results[lake_domain] = df

        return results

    # -------------------------------------------------------------------------
    # 3ï¸âƒ£ ìŠ¤í‚¤ë§ˆ ë¡œë“œ ë° ì»¬ëŸ¼ ìˆœì„œ ì •ë ¬
    # -------------------------------------------------------------------------
    def _load_schema_definition(self) -> dict:
        """ğŸ“˜ warehouse_schemas í´ë”ì—ì„œ domainë³„ ìŠ¤í‚¤ë§ˆ ì •ì˜ JSONì„ ë¡œë“œ"""
        schema_path = SCHEMA_DIR / f"{self.domain}.json"
        if not schema_path.exists():
            self.log.warning(f"âš ï¸ Schema file not found for {self.domain}")
            return {}
        with open(schema_path, "r", encoding="utf-8") as f:
            return json.load(f)

    def _get_preferred_columns(self) -> list[str]:
        schema = self._load_schema_definition()
        return [c["name"] for c in schema.get("columns", [])]

    def _reorder_columns(self, df: pd.DataFrame) -> pd.DataFrame:
        """ì»¬ëŸ¼ ìˆœì„œ ì¬ì •ë ¬"""
        preferred = self._get_preferred_columns()
        if not preferred:
            return df
        for col in preferred:
            if col not in df.columns:
                df[col] = None
        return df[preferred + [c for c in df.columns if c not in preferred]]

    # -------------------------------------------------------------------------
    # 4ï¸âƒ£ ì €ì¥ ë° ë©”íƒ€ë°ì´í„° ê´€ë¦¬
    # -------------------------------------------------------------------------
    def save_parquet(self, df: pd.DataFrame) -> Dict[str, Any]:
        """Parquet ì €ì¥"""
        table = pa.Table.from_pandas(df)
        pq.write_table(table, self.output_file.as_posix())
        size = self.output_file.stat().st_size
        self.log.info(f"âœ… Parquet saved: {self.output_file} ({len(df):,} rows, {size:,} bytes)")
        return {"file_path": str(self.output_file), "row_count": len(df), "file_size": size}

    def save_metadata(
        self,
        row_count: int,
        source_info: Optional[Dict[str, Any]] = None,
        metrics: Optional[Dict[str, Any]] = None,
        context: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        """Warehouse ë‹¨ê³„ ë©”íƒ€ë°ì´í„° ì €ì¥"""
        meta = {
            "dataset": self.domain,
            "snapshot_dt": self.snapshot_dt,
            "status": "success",
            "record_count": row_count,
            "output_file": str(self.output_file),
            "sources": source_info or {},
            "metrics": metrics or {},
            "created_at": datetime.now(timezone.utc).isoformat(),
        }
        if context:
            ti = context.get("task_instance")
            if ti:
                meta["dag_run_id"] = context.get("run_id")
                meta["task_id"] = ti.task_id

        with open(self.meta_file, "w", encoding="utf-8") as f:
            json.dump(meta, f, indent=2, ensure_ascii=False)
        self.log.info(f"ğŸ“˜ Metadata saved: {self.meta_file}")
        return meta

    def update_registry(self):
        """ì „ì—­ ìŠ¤ëƒ…ìƒ· ë ˆì§€ìŠ¤íŠ¸ë¦¬ ê°±ì‹ """
        update_global_snapshot_registry(
            domain=self.domain,
            snapshot_dt=self.snapshot_dt,
            meta_file=self.meta_file
        )
        self.log.info(f"ğŸ“ Registry updated: {self.domain} / {self.snapshot_dt}")

    # -------------------------------------------------------------------------
    # 5ï¸âƒ£ ì¶”ìƒ ë©”ì„œë“œ (í•˜ìœ„ í´ë˜ìŠ¤ êµ¬í˜„ë¶€)
    # -------------------------------------------------------------------------
    @abstractmethod
    def _normalize_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:
        """ë„ë©”ì¸ë³„ ì •ê·œí™” ë¡œì§ (í•˜ìœ„ í´ë˜ìŠ¤ êµ¬í˜„)"""
        pass

    @abstractmethod
    def _transform_business_logic(self, **kwargs) -> pd.DataFrame:
        """
        ë„ë©”ì¸ë³„ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ êµ¬í˜„ë¶€
        ì˜ˆì‹œ:
          - exchange: ê±°ë˜ì†Œ + íœ´ì¥ì¼ ë³‘í•©
          - asset: symbol_list + symbol_changes ë³‘í•©
        """
        pass

    @abstractmethod
    def build(self, **kwargs) -> Dict[str, Any]:
        """ë©”ì¸ ë¹Œë“œ í”„ë¡œì„¸ìŠ¤ (í•˜ìœ„ í´ë˜ìŠ¤ êµ¬í˜„)"""
        pass

    # -------------------------------------------------------------------------
    # 6ï¸âƒ£ ë¦¬ì†ŒìŠ¤ ì •ë¦¬
    # -------------------------------------------------------------------------
    def cleanup(self):
        """DuckDB ì—°ê²° ì¢…ë£Œ"""
        if self.conn:
            self.conn.close()
            self.conn = None

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.cleanup()
