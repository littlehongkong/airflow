# plugins/pipelines/warehouse/exchange_master_pipeline.py
import pandas as pd
from typing import Dict, Any, List
from plugins.pipelines.warehouse.base_warehouse_pipeline import BaseWarehousePipeline
from datetime import datetime, timezone

class ExchangeMasterPipeline(BaseWarehousePipeline):
    """
    âœ… ê±°ë˜ì†Œ ë§ˆìŠ¤í„° íŒŒì´í”„ë¼ì¸

    ì…ë ¥: Data Lake validated/exchange_list
    ì¶œë ¥: warehouse/exchange_master/snapshot_dt=YYYY-MM-DD/exchange_master.parquet

    íŠ¹ì§•:
    - ì „ì—­ ê±°ë˜ì†Œ ëª©ë¡ êµ¬ì¶• (íŒŒí‹°ì…˜ ì—†ìŒ)
    - êµ­ê°€ë³„ ê±°ë˜ì†Œ ë§¤í•‘ ìƒì„±
    """

    def __init__(self, snapshot_dt: str, vendor_priority: List[str] = None):
        from plugins.config.constants import WAREHOUSE_DOMAIN

        super().__init__(
            domain=WAREHOUSE_DOMAIN.get("EXCHANGE_MASTER"),
            snapshot_dt=snapshot_dt,
            vendor_priority=vendor_priority
        )

    def _normalize_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        ê±°ë˜ì†Œ ë°ì´í„° ì •ê·œí™”

        í•„ìˆ˜ ì»¬ëŸ¼:
        - country_code (êµ­ê°€ ì½”ë“œ)
        - exchange_code (ê±°ë˜ì†Œ ì½”ë“œ)
        - exchange_name (ê±°ë˜ì†Œëª…)
        """
        df = df.copy()
        df.columns = df.columns.str.lower()

        # í•„ìˆ˜ ì»¬ëŸ¼ ë§¤í•‘
        code_col = next(
            (c for c in ("code", "exchange_code", "mic", "operatingmic")
             if c in df.columns),
            None
        )

        if not code_col:
            raise ValueError("âŒ ê±°ë˜ì†Œ ë°ì´í„°ì— code/exchange_code ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")

        # í‘œì¤€ ì»¬ëŸ¼ ìƒì„±
        normalized = pd.DataFrame({
            "country_code": df.get("countryiso3", df.get("country_code", "")),
            "country_name": df.get("country", df.get("country_name", "")),
            "exchange_code": df[code_col],
            "exchange_name": df.get("name", df.get("exchange_name", "")),
            "currency": df.get("currency", ""),
            "country_iso2": df.get("countryiso2", ""),
            "timezone": df.get("timezone", ""),
            "operating_mic": df.get("operatingmic", df.get(code_col, "")),
        })

        # ë°ì´í„° ì •ì œ
        normalized["country_code"] = (
            normalized["country_code"]
            .astype(str).str.strip().str.upper()
        )
        normalized["exchange_code"] = (
            normalized["exchange_code"]
            .astype(str).str.strip().str.upper()
        )
        normalized["exchange_name"] = (
            normalized["exchange_name"]
            .astype(str).str.strip()
        )

        # í•„ìˆ˜ê°’ í•„í„°ë§
        normalized = normalized[
            (normalized["country_code"] != "") &
            (normalized["exchange_code"] != "")
            ]

        return normalized

    def _get_preferred_columns(self) -> List[str]:
        """ì¶œë ¥ ì»¬ëŸ¼ ìˆœì„œ"""
        return [
            "country_code",
            "country_name",
            "exchange_code",
            "exchange_name",
            "currency",
            "country_iso2",
            "timezone",
            "operating_mic",
        ]

    def _build_country_exchange_map(self, df: pd.DataFrame) -> Dict[str, List[str]]:
        """
        êµ­ê°€ë³„ ê±°ë˜ì†Œ ì½”ë“œ ë§¤í•‘ ìƒì„±

        Returns:
            {"KOR": ["KS", "KQ"], "USA": ["US", "NASDAQ"], ...}
        """
        mapping = (
            df.groupby("country_code")["exchange_code"]
            .apply(lambda s: sorted(set(s.dropna().astype(str))))
            .to_dict()
        )
        return mapping

    # âœ… ìµœì¢… build() êµ¬ì¡° (ë¶ˆí•„ìš”í•œ ìœ íš¨ì„± ê²€ì¦ ì½”ë“œ ì œê±° í›„)
    def build(self, **kwargs) -> Dict[str, Any]:
        """
        ê±°ë˜ì†Œ ë§ˆìŠ¤í„° ë¹Œë“œ

        í”„ë¡œì„¸ìŠ¤:
        1. validated/exchange_listì˜ ëª¨ë“  íŒŒí‹°ì…˜ ë¡œë“œ
        2. ì •ê·œí™” ë° ë³‘í•©
        3. ì¤‘ë³µ ì œê±°
        4. êµ­ê°€ë³„ ê±°ë˜ì†Œ ë§¤í•‘ ìƒì„±
        5. Parquet ì €ì¥
        6. ë©”íƒ€ë°ì´í„° ì €ì¥
        7. ë ˆì§€ìŠ¤íŠ¸ë¦¬ ê°±ì‹ 
        """
        self.log.info(f"ğŸ—ï¸ Building exchange_master for snapshot_dt={self.snapshot_dt}")

        parquet_files = self._get_validated_files(self.domain)
        self.log.info(f"ğŸ“‚ Found {len(parquet_files)} exchange_list files")

        # 2ï¸âƒ£ ëª¨ë“  parquet íŒŒì¼ ë³‘í•©
        conn = self._get_duckdb_connection()
        files_expr = ", ".join([f"'{str(p)}'" for p in parquet_files])
        raw_df = conn.execute(f"SELECT * FROM read_parquet([{files_expr}])").df()
        self.log.info(f"ğŸ“Š Loaded {len(raw_df):,} raw records")

        # 3ï¸âƒ£ ì •ê·œí™”
        normalized = self._normalize_dataframe(raw_df)

        # 4ï¸âƒ£ ì¤‘ë³µ ì œê±°
        deduplicated = normalized.drop_duplicates(subset=["exchange_code"], keep="first").reset_index(drop=True)
        self.log.info(f"ğŸ”„ Deduplicated: {len(normalized):,} â†’ {len(deduplicated):,} rows")

        # 5ï¸âƒ£ ì»¬ëŸ¼ ìˆœì„œ ì •ë ¬
        final_df = self._reorder_columns(deduplicated)

        # 6ï¸âƒ£ êµ­ê°€ë³„ ê±°ë˜ì†Œ ë§¤í•‘ ìƒì„±
        country_map = self._build_country_exchange_map(final_df)
        self.log.info(f"ğŸ—ºï¸ Country-Exchange mapping: {len(country_map)} countries")

        # 7ï¸âƒ£ Parquet ì €ì¥
        save_result = self.save_parquet(final_df)

        # 8ï¸âƒ£ ë©”íƒ€ë°ì´í„° ì €ì¥
        meta = self.save_metadata(
            row_count=len(final_df),
            source_info={
                "datasets": [self.domain],
                "validated_files": [str(p) for p in parquet_files],
                "record_counts": {"exchange_list": len(final_df)},
                "last_validated_timestamps": {
                    "exchange_list": datetime.now(timezone.utc).isoformat()
                },
            },
            metrics={"country_count": len(country_map), "vendor_priority": ["eodhd"]},
            context=kwargs.get("context"),
        )

        # 9ï¸âƒ£ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ê°±ì‹ 
        self.update_registry()

        self.log.info(
            f"âœ… [BUILD COMPLETE] exchange_master | "
            f"{len(final_df):,} exchanges | {len(country_map)} countries"
        )

        return meta
