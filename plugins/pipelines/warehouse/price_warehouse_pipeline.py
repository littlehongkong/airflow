import pandas as pd
from typing import Dict, Any
from pathlib import Path

from plugins.pipelines.warehouse.base_warehouse_pipeline import BaseWarehousePipeline
from plugins.utils.loaders.equity.price_loader import load_prices
from plugins.utils.loaders.lake.exchange_loader import load_exchange_list
from plugins.utils.loaders.warehouse.asset_master_loader import load_asset_master_latest
from plugins.utils.id_generator import generate_or_reuse_entity_id


class PriceWarehousePipeline(BaseWarehousePipeline):
    """
    ğŸ“˜ Equity Prices Warehouse Pipeline
    ----------------------------------------------------
    âœ… ì—­í• :
      - Data Lake (validated/prices) â†’ Warehouse (snapshot/prices)
      - ì¢…ëª©ë³„ ì¼ì ë‹¨ìœ„ ì‹œì„¸ ë°ì´í„° ì ì¬ (OHLC ì •ê·œí™”)
      - security_id ë¶€ì—¬ ë° ìŠ¤í‚¤ë§ˆ í‘œì¤€í™”
    """

    def __init__(self, trd_dt: str, vendor: str = "eodhd", country_code: str = None):
        super().__init__(
            domain="prices",
            domain_group="equity",
            trd_dt=trd_dt,
            vendor=vendor,
            country_code=country_code,
        )

    # -------------------------------------------------------------------------
    # 1ï¸âƒ£ Load Source Datasets
    # -------------------------------------------------------------------------
    def _load_source_datasets(self) -> Dict[str, pd.DataFrame]:
        """âœ… validated layer parquet íŒŒì¼ ë¡œë“œ"""
        exchange_df = load_exchange_list(
            self.domain_group, vendor=self.vendor, trd_dt=self.trd_dt
        )
        exchanges = exchange_df[exchange_df["CountryISO3"] == self.country_code]["Code"].tolist()
        price_df = load_prices(
            domain_group=self.domain_group,
            vendor=self.vendor,
            exchange_codes=exchanges,
            trd_dt=self.trd_dt,
        )

        # ë¡œê·¸
        dup_tickers = price_df[price_df.duplicated(subset=["code"], keep=False)]["code"].unique().tolist()
        print(
            f"ğŸš¨ Duplicate tickers detected: {dup_tickers[:20]}..."
            if dup_tickers else "âœ… No duplicate tickers found across exchanges."
        )

        self.log.info(f"âœ… Loaded {len(price_df):,} rows from {len(exchanges)} exchanges")
        return {"prices": price_df}

    # -------------------------------------------------------------------------
    # 2ï¸âƒ£ Normalize
    # -------------------------------------------------------------------------
    def _normalize_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:
        """âœ… ì»¬ëŸ¼ëª… ì •ê·œí™” ë° ìŠ¤í‚¤ë§ˆ í‘œì¤€í™”"""
        df.columns = [c.strip().lower().replace(".", "_") for c in df.columns]
        df = df.loc[:, ~df.columns.duplicated()]

        rename_map = {
            "code": "ticker",
            "marketcapitalization": "market_cap",
        }
        df = df.rename(columns={k: v for k, v in rename_map.items() if k in df.columns})

        df["ticker"] = df["ticker"].astype(str)

        return df

    # -------------------------------------------------------------------------
    # 3ï¸âƒ£ Business Logic
    # -------------------------------------------------------------------------
    def _transform_business_logic(self, **kwargs) -> pd.DataFrame:
        """âœ… ìŠ¤í‚¤ë§ˆ í•„ë“œ êµ¬ì„± ë° ID ë§¤í•‘"""
        df = kwargs["prices"]
        df = self._normalize_dataframe(df)

        # âœ… security_id ë¶€ì—¬
        df = self._assign_persistent_security_id(df)

        # âœ… OHLC ê¸°ì¤€ ì»¬ëŸ¼ í•„í„°ë§ (ìŠ¤í‚¤ë§ˆ ì¤€ìˆ˜)
        required_cols = [
            "security_id",
            "ticker",
            "date",
            "open",
            "high",
            "low",
            "close",
            "volume",
            "market_cap",
        ]
        existing_cols = [c for c in required_cols if c in df.columns]
        df = df[existing_cols]

        # âœ… ë©”íƒ€ ì»¬ëŸ¼ ì¶”ê°€ (ìŠ¤í‚¤ë§ˆ ëª…ì„¸ ê¸°ë°˜)
        df["source_vendor"] = self.vendor
        df["snapshot_date"] = pd.to_datetime(self.trd_dt).date()
        df["created_at"] = pd.Timestamp.utcnow()
        # df["updated_at"] = pd.Timestamp.utcnow() # í•„ìš”ì—†ìŒ. ìƒˆë¡œ ë§Œë“¤ì–´ì„œ ì ì¬í•˜ë¯€ë¡œ

        # ê²°ì¸¡ê°’/íƒ€ì… ì •ë¦¬
        df = df.fillna({"volume": 0, "market_cap": 0})

        # âœ… ìŠ¤í‚¤ë§ˆ ìˆœì„œ ë§ì¶”ê¸°
        schema_order = [
            "security_id",
            "ticker",
            "date",
            "open",
            "high",
            "low",
            "close",
            "volume",
            "market_cap",
            "created_at",
            "updated_at",
            "source_vendor",
            "snapshot_date",
        ]
        df = df.reindex(columns=schema_order)

        return df

    # -------------------------------------------------------------------------
    # 4ï¸âƒ£ Build
    # -------------------------------------------------------------------------
    def build(self, **kwargs) -> Dict[str, Any]:
        """âœ… ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰: Load â†’ Transform â†’ Save"""
        self.log.info(f"ğŸš€ Building Price Warehouse snapshot for {self.trd_dt}")

        datasets = self._load_source_datasets()
        df = self._transform_business_logic(**datasets)

        # ì €ì¥
        save_info = self.save_parquet(df)

        # ë©”íƒ€ë°ì´í„° ê¸°ë¡
        meta_info = self.save_metadata(
            row_count=len(df),
            file_path=save_info["file_path"],
            vendor=self.vendor,
            layer=self.layer,
            snapshot_date=self.trd_dt,
        )

        self._update_domain_metadata(record_count=len(df))
        self.log.info(f"ğŸ Price Warehouse build complete: {save_info['file_path']}")
        return meta_info

    # -------------------------------------------------------------------------
    # ğŸ”‘ Persistent ID Assignment
    # -------------------------------------------------------------------------
    def _assign_persistent_security_id(self, df: pd.DataFrame) -> pd.DataFrame:
        """asset_master ê¸°ë°˜ security_id ë§¤í•‘"""
        if df.empty:
            self.log.warning("âš ï¸ No records to assign security_id (empty DataFrame).")
            return df

        master_df = load_asset_master_latest(self.domain_group)

        # âœ… êµ­ê°€ë³„ join key ì„¤ì •
        if self.country_code in ["USA", "US"]:
            join_keys = ["ticker"]  # ğŸ‡ºğŸ‡¸ ë¯¸êµ­ì€ ticker ë‹¨ìœ„ ë§¤í•‘
        else:
            join_keys = ["ticker", "exchange_code"]  # ğŸ‡°ğŸ‡· ë“±ì€ ê±°ë˜ì†Œë³„

        self.log.info(f"ğŸ§© Using join keys for mapping: {join_keys}")

        # âœ… key ì •ê·œí™” (ëŒ€ì†Œë¬¸ì/ê³µë°±)
        for col in join_keys:
            df[col] = df[col].astype(str).str.upper().str.strip()
            master_df[col] = master_df[col].astype(str).str.upper().str.strip()

        # âœ… ë§¤í•‘ ìˆ˜í–‰ (ë²¡í„°í™”)
        df = df.merge(
            master_df[join_keys + ["security_id"]],
            on=join_keys,
            how="inner"
        )

        self.log.info(
            f"ğŸ”‘ Assigned security_id for {len(df):,} rows "
        )
        return df


